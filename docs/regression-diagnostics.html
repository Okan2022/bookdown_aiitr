<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Regression Diagnostics | 08-Further-Explanations</title>
  <meta name="description" content="Chapter 8 Regression Diagnostics | 08-Further-Explanations" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Regression Diagnostics | 08-Further-Explanations" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Regression Diagnostics | 08-Further-Explanations" />
  
  
  

<meta name="author" content="Okan Sarioglu" />


<meta name="date" content="2025-04-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="further-explanations.html"/>
<link rel="next" href="solutions-exercises.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<div style="text-align: center;">
  <img src="images/AIITR.svg" alt="AIITR Logo" style="vertical-align: middle; margin-left: 5px; width: 125px; height: auto;">
  <li style="display: inline; list-style-type: none;"><a href="./">by Okan Sarioglu</a></li>
</div>
<div style="text-align: center; margin-top: 10px;">
  <a href="https://github.com/Okan2022" target="_blank">
    <img src="images/github-mark.svg" alt="GitHub" style="vertical-align: middle; width: 25px; height: auto;">
  </a>
  <a href="https://www.linkedin.com/in/osarioglu" target="_blank" style="margin-left: 10px;">
    <img src="images/LinkedIn_icon.svg" alt="LinkedIn" style="vertical-align: middle; width: 25px; height: auto;">
  </a>
</div> 
<div style="text-align: center;margin-top: 10px;margin-bottom: 10px">
<a href="aiitr_script.R" download="aiitr_script.R" style="display: inline-block; padding: 10px 20px; font-size: 16px; color: #fff; background-color: #4169e1; text-decoration: none; border-radius: 8px;">Download R Script</a>
</div>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome !</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-course"><i class="fa fa-check"></i>About this Course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-me"><i class="fa fa-check"></i>About Me</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals.html"><a href="fundamentals.html"><i class="fa fa-check"></i><b>1</b> Fundamentals</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals.html"><a href="fundamentals.html#getting-familiar-with-rstudio-and-establishing-a-workflow"><i class="fa fa-check"></i><b>1.1</b> Getting familiar with RStudio and establishing a Workflow</a></li>
<li class="chapter" data-level="1.2" data-path="fundamentals.html"><a href="fundamentals.html#lets-get-started-r-as-a-fancy-calculator"><i class="fa fa-check"></i><b>1.2</b> Lets get started: R as a fancy calculator</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals.html"><a href="fundamentals.html#ifelse-statements-and-the-ifelse-function"><i class="fa fa-check"></i><b>1.3</b> <code>ifelse</code> statements and the <code>ifelse()</code> function</a></li>
<li class="chapter" data-level="1.4" data-path="fundamentals.html"><a href="fundamentals.html#outlook"><i class="fa fa-check"></i><b>1.4</b> Outlook</a></li>
<li class="chapter" data-level="1.5" data-path="fundamentals.html"><a href="fundamentals.html#exercise-section"><i class="fa fa-check"></i><b>1.5</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>2</b> Data Manipulation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-manipulation.html"><a href="data-manipulation.html#packages"><i class="fa fa-check"></i><b>2.1</b> Packages</a></li>
<li class="chapter" data-level="2.2" data-path="data-manipulation.html"><a href="data-manipulation.html#working-with-packages"><i class="fa fa-check"></i><b>2.2</b> Working with packages</a></li>
<li class="chapter" data-level="2.3" data-path="data-manipulation.html"><a href="data-manipulation.html#the-data-we-will-work-with-the-european-social-survey-ess"><i class="fa fa-check"></i><b>2.3</b> The Data we will work with: The European Social Survey (ESS)</a></li>
<li class="chapter" data-level="2.4" data-path="data-manipulation.html"><a href="data-manipulation.html#lets-wrangle-the-data-the-dplyr-package"><i class="fa fa-check"></i><b>2.4</b> Let’s wrangle the data: The <code>dplyr</code> package</a></li>
<li class="chapter" data-level="2.5" data-path="data-manipulation.html"><a href="data-manipulation.html#merging-datasets"><i class="fa fa-check"></i><b>2.5</b> Merging Datasets</a></li>
<li class="chapter" data-level="2.6" data-path="data-manipulation.html"><a href="data-manipulation.html#outlook-1"><i class="fa fa-check"></i><b>2.6</b> Outlook</a></li>
<li class="chapter" data-level="2.7" data-path="data-manipulation.html"><a href="data-manipulation.html#exercise-section-1"><i class="fa fa-check"></i><b>2.7</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-visualisation.html"><a href="data-visualisation.html"><i class="fa fa-check"></i><b>3</b> Data Visualisation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-visualisation.html"><a href="data-visualisation.html#introduction-to-ggplot2"><i class="fa fa-check"></i><b>3.1</b> Introduction to <code>ggplot2</code></a></li>
<li class="chapter" data-level="3.2" data-path="data-visualisation.html"><a href="data-visualisation.html#distributions-histogram-density-plots-and-boxplots"><i class="fa fa-check"></i><b>3.2</b> Distributions: Histogram, Density Plots, and Boxplots</a></li>
<li class="chapter" data-level="3.3" data-path="data-visualisation.html"><a href="data-visualisation.html#ranking-barplot"><i class="fa fa-check"></i><b>3.3</b> Ranking: Barplot</a></li>
<li class="chapter" data-level="3.4" data-path="data-visualisation.html"><a href="data-visualisation.html#evolution-line-chart"><i class="fa fa-check"></i><b>3.4</b> Evolution: Line Chart</a></li>
<li class="chapter" data-level="3.5" data-path="data-visualisation.html"><a href="data-visualisation.html#correlation-scatterplots"><i class="fa fa-check"></i><b>3.5</b> Correlation: Scatterplots</a></li>
<li class="chapter" data-level="3.6" data-path="data-visualisation.html"><a href="data-visualisation.html#making-plots-with-facet_wrap-and-facet_grid"><i class="fa fa-check"></i><b>3.6</b> Making Plots with <code>facet_wrap()</code> and <code>facet_grid()</code></a></li>
<li class="chapter" data-level="3.7" data-path="data-visualisation.html"><a href="data-visualisation.html#outlook-2"><i class="fa fa-check"></i><b>3.7</b> Outlook</a></li>
<li class="chapter" data-level="3.8" data-path="data-visualisation.html"><a href="data-visualisation.html#outlook-3"><i class="fa fa-check"></i><b>3.8</b> Outlook</a></li>
<li class="chapter" data-level="3.9" data-path="data-visualisation.html"><a href="data-visualisation.html#exercise-section-2"><i class="fa fa-check"></i><b>3.9</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>4</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#standard-descriptive-statistics"><i class="fa fa-check"></i><b>4.1</b> Standard Descriptive Statistics</a></li>
<li class="chapter" data-level="4.2" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#working-with-eda-packages"><i class="fa fa-check"></i><b>4.2</b> Working with EDA packages</a></li>
<li class="chapter" data-level="4.3" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#conclusion"><i class="fa fa-check"></i><b>4.3</b> Conclusion</a></li>
<li class="chapter" data-level="4.4" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#exercise-section-3"><i class="fa fa-check"></i><b>4.4</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-analysis.html"><a href="data-analysis.html"><i class="fa fa-check"></i><b>5</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="data-analysis.html"><a href="data-analysis.html#linear-regression"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="data-analysis.html"><a href="data-analysis.html#hypothesis-testing-in-r"><i class="fa fa-check"></i><b>5.2</b> Hypothesis Testing in R</a></li>
<li class="chapter" data-level="5.3" data-path="data-analysis.html"><a href="data-analysis.html#multivariate-regression"><i class="fa fa-check"></i><b>5.3</b> Multivariate Regression</a></li>
<li class="chapter" data-level="5.4" data-path="data-analysis.html"><a href="data-analysis.html#categorical-variables"><i class="fa fa-check"></i><b>5.4</b> Categorical Variables</a></li>
<li class="chapter" data-level="5.5" data-path="data-analysis.html"><a href="data-analysis.html#outlook-4"><i class="fa fa-check"></i><b>5.5</b> Outlook</a></li>
<li class="chapter" data-level="5.6" data-path="data-analysis.html"><a href="data-analysis.html#exercise-section-4"><i class="fa fa-check"></i><b>5.6</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="loops-and-functions.html"><a href="loops-and-functions.html"><i class="fa fa-check"></i><b>6</b> Loops and Functions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="loops-and-functions.html"><a href="loops-and-functions.html#loops"><i class="fa fa-check"></i><b>6.1</b> Loops</a></li>
<li class="chapter" data-level="6.2" data-path="loops-and-functions.html"><a href="loops-and-functions.html#apply-function-family"><i class="fa fa-check"></i><b>6.2</b> <code>apply()</code> Function Family</a></li>
<li class="chapter" data-level="6.3" data-path="loops-and-functions.html"><a href="loops-and-functions.html#writing-your-own-functions"><i class="fa fa-check"></i><b>6.3</b> Writing your own functions</a></li>
<li class="chapter" data-level="6.4" data-path="loops-and-functions.html"><a href="loops-and-functions.html#outlook-5"><i class="fa fa-check"></i><b>6.4</b> Outlook</a></li>
<li class="chapter" data-level="6.5" data-path="loops-and-functions.html"><a href="loops-and-functions.html#exercise-section-5"><i class="fa fa-check"></i><b>6.5</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-explanations.html"><a href="further-explanations.html"><i class="fa fa-check"></i><b>7</b> Further Explanations</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-explanations.html"><a href="further-explanations.html#descriptive-statistics-fundamentals"><i class="fa fa-check"></i><b>7.1</b> Descriptive Statistics Fundamentals</a></li>
<li class="chapter" data-level="7.2" data-path="further-explanations.html"><a href="further-explanations.html#working-with-distributions"><i class="fa fa-check"></i><b>7.2</b> Working with Distributions</a></li>
<li class="chapter" data-level="7.3" data-path="further-explanations.html"><a href="further-explanations.html#conclusion-1"><i class="fa fa-check"></i><b>7.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#model-fit-bivariate-regression"><i class="fa fa-check"></i><b>8.1</b> Model Fit: Bivariate Regression</a></li>
<li class="chapter" data-level="8.2" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#model-fit-multivariate-regression"><i class="fa fa-check"></i><b>8.2</b> Model Fit: Multivariate Regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="solutions-exercises.html"><a href="solutions-exercises.html"><i class="fa fa-check"></i><b>9</b> Solutions Exercises</a>
<ul>
<li class="chapter" data-level="9.1" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-1-fundamentals"><i class="fa fa-check"></i><b>9.1</b> Chapter 1: Fundamentals</a></li>
<li class="chapter" data-level="9.2" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-2-data-manipulation"><i class="fa fa-check"></i><b>9.2</b> Chapter 2: Data Manipulation</a></li>
<li class="chapter" data-level="9.3" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-3-data-visualisation"><i class="fa fa-check"></i><b>9.3</b> Chapter 3: Data Visualisation</a></li>
<li class="chapter" data-level="9.4" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-4-exploratory-data-analysis"><i class="fa fa-check"></i><b>9.4</b> Chapter 4: Exploratory Data Analysis</a></li>
<li class="chapter" data-level="9.5" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-5-data-analysis"><i class="fa fa-check"></i><b>9.5</b> Chapter 5: Data Analysis</a></li>
<li class="chapter" data-level="9.6" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-6-loops-and-functions"><i class="fa fa-check"></i><b>9.6</b> Chapter 6: Loops and Functions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">08-Further-Explanations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-diagnostics" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Regression Diagnostics<a href="regression-diagnostics.html#regression-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="model-fit-bivariate-regression" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Model Fit: Bivariate Regression<a href="regression-diagnostics.html#model-fit-bivariate-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now the linear regression has a lot of assumptions, it is not like we can run the model every time how we want. Since it is a model, it makes assumptions and instead of just assuming them to be right, we can test them. To techniques to test them are called <strong>Measures of Fit</strong>. Because they test how much our data fits the data. Let us have a look at the assumptions and how we can test them:</p>
<div id="measures-of-fit" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Measures of Fit<a href="regression-diagnostics.html#measures-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="residuals" class="section level4 hasAnchor" number="8.1.1.1">
<h4><span class="header-section-number">8.1.1.1</span> Residuals<a href="regression-diagnostics.html#residuals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>1.) Calculating Residuals:</p>
<p><span class="math display">\[
Residuals = y_i - \hat{y_i} = y_i - (\hat{\beta_0} - \hat{\beta_1}x_i)
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(y_i\)</span> = our actual observed values of our dependent variable (<code>df$y</code>)</p></li>
<li><p><span class="math inline">\(\hat{y_i}\)</span> = are our predicted values based on our OLS estimator</p></li>
</ul>
<p>Reconsider the graph at 2.2.1, the residuals are basically the red lines, thus the distance from the line to the point. We can calculate the residuals for our graph:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="regression-diagnostics.html#cb31-1" tabindex="-1"></a><span class="co">#Getting the data</span></span>
<span id="cb31-2"><a href="regression-diagnostics.html#cb31-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># For reproducibility</span></span>
<span id="cb31-3"><a href="regression-diagnostics.html#cb31-3" tabindex="-1"></a></span>
<span id="cb31-4"><a href="regression-diagnostics.html#cb31-4" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">30</span> </span>
<span id="cb31-5"><a href="regression-diagnostics.html#cb31-5" tabindex="-1"></a></span>
<span id="cb31-6"><a href="regression-diagnostics.html#cb31-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n) <span class="sc">*</span> <span class="dv">10</span> </span>
<span id="cb31-7"><a href="regression-diagnostics.html#cb31-7" tabindex="-1"></a></span>
<span id="cb31-8"><a href="regression-diagnostics.html#cb31-8" tabindex="-1"></a>categorical_variable <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), n, <span class="at">replace =</span> <span class="cn">TRUE</span>))</span>
<span id="cb31-9"><a href="regression-diagnostics.html#cb31-9" tabindex="-1"></a></span>
<span id="cb31-10"><a href="regression-diagnostics.html#cb31-10" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fl">0.8</span> <span class="sc">+</span> <span class="fl">1.6</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">3</span>)</span>
<span id="cb31-11"><a href="regression-diagnostics.html#cb31-11" tabindex="-1"></a></span>
<span id="cb31-12"><a href="regression-diagnostics.html#cb31-12" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x,y, categorical_variable)</span>
<span id="cb31-13"><a href="regression-diagnostics.html#cb31-13" tabindex="-1"></a></span>
<span id="cb31-14"><a href="regression-diagnostics.html#cb31-14" tabindex="-1"></a><span class="co"># And or Model</span></span>
<span id="cb31-15"><a href="regression-diagnostics.html#cb31-15" tabindex="-1"></a><span class="co">#running a linear regression</span></span>
<span id="cb31-16"><a href="regression-diagnostics.html#cb31-16" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, </span>
<span id="cb31-17"><a href="regression-diagnostics.html#cb31-17" tabindex="-1"></a>             <span class="at">data =</span> df) </span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="regression-diagnostics.html#cb32-1" tabindex="-1"></a><span class="co">#First, we calculate the predictions for y</span></span>
<span id="cb32-2"><a href="regression-diagnostics.html#cb32-2" tabindex="-1"></a>df<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> <span class="fl">1.6821</span> <span class="sc">+</span> <span class="fl">1.5394</span><span class="sc">*</span>df<span class="sc">$</span>x </span>
<span id="cb32-3"><a href="regression-diagnostics.html#cb32-3" tabindex="-1"></a></span>
<span id="cb32-4"><a href="regression-diagnostics.html#cb32-4" tabindex="-1"></a><span class="co">#We get the Residuals by subtracting our actual y from y_hat</span></span>
<span id="cb32-5"><a href="regression-diagnostics.html#cb32-5" tabindex="-1"></a>df<span class="sc">$</span>residuals <span class="ot">&lt;-</span> df<span class="sc">$</span>y <span class="sc">-</span> df<span class="sc">$</span>y_hat </span>
<span id="cb32-6"><a href="regression-diagnostics.html#cb32-6" tabindex="-1"></a></span>
<span id="cb32-7"><a href="regression-diagnostics.html#cb32-7" tabindex="-1"></a><span class="co">#cheking it</span></span>
<span id="cb32-8"><a href="regression-diagnostics.html#cb32-8" tabindex="-1"></a><span class="fu">head</span>(df) </span></code></pre></div>
<pre><code>##          x         y categorical_variable     y_hat  residuals
## 1 2.875775  6.680633                    0  6.109068  0.5715646
## 2 7.883051 12.527668                    1 13.817269 -1.2896015
## 3 4.089769 10.029008                    0  7.977891  2.0511170
## 4 8.830174 17.562679                    1 15.275270  2.2874090
## 5 9.404673 18.312220                    1 16.159653  2.1525664
## 6 0.455565  3.594825                    0  2.383397  1.2114280</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="regression-diagnostics.html#cb34-1" tabindex="-1"></a><span class="co">#We could have done that automatically with R as well !  </span></span>
<span id="cb34-2"><a href="regression-diagnostics.html#cb34-2" tabindex="-1"></a>df<span class="sc">$</span>residuals_auto <span class="ot">&lt;-</span> <span class="fu">residuals</span>(model1)</span>
<span id="cb34-3"><a href="regression-diagnostics.html#cb34-3" tabindex="-1"></a></span>
<span id="cb34-4"><a href="regression-diagnostics.html#cb34-4" tabindex="-1"></a><span class="co">#Checking it</span></span>
<span id="cb34-5"><a href="regression-diagnostics.html#cb34-5" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>##          x         y categorical_variable     y_hat  residuals residuals_auto
## 1 2.875775  6.680633                    0  6.109068  0.5715646      0.5716538
## 2 7.883051 12.527668                    1 13.817269 -1.2896015     -1.2892991
## 3 4.089769 10.029008                    0  7.977891  2.0511170      2.0512579
## 4 8.830174 17.562679                    1 15.275270  2.2874090      2.2877518
## 5 9.404673 18.312220                    1 16.159653  2.1525664      2.1529337
## 6 0.455565  3.594825                    0  2.383397  1.2114280      1.2114141</code></pre>
<p>Let us have a look at a so-called <strong>Residual Plot</strong>: On the x-axis you plot the <strong>fitted values</strong>, thus our <code>y_hat</code>. On the y-axis you plot the residuals, thus <span class="math inline">\(y-\hat{y}\)</span>. Then you plot a horizontal line at y = 0. All dots on those lines show us the values correctly predicted by our model.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="regression-diagnostics.html#cb36-1" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(x, residuals_auto)) <span class="sc">+</span> </span>
<span id="cb36-2"><a href="regression-diagnostics.html#cb36-2" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb36-3"><a href="regression-diagnostics.html#cb36-3" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb36-4"><a href="regression-diagnostics.html#cb36-4" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Residuals&quot;</span>, <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">1</span>), </span>
<span id="cb36-5"><a href="regression-diagnostics.html#cb36-5" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="dv">6</span>)) <span class="sc">+</span></span>
<span id="cb36-6"><a href="regression-diagnostics.html#cb36-6" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Fitted Values&quot;</span>, <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">1</span>), </span>
<span id="cb36-7"><a href="regression-diagnostics.html#cb36-7" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb36-8"><a href="regression-diagnostics.html#cb36-8" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/residual%20plot-1.png" width="672" /></p>
</div>
<div id="homoskedasticity-and-heteroskedasticity" class="section level4 hasAnchor" number="8.1.1.2">
<h4><span class="header-section-number">8.1.1.2</span> Homoskedasticity and Heteroskedasticity<a href="regression-diagnostics.html#homoskedasticity-and-heteroskedasticity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One assumption of linear regression is that the variance of the error term is not correlated with our independent variable. Well, that is quite technocratic and means basically, that the residuals are distributed equally over the independent variables. Let us plot it to get a visual intuition:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="regression-diagnostics.html#cb37-1" tabindex="-1"></a><span class="co">#setting seed for reproduciability</span></span>
<span id="cb37-2"><a href="regression-diagnostics.html#cb37-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb37-3"><a href="regression-diagnostics.html#cb37-3" tabindex="-1"></a></span>
<span id="cb37-4"><a href="regression-diagnostics.html#cb37-4" tabindex="-1"></a><span class="co"># Generate some data</span></span>
<span id="cb37-5"><a href="regression-diagnostics.html#cb37-5" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">150</span>, <span class="fl">0.05</span>, <span class="dv">1</span>)</span>
<span id="cb37-6"><a href="regression-diagnostics.html#cb37-6" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">150</span>, <span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb37-7"><a href="regression-diagnostics.html#cb37-7" tabindex="-1"></a></span>
<span id="cb37-8"><a href="regression-diagnostics.html#cb37-8" tabindex="-1"></a><span class="co">#homoskedastic data </span></span>
<span id="cb37-9"><a href="regression-diagnostics.html#cb37-9" tabindex="-1"></a>y_homo <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> x <span class="sc">+</span> e </span>
<span id="cb37-10"><a href="regression-diagnostics.html#cb37-10" tabindex="-1"></a><span class="co">#heteroskedastic data </span></span>
<span id="cb37-11"><a href="regression-diagnostics.html#cb37-11" tabindex="-1"></a>y_hetero <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> x <span class="sc">+</span> e<span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb37-12"><a href="regression-diagnostics.html#cb37-12" tabindex="-1"></a><span class="co">#making a data frame with both data</span></span>
<span id="cb37-13"><a href="regression-diagnostics.html#cb37-13" tabindex="-1"></a>df_homo_hetero <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y_homo, y_hetero)</span>
<span id="cb37-14"><a href="regression-diagnostics.html#cb37-14" tabindex="-1"></a></span>
<span id="cb37-15"><a href="regression-diagnostics.html#cb37-15" tabindex="-1"></a><span class="co"># Scatterplot with homoscedasticity</span></span>
<span id="cb37-16"><a href="regression-diagnostics.html#cb37-16" tabindex="-1"></a>homoskedastic_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df_homo_hetero, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_homo)) <span class="sc">+</span></span>
<span id="cb37-17"><a href="regression-diagnostics.html#cb37-17" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb37-18"><a href="regression-diagnostics.html#cb37-18" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span><span class="co"># Add linear regression line</span></span>
<span id="cb37-19"><a href="regression-diagnostics.html#cb37-19" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Y&quot;</span>, <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="fl">0.5</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>)) <span class="sc">+</span></span>
<span id="cb37-20"><a href="regression-diagnostics.html#cb37-20" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Homoskedastic Plot&quot;</span>) <span class="sc">+</span></span>
<span id="cb37-21"><a href="regression-diagnostics.html#cb37-21" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb37-22"><a href="regression-diagnostics.html#cb37-22" tabindex="-1"></a></span>
<span id="cb37-23"><a href="regression-diagnostics.html#cb37-23" tabindex="-1"></a><span class="co"># Scatterplot with heteroscedasticity</span></span>
<span id="cb37-24"><a href="regression-diagnostics.html#cb37-24" tabindex="-1"></a>heteroskedastic_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df_homo_hetero, </span>
<span id="cb37-25"><a href="regression-diagnostics.html#cb37-25" tabindex="-1"></a>                               <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_hetero)) <span class="sc">+</span></span>
<span id="cb37-26"><a href="regression-diagnostics.html#cb37-26" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb37-27"><a href="regression-diagnostics.html#cb37-27" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span><span class="co">#Add linear regression line</span></span>
<span id="cb37-28"><a href="regression-diagnostics.html#cb37-28" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Heteroskedastic Plot&quot;</span>) <span class="sc">+</span></span>
<span id="cb37-29"><a href="regression-diagnostics.html#cb37-29" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Y&quot;</span>, <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="fl">0.5</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>)) <span class="sc">+</span></span>
<span id="cb37-30"><a href="regression-diagnostics.html#cb37-30" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb37-31"><a href="regression-diagnostics.html#cb37-31" tabindex="-1"></a></span>
<span id="cb37-32"><a href="regression-diagnostics.html#cb37-32" tabindex="-1"></a><span class="co"># Combine plots using facet_wrap</span></span>
<span id="cb37-33"><a href="regression-diagnostics.html#cb37-33" tabindex="-1"></a>facet_plots <span class="ot">&lt;-</span> <span class="fu">ggarrange</span>(homoskedastic_plot, heteroskedastic_plot, <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb37-34"><a href="regression-diagnostics.html#cb37-34" tabindex="-1"></a></span>
<span id="cb37-35"><a href="regression-diagnostics.html#cb37-35" tabindex="-1"></a><span class="co"># Print the combined plots</span></span>
<span id="cb37-36"><a href="regression-diagnostics.html#cb37-36" tabindex="-1"></a><span class="fu">print</span>(facet_plots)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/homo%20and%20hetero%20plot-1.png" width="672" /></p>
<p>In the left plot, you see the homoskedastic data. The dots are equally and constantly distributed around the fitted line. However, the right plot shows that the more the independent variable <strong>x</strong> increases, the more the observations are increasing. The dots are not constantly distributed over the line. In the case, that the data is heteroskedastic, then this is a problem. You could try to transform the independent variable by taking the logarithm (We will look into that later). You could also use so-called heteroskedastic regression, but this an advanced model.</p>
</div>
</div>
<div id="tss-ess-and-r2" class="section level3 hasAnchor" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> TSS, ESS and <span class="math inline">\(R^2\)</span><a href="regression-diagnostics.html#tss-ess-and-r2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we have the Residuals, we can calculate the Total Sum of Square (TSS), the Explained Sum of Squared ESS, and <span class="math inline">\(R^2\)</span>:</p>
<ul>
<li><p>TSS (Variation in the DV): <span class="math inline">\(TSS =\sum(y_i - \bar{y})^2\)</span> , we just subtract our actual values (<code>df$y</code>) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable.</p></li>
<li><p>ESS (Variation we explain in the DV): <span class="math inline">\(ESS = \sum(\hat{y_i} - \bar{y})^2\)</span> , now we use our predicted values (<code>df$y_hat</code>) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model.</p></li>
<li><p><span class="math inline">\(R^2\)</span> (The Variation we can predict from our model): <span class="math inline">\(R^2 = \frac{ESS}{TSS}\)</span> , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this <span class="math inline">\(R^2\)</span> is 1. If our model could not explain anything the variation would be 0, since the values of both cannot be negative. Let us calculate them:</p></li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="regression-diagnostics.html#cb38-1" tabindex="-1"></a><span class="co">#total sum of squares</span></span>
<span id="cb38-2"><a href="regression-diagnostics.html#cb38-2" tabindex="-1"></a>tss <span class="ot">&lt;-</span> <span class="fu">sum</span>((df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb38-3"><a href="regression-diagnostics.html#cb38-3" tabindex="-1"></a><span class="co">#explained sum of squares</span></span>
<span id="cb38-4"><a href="regression-diagnostics.html#cb38-4" tabindex="-1"></a>ess <span class="ot">&lt;-</span> <span class="fu">sum</span>((df<span class="sc">$</span>y_hat <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb38-5"><a href="regression-diagnostics.html#cb38-5" tabindex="-1"></a><span class="co">#caculating r squared</span></span>
<span id="cb38-6"><a href="regression-diagnostics.html#cb38-6" tabindex="-1"></a>r_squared <span class="ot">&lt;-</span> ess<span class="sc">/</span>tss</span>
<span id="cb38-7"><a href="regression-diagnostics.html#cb38-7" tabindex="-1"></a></span>
<span id="cb38-8"><a href="regression-diagnostics.html#cb38-8" tabindex="-1"></a><span class="co">#Printing it</span></span>
<span id="cb38-9"><a href="regression-diagnostics.html#cb38-9" tabindex="-1"></a>r_squared</span></code></pre></div>
<pre><code>## [1] 0.7631199</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="regression-diagnostics.html#cb40-1" tabindex="-1"></a><span class="co">#Summarizing it</span></span>
<span id="cb40-2"><a href="regression-diagnostics.html#cb40-2" tabindex="-1"></a><span class="fu">summary</span>(model1)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.7630777</code></pre>
</div>
<div id="influential-outliers" class="section level3 hasAnchor" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Influential Outliers<a href="regression-diagnostics.html#influential-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Outliers are extremely deviating values, which can impact our analysis and bias it. Therefore, we have to check, if our data contains such values. But first let us see how they can impact our data:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="regression-diagnostics.html#cb42-1" tabindex="-1"></a><span class="co">#set seed </span></span>
<span id="cb42-2"><a href="regression-diagnostics.html#cb42-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">069</span>)</span>
<span id="cb42-3"><a href="regression-diagnostics.html#cb42-3" tabindex="-1"></a></span>
<span id="cb42-4"><a href="regression-diagnostics.html#cb42-4" tabindex="-1"></a><span class="co">#generate fake data with outlier </span></span>
<span id="cb42-5"><a href="regression-diagnostics.html#cb42-5" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(<span class="dv">10</span>, <span class="at">min =</span> <span class="dv">30</span>, <span class="at">max =</span> <span class="dv">70</span>))</span>
<span id="cb42-6"><a href="regression-diagnostics.html#cb42-6" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10</span> , <span class="at">mean =</span> <span class="dv">200</span>, <span class="at">sd =</span> <span class="dv">50</span>)</span>
<span id="cb42-7"><a href="regression-diagnostics.html#cb42-7" tabindex="-1"></a>y1[<span class="dv">9</span>] <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb42-8"><a href="regression-diagnostics.html#cb42-8" tabindex="-1"></a>data_outlier <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, y1)</span>
<span id="cb42-9"><a href="regression-diagnostics.html#cb42-9" tabindex="-1"></a></span>
<span id="cb42-10"><a href="regression-diagnostics.html#cb42-10" tabindex="-1"></a><span class="co">#Model with Outlier </span></span>
<span id="cb42-11"><a href="regression-diagnostics.html#cb42-11" tabindex="-1"></a>model_outlier <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1 <span class="sc">~</span> x1) </span>
<span id="cb42-12"><a href="regression-diagnostics.html#cb42-12" tabindex="-1"></a></span>
<span id="cb42-13"><a href="regression-diagnostics.html#cb42-13" tabindex="-1"></a><span class="co">#Model without Outlier</span></span>
<span id="cb42-14"><a href="regression-diagnostics.html#cb42-14" tabindex="-1"></a>model_without_outlier <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1[<span class="sc">-</span><span class="dv">9</span>] <span class="sc">~</span> x1[<span class="sc">-</span><span class="dv">9</span>]) </span>
<span id="cb42-15"><a href="regression-diagnostics.html#cb42-15" tabindex="-1"></a></span>
<span id="cb42-16"><a href="regression-diagnostics.html#cb42-16" tabindex="-1"></a><span class="co">#Plotting the Data </span></span>
<span id="cb42-17"><a href="regression-diagnostics.html#cb42-17" tabindex="-1"></a></span>
<span id="cb42-18"><a href="regression-diagnostics.html#cb42-18" tabindex="-1"></a><span class="co"># Scatter plot with points</span></span>
<span id="cb42-19"><a href="regression-diagnostics.html#cb42-19" tabindex="-1"></a><span class="fu">ggplot</span>(data_outlier, <span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> y1)) <span class="sc">+</span></span>
<span id="cb42-20"><a href="regression-diagnostics.html#cb42-20" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">20</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb42-21"><a href="regression-diagnostics.html#cb42-21" tabindex="-1"></a>  <span class="co"># Regression line for the model with outlier</span></span>
<span id="cb42-22"><a href="regression-diagnostics.html#cb42-22" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">slope =</span> model_outlier<span class="sc">$</span>coefficients[<span class="dv">2</span>], <span class="at">intercept =</span></span>
<span id="cb42-23"><a href="regression-diagnostics.html#cb42-23" tabindex="-1"></a>              model_outlier<span class="sc">$</span>coefficients[<span class="dv">1</span>], </span>
<span id="cb42-24"><a href="regression-diagnostics.html#cb42-24" tabindex="-1"></a>              <span class="at">color =</span> <span class="st">&quot;Model with Outlier&quot;</span>), <span class="at">linewidth =</span> <span class="fl">0.75</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb42-25"><a href="regression-diagnostics.html#cb42-25" tabindex="-1"></a>  <span class="co"># Regression line for the model without outlier</span></span>
<span id="cb42-26"><a href="regression-diagnostics.html#cb42-26" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">slope =</span> model_without_outlier<span class="sc">$</span>coefficients[<span class="dv">2</span>], </span>
<span id="cb42-27"><a href="regression-diagnostics.html#cb42-27" tabindex="-1"></a>              <span class="at">intercept =</span> model_without_outlier<span class="sc">$</span>coefficients[<span class="dv">1</span>], </span>
<span id="cb42-28"><a href="regression-diagnostics.html#cb42-28" tabindex="-1"></a>              <span class="at">color =</span> <span class="st">&quot;Model without Outlier&quot;</span>), <span class="at">linewidth =</span> <span class="fl">0.75</span>, </span>
<span id="cb42-29"><a href="regression-diagnostics.html#cb42-29" tabindex="-1"></a>              <span class="at">show.legend =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb42-30"><a href="regression-diagnostics.html#cb42-30" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Independent Variable&quot;</span>) <span class="sc">+</span></span>
<span id="cb42-31"><a href="regression-diagnostics.html#cb42-31" tabindex="-1"></a>  <span class="co"># Adding legend</span></span>
<span id="cb42-32"><a href="regression-diagnostics.html#cb42-32" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> </span>
<span id="cb42-33"><a href="regression-diagnostics.html#cb42-33" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="fu">c</span>(<span class="fl">0.15</span>,<span class="fl">0.9</span>), </span>
<span id="cb42-34"><a href="regression-diagnostics.html#cb42-34" tabindex="-1"></a>        <span class="at">legend.title =</span> <span class="fu">element_blank</span>()) </span></code></pre></div>
<pre><code>## Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2
## 3.5.0.
## ℹ Please use the `legend.position.inside` argument of `theme()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.</code></pre>
<p><img src="bookdownproj_files/figure-html/plotting%20bias%20through%20outliers-1.png" width="672" /></p>
<p>We see that this one observation completely biases our sample. But how do we find out, which observation is an influential outlier? There is a metric called <strong>Cook’s Distance</strong>, we can use. Let us do it and plot it in R.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="regression-diagnostics.html#cb44-1" tabindex="-1"></a><span class="co">#Cooks Distance can be calculated with a built-in function</span></span>
<span id="cb44-2"><a href="regression-diagnostics.html#cb44-2" tabindex="-1"></a>data_outlier<span class="sc">$</span>cooks_distance <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(model_outlier) </span>
<span id="cb44-3"><a href="regression-diagnostics.html#cb44-3" tabindex="-1"></a></span>
<span id="cb44-4"><a href="regression-diagnostics.html#cb44-4" tabindex="-1"></a><span class="co">#Plotting it</span></span>
<span id="cb44-5"><a href="regression-diagnostics.html#cb44-5" tabindex="-1"></a><span class="fu">ggplot</span>(data_outlier, <span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> cooks_distance)) <span class="sc">+</span> </span>
<span id="cb44-6"><a href="regression-diagnostics.html#cb44-6" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">colour =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb44-7"><a href="regression-diagnostics.html#cb44-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Cook&#39;s Distance&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Independent Variables&quot;</span>) <span class="sc">+</span> </span>
<span id="cb44-8"><a href="regression-diagnostics.html#cb44-8" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span> </span>
<span id="cb44-9"><a href="regression-diagnostics.html#cb44-9" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/plotting%20cooks%20distance-1.png" width="672" /></p>
<p>We can clearly see that <strong>Cook’s Distance</strong> detected the outlier. The rule is that values with a cooks distance bigger than 1 have to be eliminated. You can do that with the <code>filter()</code> function and run the model afterward again without the outliers.</p>
</div>
<div id="functional-form" class="section level3 hasAnchor" number="8.1.4">
<h3><span class="header-section-number">8.1.4</span> Functional Form<a href="regression-diagnostics.html#functional-form" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First let us get our data:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="regression-diagnostics.html#cb45-1" tabindex="-1"></a><span class="co">#Simulate further data</span></span>
<span id="cb45-2"><a href="regression-diagnostics.html#cb45-2" tabindex="-1"></a>X_quadratic <span class="ot">&lt;-</span> X <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">50</span>, <span class="at">min =</span> <span class="sc">-</span><span class="dv">5</span>, <span class="at">max =</span> <span class="dv">5</span>)</span>
<span id="cb45-3"><a href="regression-diagnostics.html#cb45-3" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, <span class="at">sd =</span> <span class="dv">1</span>)  </span>
<span id="cb45-4"><a href="regression-diagnostics.html#cb45-4" tabindex="-1"></a></span>
<span id="cb45-5"><a href="regression-diagnostics.html#cb45-5" tabindex="-1"></a><span class="co">#True relation</span></span>
<span id="cb45-6"><a href="regression-diagnostics.html#cb45-6" tabindex="-1"></a>Y_quadratic <span class="ot">&lt;-</span> X<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> X <span class="sc">+</span> u</span>
<span id="cb45-7"><a href="regression-diagnostics.html#cb45-7" tabindex="-1"></a></span>
<span id="cb45-8"><a href="regression-diagnostics.html#cb45-8" tabindex="-1"></a><span class="co">#Making a data frame out of it</span></span>
<span id="cb45-9"><a href="regression-diagnostics.html#cb45-9" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X_quadratic, Y_quadratic)</span></code></pre></div>
<p>Linear regression is a mathematical model. Therefore it is based on assumptions. But we should not just assume them, we should test them! One assumption is that linearity is assumed between X and Y. But that can be problematic consider following example:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="regression-diagnostics.html#cb46-1" tabindex="-1"></a><span class="co"># estimate a simple regression model </span></span>
<span id="cb46-2"><a href="regression-diagnostics.html#cb46-2" tabindex="-1"></a>model_simple <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y_quadratic <span class="sc">~</span> X_quadratic, <span class="at">data =</span> df2)</span>
<span id="cb46-3"><a href="regression-diagnostics.html#cb46-3" tabindex="-1"></a></span>
<span id="cb46-4"><a href="regression-diagnostics.html#cb46-4" tabindex="-1"></a><span class="co"># Summarize it</span></span>
<span id="cb46-5"><a href="regression-diagnostics.html#cb46-5" tabindex="-1"></a><span class="fu">summary</span>(model_simple)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y_quadratic ~ X_quadratic, data = df2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.703 -6.508 -1.478  5.277 17.464 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   7.5912     1.0914   6.955 8.61e-09 ***
## X_quadratic   2.0220     0.3948   5.122 5.32e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.709 on 48 degrees of freedom
## Multiple R-squared:  0.3534, Adjusted R-squared:  0.3399 
## F-statistic: 26.23 on 1 and 48 DF,  p-value: 5.321e-06</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="regression-diagnostics.html#cb48-1" tabindex="-1"></a><span class="co">#Plot it</span></span>
<span id="cb48-2"><a href="regression-diagnostics.html#cb48-2" tabindex="-1"></a><span class="fu">ggplot</span>(df2, <span class="fu">aes</span>(<span class="at">x =</span> X_quadratic, <span class="at">y =</span> Y_quadratic)) <span class="sc">+</span> </span>
<span id="cb48-3"><a href="regression-diagnostics.html#cb48-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">20</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb48-4"><a href="regression-diagnostics.html#cb48-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> </span>
<span id="cb48-5"><a href="regression-diagnostics.html#cb48-5" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdownproj_files/figure-html/plotting%20linear%20fit%20through%20quadratic%20form-1.png" width="672" /></p>
<p>As you can see something look wrong. There seems to be a correlation between the two variables, but it does not seem linear. In such a case it does make sense to square the independent variable and run the regression again:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="regression-diagnostics.html#cb50-1" tabindex="-1"></a><span class="co"># estimate a simple regression model </span></span>
<span id="cb50-2"><a href="regression-diagnostics.html#cb50-2" tabindex="-1"></a>model_quadratic <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y_quadratic <span class="sc">~</span> X_quadratic<span class="sc">^</span><span class="dv">2</span>, <span class="at">data =</span> df2)</span>
<span id="cb50-3"><a href="regression-diagnostics.html#cb50-3" tabindex="-1"></a></span>
<span id="cb50-4"><a href="regression-diagnostics.html#cb50-4" tabindex="-1"></a><span class="co">#Summarize it </span></span>
<span id="cb50-5"><a href="regression-diagnostics.html#cb50-5" tabindex="-1"></a><span class="fu">summary</span>(model_quadratic)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y_quadratic ~ X_quadratic^2, data = df2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.703 -6.508 -1.478  5.277 17.464 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   7.5912     1.0914   6.955 8.61e-09 ***
## X_quadratic   2.0220     0.3948   5.122 5.32e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.709 on 48 degrees of freedom
## Multiple R-squared:  0.3534, Adjusted R-squared:  0.3399 
## F-statistic: 26.23 on 1 and 48 DF,  p-value: 5.321e-06</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="regression-diagnostics.html#cb52-1" tabindex="-1"></a><span class="co">#Plot it</span></span>
<span id="cb52-2"><a href="regression-diagnostics.html#cb52-2" tabindex="-1"></a><span class="fu">ggplot</span>(df2, <span class="fu">aes</span>(<span class="at">x =</span> X_quadratic, <span class="at">y =</span> Y_quadratic)) <span class="sc">+</span> </span>
<span id="cb52-3"><a href="regression-diagnostics.html#cb52-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">20</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb52-4"><a href="regression-diagnostics.html#cb52-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">2</span>), </span>
<span id="cb52-5"><a href="regression-diagnostics.html#cb52-5" tabindex="-1"></a>              <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, </span>
<span id="cb52-6"><a href="regression-diagnostics.html#cb52-6" tabindex="-1"></a>              <span class="at">se =</span> <span class="cn">FALSE</span>,) <span class="sc">+</span> </span>
<span id="cb52-7"><a href="regression-diagnostics.html#cb52-7" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;X&quot;</span>, <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">1</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb52-8"><a href="regression-diagnostics.html#cb52-8" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Y&quot;</span>) <span class="sc">+</span></span>
<span id="cb52-9"><a href="regression-diagnostics.html#cb52-9" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/plotting%20quadratic%20fit%20with%20quadratic%20form-1.png" width="672" /></p>
<p>Well that looks better and is the proper way to deal with quadratic relationships in linear regression. Well, data can take on not only a quadratic form, it could also take on a form of a square-root function. I will show the most classical example of such a functional form. The <code>gapminder</code> data is loaded. It contains data about the average life expectancy (<code>lifeExp</code>)and the GDP per capita (<code>gdpPercap</code>) of countries in different years. Let us look if the GDP per Capita is correlated with Life Expectancy:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="regression-diagnostics.html#cb53-1" tabindex="-1"></a><span class="co">#checking the data</span></span>
<span id="cb53-2"><a href="regression-diagnostics.html#cb53-2" tabindex="-1"></a><span class="fu">head</span>(gapminder)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 6
##   country     continent  year lifeExp      pop gdpPercap
##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;
## 1 Afghanistan Asia       1952    28.8  8425333      779.
## 2 Afghanistan Asia       1957    30.3  9240934      821.
## 3 Afghanistan Asia       1962    32.0 10267083      853.
## 4 Afghanistan Asia       1967    34.0 11537966      836.
## 5 Afghanistan Asia       1972    36.1 13079460      740.
## 6 Afghanistan Asia       1977    38.4 14880372      786.</code></pre>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="regression-diagnostics.html#cb55-1" tabindex="-1"></a><span class="co">#Plotting it</span></span>
<span id="cb55-2"><a href="regression-diagnostics.html#cb55-2" tabindex="-1"></a><span class="fu">ggplot</span>(gapminder, <span class="fu">aes</span>(gdpPercap, lifeExp)) <span class="sc">+</span> </span>
<span id="cb55-3"><a href="regression-diagnostics.html#cb55-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb55-4"><a href="regression-diagnostics.html#cb55-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> </span>
<span id="cb55-5"><a href="regression-diagnostics.html#cb55-5" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Life Expectancy&quot;</span>, <span class="fu">seq</span>(<span class="dv">30</span>, <span class="dv">80</span>, <span class="dv">10</span>), </span>
<span id="cb55-6"><a href="regression-diagnostics.html#cb55-6" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">30</span>, <span class="dv">80</span>)) <span class="sc">+</span> </span>
<span id="cb55-7"><a href="regression-diagnostics.html#cb55-7" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdownproj_files/figure-html/unlogged%20fit-1.png" width="672" /></p>
<p>Well, that looks terrible. What can we do? I already mentioned that the fitted line looks like a square-root function (<span class="math inline">\(y = \beta{\sqrt{x}}\)</span> ). When you take the logarithm of square root, you neutralize the square root and only x remains <span class="math inline">\(\log{(\sqrt{x})} = x\)</span>. When you do that the functional form changes to <span class="math inline">\(y = \beta{x}\)</span>. Well, that is exactly the systematic component we are after:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="regression-diagnostics.html#cb57-1" tabindex="-1"></a><span class="fu">ggplot</span>(gapminder, <span class="fu">aes</span>(<span class="fu">log</span>(gdpPercap), lifeExp)) <span class="sc">+</span> </span>
<span id="cb57-2"><a href="regression-diagnostics.html#cb57-2" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb57-3"><a href="regression-diagnostics.html#cb57-3" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> </span>
<span id="cb57-4"><a href="regression-diagnostics.html#cb57-4" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Life Expectancy&quot;</span>, <span class="fu">seq</span>(<span class="dv">30</span>, <span class="dv">80</span>, <span class="dv">10</span>), </span>
<span id="cb57-5"><a href="regression-diagnostics.html#cb57-5" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">30</span>, <span class="dv">80</span>)) <span class="sc">+</span> </span>
<span id="cb57-6"><a href="regression-diagnostics.html#cb57-6" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;GDP per Capita&quot;</span>) <span class="sc">+</span></span>
<span id="cb57-7"><a href="regression-diagnostics.html#cb57-7" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdownproj_files/figure-html/logged%20fit-1.png" width="672" /></p>
<p>This looks more like what we want to achieve. What can we see in that plot? When we run a model we want to take the logarithm of the independent variable, when we expect the following: If the most observations of a variable are low, but some observations are extremely high such functional forms can occur. In our example, most of the countries have a low GDP per Capita, but some countries such as the Western European countries or the USA have such a a high level of GDP per Capita, they change the functional form of the fitted line. These are influential outliers, but too much to delete them. It could bias the representativeness of the sample, therefore we can deal with them by taking the logarithm.</p>
</div>
<div id="independent-observation" class="section level3 hasAnchor" number="8.1.5">
<h3><span class="header-section-number">8.1.5</span> Independent Observation<a href="regression-diagnostics.html#independent-observation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another assumption of the linear regression model is the independent, identically distributed (i.i.d) assumption. That sounds complicated but it really is not. Consider following plot:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="regression-diagnostics.html#cb59-1" tabindex="-1"></a><span class="co"># Getting the Data</span></span>
<span id="cb59-2"><a href="regression-diagnostics.html#cb59-2" tabindex="-1"></a><span class="co"># set seed</span></span>
<span id="cb59-3"><a href="regression-diagnostics.html#cb59-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb59-4"><a href="regression-diagnostics.html#cb59-4" tabindex="-1"></a></span>
<span id="cb59-5"><a href="regression-diagnostics.html#cb59-5" tabindex="-1"></a><span class="co"># generate a date vector</span></span>
<span id="cb59-6"><a href="regression-diagnostics.html#cb59-6" tabindex="-1"></a>date <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">as.Date</span>(<span class="st">&quot;1960/1/1&quot;</span>), <span class="fu">as.Date</span>(<span class="st">&quot;2020/1/1&quot;</span>), <span class="st">&quot;years&quot;</span>)</span>
<span id="cb59-7"><a href="regression-diagnostics.html#cb59-7" tabindex="-1"></a></span>
<span id="cb59-8"><a href="regression-diagnostics.html#cb59-8" tabindex="-1"></a><span class="co"># initialize the employment vector</span></span>
<span id="cb59-9"><a href="regression-diagnostics.html#cb59-9" tabindex="-1"></a>y_time <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5000</span>, <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="fu">length</span>(date)<span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb59-10"><a href="regression-diagnostics.html#cb59-10" tabindex="-1"></a></span>
<span id="cb59-11"><a href="regression-diagnostics.html#cb59-11" tabindex="-1"></a><span class="co"># generate time series observations with random influences</span></span>
<span id="cb59-12"><a href="regression-diagnostics.html#cb59-12" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(date)) {</span>
<span id="cb59-13"><a href="regression-diagnostics.html#cb59-13" tabindex="-1"></a>  </span>
<span id="cb59-14"><a href="regression-diagnostics.html#cb59-14" tabindex="-1"></a>    y_time[i] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">50</span> <span class="sc">+</span> <span class="fl">0.98</span> <span class="sc">*</span> y_time[i<span class="dv">-1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">200</span>)</span>
<span id="cb59-15"><a href="regression-diagnostics.html#cb59-15" tabindex="-1"></a>}</span>
<span id="cb59-16"><a href="regression-diagnostics.html#cb59-16" tabindex="-1"></a></span>
<span id="cb59-17"><a href="regression-diagnostics.html#cb59-17" tabindex="-1"></a><span class="co"># Plot it</span></span>
<span id="cb59-18"><a href="regression-diagnostics.html#cb59-18" tabindex="-1"></a>df_time_series <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y_time, date)</span>
<span id="cb59-19"><a href="regression-diagnostics.html#cb59-19" tabindex="-1"></a><span class="fu">ggplot</span>(df_time_series, <span class="fu">aes</span>(date, y_time)) <span class="sc">+</span> </span>
<span id="cb59-20"><a href="regression-diagnostics.html#cb59-20" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb59-21"><a href="regression-diagnostics.html#cb59-21" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Y&quot;</span>) <span class="sc">+</span></span>
<span id="cb59-22"><a href="regression-diagnostics.html#cb59-22" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Year&quot;</span>) <span class="sc">+</span></span>
<span id="cb59-23"><a href="regression-diagnostics.html#cb59-23" tabindex="-1"></a>  <span class="fu">theme_bw</span>()  </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/time%20trend-1.png" width="672" /></p>
<p>If you look at the plot, can we assume that the observation Year = 2000 is independent from the Years before? No, the observation in the years are correlated to each other, thus the assumption is violated. This is basically the huge problem of working with longitudinal data (time-series cross-sectional or panel). If you face such problems there are plenty of other methods to use: Interrupted time series, Difference-in-Difference Designs, Panel-Matching, Fixed-Effects Models etc.</p>
</div>
</div>
<div id="model-fit-multivariate-regression" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Model Fit: Multivariate Regression<a href="regression-diagnostics.html#model-fit-multivariate-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="model-fit-adjusted-r-squared" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Model Fit: Adjusted R-squared<a href="regression-diagnostics.html#model-fit-adjusted-r-squared" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The last important aspect of Multivariate Regression is the Adjusted R-squared measure. Reconsider, the calculation of the classical R-squared:</p>
<ul>
<li><p>TSS (Variation in the DV): <span class="math inline">\(TSS =\sum(y_i - \bar{y})^2\)</span> , we just subtract our actual values (<code>df$y</code>) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable.</p></li>
<li><p>ESS (Variation we explain in the DV): <span class="math inline">\(ESS = \sum(\hat{y_i} - \bar{y})^2\)</span> , now we use our predicted values (<code>df$y_hat</code>) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model.</p></li>
<li><p><span class="math inline">\(R^2\)</span> (The Variation we can predict from our model): <span class="math inline">\(R^2 = \frac{ESS}{TSS}\)</span> , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this <span class="math inline">\(R^2\)</span> is 1.</p></li>
</ul>
<p>The problem with the classical R-squared is, that if you would add useless independent variables to it, the classical R-squared would decrease, although your model did not increase in explanatory power. This is called <strong>overfitting</strong>. However, adjusted R-squared will account for that problem by introducing a “penalty” for every additional variable. Mathematically, it looks like this:</p>
<p><span class="math display">\[ Adj.R^2 = 1 - \frac{(1 - R^2)*(N - 1)}{N - k - 1} \]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(R^2\)</span> is our classical R-squared calculated (<span class="math inline">\(\frac{TSS}{ESS}\)</span>)</p></li>
<li><p><span class="math inline">\(N\)</span> is the number of observations in our sample</p></li>
<li><p><span class="math inline">\(k\)</span> is the number of independent variables</p></li>
</ul>
<p>In <code>R</code>, we can extract the adjusted R-squared simply from our model in chunk, multivariate regression:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="regression-diagnostics.html#cb60-1" tabindex="-1"></a><span class="co">#running multivariate model</span></span>
<span id="cb60-2"><a href="regression-diagnostics.html#cb60-2" tabindex="-1"></a>multivariate_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> categorical_variable, <span class="at">data =</span> df)</span>
<span id="cb60-3"><a href="regression-diagnostics.html#cb60-3" tabindex="-1"></a></span>
<span id="cb60-4"><a href="regression-diagnostics.html#cb60-4" tabindex="-1"></a><span class="co">#Getting summary</span></span>
<span id="cb60-5"><a href="regression-diagnostics.html#cb60-5" tabindex="-1"></a><span class="fu">summary</span>(multivariate_model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x + categorical_variable, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5183 -1.3840 -0.5014  1.2393  5.5808 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             1.9799     1.0679   1.854   0.0747 .  
## x                       1.5557     0.1621   9.596 3.42e-10 ***
## categorical_variable1  -1.0667     0.9637  -1.107   0.2781    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.533 on 27 degrees of freedom
## Multiple R-squared:  0.7734, Adjusted R-squared:  0.7566 
## F-statistic: 46.07 on 2 and 27 DF,  p-value: 1.982e-09</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="regression-diagnostics.html#cb62-1" tabindex="-1"></a><span class="co">#Extract Adjusted R-squared</span></span>
<span id="cb62-2"><a href="regression-diagnostics.html#cb62-2" tabindex="-1"></a><span class="fu">summary</span>(multivariate_model)<span class="sc">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.7565721</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="regression-diagnostics.html#cb64-1" tabindex="-1"></a><span class="co">#Calculating by hand</span></span>
<span id="cb64-2"><a href="regression-diagnostics.html#cb64-2" tabindex="-1"></a>adj_r_squared <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> (((<span class="dv">1</span><span class="sc">-</span><span class="fu">summary</span>(multivariate_model)<span class="sc">$</span>r.squared) <span class="sc">*</span> (<span class="fu">nrow</span>(df) <span class="sc">-</span> <span class="dv">1</span>))<span class="sc">/</span>(<span class="fu">nrow</span>(df) <span class="sc">-</span> <span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb64-3"><a href="regression-diagnostics.html#cb64-3" tabindex="-1"></a></span>
<span id="cb64-4"><a href="regression-diagnostics.html#cb64-4" tabindex="-1"></a><span class="co">#printing it</span></span>
<span id="cb64-5"><a href="regression-diagnostics.html#cb64-5" tabindex="-1"></a><span class="fu">print</span>(adj_r_squared)</span></code></pre></div>
<pre><code>## [1] 0.7565721</code></pre>
</div>
<div id="omitted-variable-bias" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Omitted Variable Bias<a href="regression-diagnostics.html#omitted-variable-bias" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I already mentioned one (abstract) reason why we should include other variables in our model. But there is more to it: You could find effects between two variables X and Y, but it could be that in Reality there is not an association. For example, let us say you collect data about ice cream and shark attacks. Ice cream sales is your independent variable and you want to explain the number of shark attacks, here is your data:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="regression-diagnostics.html#cb66-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility </span></span>
<span id="cb66-2"><a href="regression-diagnostics.html#cb66-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)  </span>
<span id="cb66-3"><a href="regression-diagnostics.html#cb66-3" tabindex="-1"></a></span>
<span id="cb66-4"><a href="regression-diagnostics.html#cb66-4" tabindex="-1"></a><span class="co"># Number of data points </span></span>
<span id="cb66-5"><a href="regression-diagnostics.html#cb66-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb66-6"><a href="regression-diagnostics.html#cb66-6" tabindex="-1"></a></span>
<span id="cb66-7"><a href="regression-diagnostics.html#cb66-7" tabindex="-1"></a><span class="co"># Simulate diet data (assuming a normal distribution) </span></span>
<span id="cb66-8"><a href="regression-diagnostics.html#cb66-8" tabindex="-1"></a>temperature <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">1500</span>, <span class="at">sd =</span> <span class="dv">200</span>)  </span>
<span id="cb66-9"><a href="regression-diagnostics.html#cb66-9" tabindex="-1"></a></span>
<span id="cb66-10"><a href="regression-diagnostics.html#cb66-10" tabindex="-1"></a><span class="co"># Simulate exercise data (assuming a normal distribution) </span></span>
<span id="cb66-11"><a href="regression-diagnostics.html#cb66-11" tabindex="-1"></a>ice_cream_sales <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">3</span>, <span class="at">sd =</span> <span class="dv">1</span>)  </span>
<span id="cb66-12"><a href="regression-diagnostics.html#cb66-12" tabindex="-1"></a></span>
<span id="cb66-13"><a href="regression-diagnostics.html#cb66-13" tabindex="-1"></a><span class="co"># Simulate weight loss data </span></span>
<span id="cb66-14"><a href="regression-diagnostics.html#cb66-14" tabindex="-1"></a>violence_crime_true <span class="ot">&lt;-</span> <span class="fl">0.2</span> <span class="sc">*</span> temperature <span class="sc">-</span> </span>
<span id="cb66-15"><a href="regression-diagnostics.html#cb66-15" tabindex="-1"></a>  <span class="fl">0.5</span> <span class="sc">*</span> ice_cream_sales <span class="sc">+</span> </span>
<span id="cb66-16"><a href="regression-diagnostics.html#cb66-16" tabindex="-1"></a>  <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">5</span>) </span>
<span id="cb66-17"><a href="regression-diagnostics.html#cb66-17" tabindex="-1"></a></span>
<span id="cb66-18"><a href="regression-diagnostics.html#cb66-18" tabindex="-1"></a><span class="co"># Create a data frame </span></span>
<span id="cb66-19"><a href="regression-diagnostics.html#cb66-19" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">temperature =</span> temperature,<span class="at">ice_cream_sales =</span> ice_cream_sales,                     <span class="at">violence_crime_true =</span> violence_crime_true)  </span>
<span id="cb66-20"><a href="regression-diagnostics.html#cb66-20" tabindex="-1"></a></span>
<span id="cb66-21"><a href="regression-diagnostics.html#cb66-21" tabindex="-1"></a><span class="co"># Fit a model without including the diet variable </span></span>
<span id="cb66-22"><a href="regression-diagnostics.html#cb66-22" tabindex="-1"></a>model_without_temperature <span class="ot">&lt;-</span> <span class="fu">lm</span>(violence_crime_true <span class="sc">~</span> ice_cream_sales, <span class="at">data =</span> data)</span>
<span id="cb66-23"><a href="regression-diagnostics.html#cb66-23" tabindex="-1"></a></span>
<span id="cb66-24"><a href="regression-diagnostics.html#cb66-24" tabindex="-1"></a><span class="co">#Fit a model with only the temperature variable</span></span>
<span id="cb66-25"><a href="regression-diagnostics.html#cb66-25" tabindex="-1"></a></span>
<span id="cb66-26"><a href="regression-diagnostics.html#cb66-26" tabindex="-1"></a>model_with_only_temperature <span class="ot">&lt;-</span> <span class="fu">lm</span>(violence_crime_true <span class="sc">~</span>temperature,                        <span class="at">data =</span> data)</span>
<span id="cb66-27"><a href="regression-diagnostics.html#cb66-27" tabindex="-1"></a></span>
<span id="cb66-28"><a href="regression-diagnostics.html#cb66-28" tabindex="-1"></a><span class="co"># Fit a model including both diet and exercise variables </span></span>
<span id="cb66-29"><a href="regression-diagnostics.html#cb66-29" tabindex="-1"></a>model_with_temperature <span class="ot">&lt;-</span> <span class="fu">lm</span>(violence_crime_true <span class="sc">~</span> ice_cream_sales <span class="sc">+</span> temperature,                        <span class="at">data =</span> data)  </span>
<span id="cb66-30"><a href="regression-diagnostics.html#cb66-30" tabindex="-1"></a></span>
<span id="cb66-31"><a href="regression-diagnostics.html#cb66-31" tabindex="-1"></a><span class="co"># Output the summary of both models </span></span>
<span id="cb66-32"><a href="regression-diagnostics.html#cb66-32" tabindex="-1"></a><span class="fu">summary</span>(model_without_temperature) </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = violence_crime_true ~ ice_cream_sales, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -94.788 -23.736  -2.348  22.264  98.754 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      287.681     11.623  24.751   &lt;2e-16 ***
## ice_cream_sales    4.090      3.741   1.093    0.277    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 35.94 on 98 degrees of freedom
## Multiple R-squared:  0.01205,    Adjusted R-squared:  0.001969 
## F-statistic: 1.195 on 1 and 98 DF,  p-value: 0.2769</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="regression-diagnostics.html#cb68-1" tabindex="-1"></a><span class="fu">summary</span>(model_with_only_temperature)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = violence_crime_true ~ temperature, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.9969  -3.8454   0.1718   2.9121  11.7502 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -3.535495   4.576293  -0.773    0.442    
## temperature  0.201591   0.003021  66.727   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.307 on 98 degrees of freedom
## Multiple R-squared:  0.9785, Adjusted R-squared:  0.9782 
## F-statistic:  4452 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="regression-diagnostics.html#cb70-1" tabindex="-1"></a><span class="fu">summary</span>(model_with_temperature)  </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = violence_crime_true ~ ice_cream_sales + temperature, 
##     data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.4770  -3.6734  -0.0914   2.9449  12.1843 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -2.396657   4.694786  -0.510    0.611    
## ice_cream_sales -0.596143   0.556470  -1.071    0.287    
## temperature      0.202005   0.003043  66.373   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.303 on 97 degrees of freedom
## Multiple R-squared:  0.9787, Adjusted R-squared:  0.9783 
## F-statistic:  2230 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="regression-diagnostics.html#cb72-1" tabindex="-1"></a><span class="co">#Let us display both models next to each other</span></span>
<span id="cb72-2"><a href="regression-diagnostics.html#cb72-2" tabindex="-1"></a><span class="co">#EDIT: I created this function specifically, the code for the function is at the top.   </span></span>
<span id="cb72-3"><a href="regression-diagnostics.html#cb72-3" tabindex="-1"></a><span class="fu">table_ovb</span>(model_without_temperature, model_with_temperature)</span></code></pre></div>
<table class="table" style="color: black; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Model without Temperature
</th>
<th style="text-align:right;">
Model with Temperature
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
287.680876
</td>
<td style="text-align:right;">
-2.3966572
</td>
</tr>
<tr>
<td style="text-align:left;">
Ice Cream Sales
</td>
<td style="text-align:right;">
4.090356
</td>
<td style="text-align:right;">
-0.5961434
</td>
</tr>
<tr>
<td style="text-align:left;">
Temperature
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
0.2020051
</td>
</tr>
</tbody>
</table>
<p>We can see that the coefficient changes dramatically. What happened? Well, one important assumption of linear regression is that the error term captures all variance not explained by our model and is <strong>not correlated</strong> with the independent variable(s) nor the dependent variable. But if there is unexplained variation in our model that is correlated with our independent variable, then this assumption is violated. In our example, we can see that ice cream sales coefficient in the first model is biased, because ice cream sales and is correlated to temperature. The warmer it gets, the more ice cream is sold. But, the warmer it gets, the more violent people get, therefore we have an omitted variable and that is temperature. When we include temperature in the model, we see the problem of omitted variable bias: It biases our coefficients, by either overestimating (like in our example) or by underestimating it. What we should do in such a case, is to delete the omitted variable, which is the drastically changing variable (ice cream sales in our case). This is also the reason, why people talk about additional variables as control variables in a multiple linear model. This way you can control if an association between two variables is due to omitted variable bias or other variables, which can explain the variation better.</p>
</div>
<div id="multicollinearity" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Multicollinearity<a href="regression-diagnostics.html#multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another, and I promise, the last OLS assumption, which has to tested is that <strong>there is no Multicollinearity</strong>. The concept is simple: The independent variables should not be correlated. In our previous example, ice cream sales and temperature were correlated. This would have hurt these assumption. In strong cases, multicollinearity can bias our estimates, so that they gain statistical significance and lead us to wrong conclusions. Let us look at an obvious example. You want to find out how the grades of children is affected by different factors. You choose 2 factors: The time they spent on doing their homework (<em>learning time</em>) and the time they spent on playing video games (<em>gaming time</em>). The systematic component looks like this:</p>
<p><span class="math display">\[
Grades_i = \beta_0 + \beta_1*\text{learning time}_i + \beta_2 *\text{gaming time}_i + \epsilon_i
\]</span></p>
<p>Let us compute them:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="regression-diagnostics.html#cb73-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb73-2"><a href="regression-diagnostics.html#cb73-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb73-3"><a href="regression-diagnostics.html#cb73-3" tabindex="-1"></a></span>
<span id="cb73-4"><a href="regression-diagnostics.html#cb73-4" tabindex="-1"></a><span class="co"># Number of samples</span></span>
<span id="cb73-5"><a href="regression-diagnostics.html#cb73-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb73-6"><a href="regression-diagnostics.html#cb73-6" tabindex="-1"></a></span>
<span id="cb73-7"><a href="regression-diagnostics.html#cb73-7" tabindex="-1"></a><span class="co"># True coefficients</span></span>
<span id="cb73-8"><a href="regression-diagnostics.html#cb73-8" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="dv">80</span></span>
<span id="cb73-9"><a href="regression-diagnostics.html#cb73-9" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> <span class="fl">1.5</span></span>
<span id="cb73-10"><a href="regression-diagnostics.html#cb73-10" tabindex="-1"></a>beta_2 <span class="ot">&lt;-</span> <span class="fl">1.5</span></span>
<span id="cb73-11"><a href="regression-diagnostics.html#cb73-11" tabindex="-1"></a></span>
<span id="cb73-12"><a href="regression-diagnostics.html#cb73-12" tabindex="-1"></a><span class="co"># Generate independent variables</span></span>
<span id="cb73-13"><a href="regression-diagnostics.html#cb73-13" tabindex="-1"></a>learning_time <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb73-14"><a href="regression-diagnostics.html#cb73-14" tabindex="-1"></a>gaming_time <span class="ot">&lt;-</span> <span class="fl">0.7</span> <span class="sc">*</span> learning_time <span class="sc">+</span> </span>
<span id="cb73-15"><a href="regression-diagnostics.html#cb73-15" tabindex="-1"></a>  <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.7</span><span class="sc">^</span><span class="dv">2</span>) <span class="sc">*</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="dv">1</span>) </span>
<span id="cb73-16"><a href="regression-diagnostics.html#cb73-16" tabindex="-1"></a></span>
<span id="cb73-17"><a href="regression-diagnostics.html#cb73-17" tabindex="-1"></a><span class="co">#generate error term</span></span>
<span id="cb73-18"><a href="regression-diagnostics.html#cb73-18" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">3</span>)</span>
<span id="cb73-19"><a href="regression-diagnostics.html#cb73-19" tabindex="-1"></a></span>
<span id="cb73-20"><a href="regression-diagnostics.html#cb73-20" tabindex="-1"></a><span class="co"># Generate grades</span></span>
<span id="cb73-21"><a href="regression-diagnostics.html#cb73-21" tabindex="-1"></a>grades <span class="ot">&lt;-</span> beta_0 <span class="sc">+</span> beta_1 <span class="sc">*</span> learning_time <span class="sc">+</span> beta_2 <span class="sc">*</span> gaming_time <span class="sc">+</span> epsilon</span>
<span id="cb73-22"><a href="regression-diagnostics.html#cb73-22" tabindex="-1"></a></span>
<span id="cb73-23"><a href="regression-diagnostics.html#cb73-23" tabindex="-1"></a><span class="co"># Create a data frame</span></span>
<span id="cb73-24"><a href="regression-diagnostics.html#cb73-24" tabindex="-1"></a>df_grades <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(learning_time,</span>
<span id="cb73-25"><a href="regression-diagnostics.html#cb73-25" tabindex="-1"></a>                   gaming_time,</span>
<span id="cb73-26"><a href="regression-diagnostics.html#cb73-26" tabindex="-1"></a>                   grades)</span>
<span id="cb73-27"><a href="regression-diagnostics.html#cb73-27" tabindex="-1"></a></span>
<span id="cb73-28"><a href="regression-diagnostics.html#cb73-28" tabindex="-1"></a><span class="co"># Display first few rows of the data frame</span></span>
<span id="cb73-29"><a href="regression-diagnostics.html#cb73-29" tabindex="-1"></a>grades_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(grades <span class="sc">~</span> learning_time <span class="sc">+</span> gaming_time, </span>
<span id="cb73-30"><a href="regression-diagnostics.html#cb73-30" tabindex="-1"></a>                   <span class="at">data =</span> df_grades)</span>
<span id="cb73-31"><a href="regression-diagnostics.html#cb73-31" tabindex="-1"></a></span>
<span id="cb73-32"><a href="regression-diagnostics.html#cb73-32" tabindex="-1"></a><span class="co">#Getting the summary</span></span>
<span id="cb73-33"><a href="regression-diagnostics.html#cb73-33" tabindex="-1"></a><span class="fu">summary</span>(grades_model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = grades ~ learning_time + gaming_time, data = df_grades)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.2868 -1.9842 -0.0991  1.9482  6.2446 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    80.1718     0.6668 120.231  &lt; 2e-16 ***
## learning_time   1.2665     0.3335   3.798 0.000255 ***
## gaming_time     1.7860     0.4312   4.142 7.36e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.822 on 97 degrees of freedom
## Multiple R-squared:  0.8661, Adjusted R-squared:  0.8634 
## F-statistic: 313.8 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>By looking at the model, we could conclude that the more a student learns, the better its grades on average, holding all else constant. So far, so clear, but the same goes for gaming time. Why is that the case? Because if we think that a student has per day 3 hours, which the student can assign to either learning or gaming, than both are correlated, because assigning 2 hours to learning means 1 hour for gaming, 0.5 hours for learning means 2.5 hours for gaming and so forth. This means both coefficients are explaining each other and bias each other. How to detect them?</p>
<div id="testing-correlations-to-each-other" class="section level4 hasAnchor" number="8.2.3.1">
<h4><span class="header-section-number">8.2.3.1</span> Testing Correlations to each other<a href="regression-diagnostics.html#testing-correlations-to-each-other" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The first technique is to check the correlations of the variables to each other beforehand. You can do that two-ways: Just print out a correlation table:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="regression-diagnostics.html#cb75-1" tabindex="-1"></a><span class="co">#First store the variables you need in a seperate data frame </span></span>
<span id="cb75-2"><a href="regression-diagnostics.html#cb75-2" tabindex="-1"></a>cormatrix_data <span class="ot">&lt;-</span> df_grades <span class="sc">%&gt;%</span> </span>
<span id="cb75-3"><a href="regression-diagnostics.html#cb75-3" tabindex="-1"></a>  <span class="fu">select</span>(learning_time, gaming_time)</span>
<span id="cb75-4"><a href="regression-diagnostics.html#cb75-4" tabindex="-1"></a></span>
<span id="cb75-5"><a href="regression-diagnostics.html#cb75-5" tabindex="-1"></a></span>
<span id="cb75-6"><a href="regression-diagnostics.html#cb75-6" tabindex="-1"></a><span class="co">#Second, calculate the table, the 2 at the end are the dimensions</span></span>
<span id="cb75-7"><a href="regression-diagnostics.html#cb75-7" tabindex="-1"></a>cormatrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(cormatrix_data) <span class="co">#Calculate the correlations</span></span>
<span id="cb75-8"><a href="regression-diagnostics.html#cb75-8" tabindex="-1"></a><span class="fu">round</span>(cormatrix, <span class="dv">2</span>) <span class="co">#round it to the second digit and display it </span></span></code></pre></div>
<pre><code>##               learning_time gaming_time
## learning_time          1.00        0.95
## gaming_time            0.95        1.00</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="regression-diagnostics.html#cb77-1" tabindex="-1"></a><span class="co">#We could have also done this code in one step </span></span>
<span id="cb77-2"><a href="regression-diagnostics.html#cb77-2" tabindex="-1"></a></span>
<span id="cb77-3"><a href="regression-diagnostics.html#cb77-3" tabindex="-1"></a><span class="co">#df_grades %&gt;% </span></span>
<span id="cb77-4"><a href="regression-diagnostics.html#cb77-4" tabindex="-1"></a><span class="co">#  select(learning_time, gaming_time) %&gt;% </span></span>
<span id="cb77-5"><a href="regression-diagnostics.html#cb77-5" tabindex="-1"></a><span class="co">#  cor() %&gt;% </span></span>
<span id="cb77-6"><a href="regression-diagnostics.html#cb77-6" tabindex="-1"></a><span class="co">#  round(2) </span></span></code></pre></div>
<p>We can see that the correlation between both variables is way to high. Now, with only two variables the table works fine, but what if we have, let us say, 20 variables? It could get messy, therefore you could also use the correlation matrix, which I introduced in the chapter before. In this case, it does not make sense, since it would just print out one block. But keep it nevertheless in mind for the future.</p>
</div>
<div id="variance-of-inflation-vif" class="section level4 hasAnchor" number="8.2.3.2">
<h4><span class="header-section-number">8.2.3.2</span> Variance of Inflation (VIF)<a href="regression-diagnostics.html#variance-of-inflation-vif" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Another measure for multicolinearity and probably the most famous one, is the Variance-of-Inflation (VIF) factor. Intuitively spoken, this measure fits models with multiple variables, by calculating the variance of each variable. Then it fits a model with only one independent variable and calculates the variance of it. The result is a measure that displays high values if the variance of a variable increases, when other variables are added. That is exactly what this measure does.. The formula of it is really simple:</p>
<p><span class="math display">\[
VIF_i = \frac{1}{1 - R^2_i}
\]</span></p>
<p>where, the Variance of inflation (VIF) of variable <em>i</em> is calculated by 1 divided by 1 - the R-squared (<span class="math inline">\(R^2_i\)</span>) of the regression with only that variable.</p>
<p>In <code>R</code>, we can use the <code>VIF()</code> function from the <code>car</code> - package to do it.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="regression-diagnostics.html#cb78-1" tabindex="-1"></a><span class="co">#We only have to use the function VIF() on our model </span></span>
<span id="cb78-2"><a href="regression-diagnostics.html#cb78-2" tabindex="-1"></a><span class="fu">vif</span>(grades_model)</span></code></pre></div>
<pre><code>## learning_time   gaming_time 
##      10.21358      10.21358</code></pre>
<p>The rule is that if a value exceeds 10, it is considered critical. We should always test for multicolinearity, and if we detect it, run the regression separately without the correlated variables.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="further-explanations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="solutions-exercises.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": {}
},
"fontsettings": null,
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
