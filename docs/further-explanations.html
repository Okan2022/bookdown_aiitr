<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Further Explanations | An intuitive Introduction to R</title>
  <meta name="description" content="Chapter 7 Further Explanations | An intuitive Introduction to R" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Further Explanations | An intuitive Introduction to R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Further Explanations | An intuitive Introduction to R" />
  
  
  

<meta name="author" content="Okan Sarioglu" />


<meta name="date" content="2025-05-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="loops-and-functions.html"/>
<link rel="next" href="solutions-exercises.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/proj4-2.6.2/proj4.min.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.2.2/leaflet.js"></script>
<script src="libs/leaflet-providers-2.0.0/leaflet-providers_2.0.0.js"></script>
<script src="libs/leaflet-providers-plugin-2.2.2/leaflet-providers-plugin.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<div style="text-align: center;">
  <img src="images/AIITR.svg" alt="AIITR Logo" style="vertical-align: middle; margin-left: 5px; width: 125px; height: auto;">
</div>
<div style="text-align: center;">
<li style="list-style: none;"><a href="./">by Okan Sarioglu</a></li>
</div>
<div style="text-align: center; margin-top: 10px;">
  <a href="https://github.com/Okan2022" target="_blank">
    <img src="images/github-mark.svg" alt="GitHub" style="vertical-align: middle; width: 25px; height: auto;">
  </a>
  <a href="https://www.linkedin.com/in/osarioglu" target="_blank" style="margin-left: 10px;">
    <img src="images/LinkedIn_icon.svg" alt="LinkedIn" style="vertical-align: middle; width: 25px; height: auto;">
  </a>
</div> 
<div style="text-align: center; margin-top: 10px; margin-bottom: 10px;">
  <a href="aiitr_script.R" download="aiitr_script.R" style="display: inline-block; padding: 10px 20px; font-size: 16px; color: #fff; background-color: #4169e1; text-decoration: none; border-radius: 8px;">Download R Script</a>
</div>
<div style="text-align: center; margin-top: 10px; margin-bottom: 10px;">
  <a href="https://www.paypal.com/donate?hosted_button_id=ZBMC2WD84P75S"
     style="display: inline-block; padding: 10px 20px; font-size: 16px; color: #fff; background-color: #FCBB32; text-decoration: none; border-radius: 8px;">
     <img src="images/pp_logo.png" alt="PayPal" style="height: 24px; vertical-align: middle; margin-right: 8px;">
     Support me here!
  </a>
</div>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome !</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-course"><i class="fa fa-check"></i>About this Course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-me"><i class="fa fa-check"></i>About Me</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals.html"><a href="fundamentals.html"><i class="fa fa-check"></i><b>1</b> Fundamentals</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals.html"><a href="fundamentals.html#getting-familiar-with-rstudio-and-establishing-a-workflow"><i class="fa fa-check"></i><b>1.1</b> Getting familiar with RStudio and establishing a Workflow</a></li>
<li class="chapter" data-level="1.2" data-path="fundamentals.html"><a href="fundamentals.html#lets-get-started-r-as-a-fancy-calculator"><i class="fa fa-check"></i><b>1.2</b> Lets get started: R as a fancy calculator</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals.html"><a href="fundamentals.html#ifelse-statements-and-the-ifelse-function"><i class="fa fa-check"></i><b>1.3</b> <code>ifelse</code> statements and the <code>ifelse()</code> function</a></li>
<li class="chapter" data-level="1.4" data-path="fundamentals.html"><a href="fundamentals.html#outlook"><i class="fa fa-check"></i><b>1.4</b> Outlook</a></li>
<li class="chapter" data-level="1.5" data-path="fundamentals.html"><a href="fundamentals.html#exercise-section"><i class="fa fa-check"></i><b>1.5</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>2</b> Data Manipulation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-manipulation.html"><a href="data-manipulation.html#packages"><i class="fa fa-check"></i><b>2.1</b> Packages</a></li>
<li class="chapter" data-level="2.2" data-path="data-manipulation.html"><a href="data-manipulation.html#working-with-packages"><i class="fa fa-check"></i><b>2.2</b> Working with packages</a></li>
<li class="chapter" data-level="2.3" data-path="data-manipulation.html"><a href="data-manipulation.html#the-data-we-will-work-with-the-european-social-survey-ess"><i class="fa fa-check"></i><b>2.3</b> The Data we will work with: The European Social Survey (ESS)</a></li>
<li class="chapter" data-level="2.4" data-path="data-manipulation.html"><a href="data-manipulation.html#lets-wrangle-the-data-the-dplyr-package"><i class="fa fa-check"></i><b>2.4</b> Let’s wrangle the data: The <code>dplyr</code> package</a></li>
<li class="chapter" data-level="2.5" data-path="data-manipulation.html"><a href="data-manipulation.html#merging-datasets"><i class="fa fa-check"></i><b>2.5</b> Merging Datasets</a></li>
<li class="chapter" data-level="2.6" data-path="data-manipulation.html"><a href="data-manipulation.html#outlook-1"><i class="fa fa-check"></i><b>2.6</b> Outlook</a></li>
<li class="chapter" data-level="2.7" data-path="data-manipulation.html"><a href="data-manipulation.html#exercise-section-1"><i class="fa fa-check"></i><b>2.7</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-visualisation.html"><a href="data-visualisation.html"><i class="fa fa-check"></i><b>3</b> Data Visualisation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-visualisation.html"><a href="data-visualisation.html#introduction-to-ggplot2"><i class="fa fa-check"></i><b>3.1</b> Introduction to <code>ggplot2</code></a></li>
<li class="chapter" data-level="3.2" data-path="data-visualisation.html"><a href="data-visualisation.html#distributions-histogram-density-plots-and-boxplots"><i class="fa fa-check"></i><b>3.2</b> Distributions: Histogram, Density Plots, and Boxplots</a></li>
<li class="chapter" data-level="3.3" data-path="data-visualisation.html"><a href="data-visualisation.html#ranking-barplot"><i class="fa fa-check"></i><b>3.3</b> Ranking: Barplot</a></li>
<li class="chapter" data-level="3.4" data-path="data-visualisation.html"><a href="data-visualisation.html#evolution-line-chart"><i class="fa fa-check"></i><b>3.4</b> Evolution: Line Chart</a></li>
<li class="chapter" data-level="3.5" data-path="data-visualisation.html"><a href="data-visualisation.html#correlation-scatterplots"><i class="fa fa-check"></i><b>3.5</b> Correlation: Scatterplots</a></li>
<li class="chapter" data-level="3.6" data-path="data-visualisation.html"><a href="data-visualisation.html#making-plots-with-facet_wrap-and-facet_grid"><i class="fa fa-check"></i><b>3.6</b> Making Plots with <code>facet_wrap()</code> and <code>facet_grid()</code></a></li>
<li class="chapter" data-level="3.7" data-path="data-visualisation.html"><a href="data-visualisation.html#outlook-2"><i class="fa fa-check"></i><b>3.7</b> Outlook</a></li>
<li class="chapter" data-level="3.8" data-path="data-visualisation.html"><a href="data-visualisation.html#outlook-3"><i class="fa fa-check"></i><b>3.8</b> Outlook</a></li>
<li class="chapter" data-level="3.9" data-path="data-visualisation.html"><a href="data-visualisation.html#exercise-section-2"><i class="fa fa-check"></i><b>3.9</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>4</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#standard-descriptive-statistics"><i class="fa fa-check"></i><b>4.1</b> Standard Descriptive Statistics</a></li>
<li class="chapter" data-level="4.2" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#working-with-eda-packages"><i class="fa fa-check"></i><b>4.2</b> Working with EDA packages</a></li>
<li class="chapter" data-level="4.3" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#conclusion"><i class="fa fa-check"></i><b>4.3</b> Conclusion</a></li>
<li class="chapter" data-level="4.4" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#exercise-section-3"><i class="fa fa-check"></i><b>4.4</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-analysis.html"><a href="data-analysis.html"><i class="fa fa-check"></i><b>5</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="data-analysis.html"><a href="data-analysis.html#linear-regression"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="data-analysis.html"><a href="data-analysis.html#hypothesis-testing-in-r"><i class="fa fa-check"></i><b>5.2</b> Hypothesis Testing in R</a></li>
<li class="chapter" data-level="5.3" data-path="data-analysis.html"><a href="data-analysis.html#multivariate-regression"><i class="fa fa-check"></i><b>5.3</b> Multivariate Regression</a></li>
<li class="chapter" data-level="5.4" data-path="data-analysis.html"><a href="data-analysis.html#categorical-variables"><i class="fa fa-check"></i><b>5.4</b> Categorical Variables</a></li>
<li class="chapter" data-level="5.5" data-path="data-analysis.html"><a href="data-analysis.html#outlook-4"><i class="fa fa-check"></i><b>5.5</b> Outlook</a></li>
<li class="chapter" data-level="5.6" data-path="data-analysis.html"><a href="data-analysis.html#exercise-section-4"><i class="fa fa-check"></i><b>5.6</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="loops-and-functions.html"><a href="loops-and-functions.html"><i class="fa fa-check"></i><b>6</b> Loops and Functions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="loops-and-functions.html"><a href="loops-and-functions.html#loops"><i class="fa fa-check"></i><b>6.1</b> Loops</a></li>
<li class="chapter" data-level="6.2" data-path="loops-and-functions.html"><a href="loops-and-functions.html#apply-function-family"><i class="fa fa-check"></i><b>6.2</b> <code>apply()</code> Function Family</a></li>
<li class="chapter" data-level="6.3" data-path="loops-and-functions.html"><a href="loops-and-functions.html#writing-your-own-functions"><i class="fa fa-check"></i><b>6.3</b> Writing your own functions</a></li>
<li class="chapter" data-level="6.4" data-path="loops-and-functions.html"><a href="loops-and-functions.html#outlook-5"><i class="fa fa-check"></i><b>6.4</b> Outlook</a></li>
<li class="chapter" data-level="6.5" data-path="loops-and-functions.html"><a href="loops-and-functions.html#exercise-section-5"><i class="fa fa-check"></i><b>6.5</b> Exercise Section</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-explanations.html"><a href="further-explanations.html"><i class="fa fa-check"></i><b>7</b> Further Explanations</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-explanations.html"><a href="further-explanations.html#probability-theory"><i class="fa fa-check"></i><b>7.1</b> Probability Theory</a></li>
<li class="chapter" data-level="7.2" data-path="further-explanations.html"><a href="further-explanations.html#regression-diagnostics"><i class="fa fa-check"></i><b>7.2</b> Regression Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="solutions-exercises.html"><a href="solutions-exercises.html"><i class="fa fa-check"></i><b>8</b> Solutions Exercises</a>
<ul>
<li class="chapter" data-level="8.1" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-1-fundamentals"><i class="fa fa-check"></i><b>8.1</b> Chapter 1: Fundamentals</a></li>
<li class="chapter" data-level="8.2" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-2-data-manipulation"><i class="fa fa-check"></i><b>8.2</b> Chapter 2: Data Manipulation</a></li>
<li class="chapter" data-level="8.3" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-3-data-visualisation"><i class="fa fa-check"></i><b>8.3</b> Chapter 3: Data Visualisation</a></li>
<li class="chapter" data-level="8.4" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-4-exploratory-data-analysis"><i class="fa fa-check"></i><b>8.4</b> Chapter 4: Exploratory Data Analysis</a></li>
<li class="chapter" data-level="8.5" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-5-data-analysis"><i class="fa fa-check"></i><b>8.5</b> Chapter 5: Data Analysis</a></li>
<li class="chapter" data-level="8.6" data-path="solutions-exercises.html"><a href="solutions-exercises.html#chapter-6-loops-and-functions"><i class="fa fa-check"></i><b>8.6</b> Chapter 6: Loops and Functions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An intuitive Introduction to R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="further-explanations" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Further Explanations<a href="further-explanations.html#further-explanations" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this Chapter we start into the main strength of R: Data Analysis. After cleaning our data we are ready to analyze it, but it is always recommend to get an overview of our data with exploratory data analysis. This yields the advantage that we can detect first trends and potential problems with our data. Furthermore, we will get a better understand of the Data Generating Process and how data is generated.</p>
<p>Let us tart with loading packages and you will see, that we are loading a lot of packages due to the fact, that R is a statistical analysis software and therefore it is only natural that Statisticians and Data Scientists, who use R, are also writing packages to (1) make their own life easier, but also (2) further developing R.</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb429-1"><a href="further-explanations.html#cb429-1" tabindex="-1"></a>pacman<span class="sc">::</span><span class="fu">p_load</span>(<span class="st">&quot;dplyr&quot;</span>, <span class="st">&quot;tidyr&quot;</span>, <span class="st">&quot;ggpubr&quot;</span>, <span class="st">&quot;gapminder&quot;</span>,</span>
<span id="cb429-2"><a href="further-explanations.html#cb429-2" tabindex="-1"></a>               <span class="st">&quot;kableExtra&quot;</span>, <span class="st">&quot;car&quot;</span>)</span></code></pre></div>
<p>Defining Function for later:</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="further-explanations.html#cb430-1" tabindex="-1"></a><span class="do">###Defining a function</span></span>
<span id="cb430-2"><a href="further-explanations.html#cb430-2" tabindex="-1"></a><span class="co">#Creating Function for Kable Table </span></span>
<span id="cb430-3"><a href="further-explanations.html#cb430-3" tabindex="-1"></a>table_ovb <span class="ot">&lt;-</span> <span class="cf">function</span>(model1, model2) {</span>
<span id="cb430-4"><a href="further-explanations.html#cb430-4" tabindex="-1"></a>  <span class="co"># Determine the maximum number of coefficients between the two models</span></span>
<span id="cb430-5"><a href="further-explanations.html#cb430-5" tabindex="-1"></a>  max_coef <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">length</span>(model1<span class="sc">$</span>coefficients), <span class="fu">length</span>(model2<span class="sc">$</span>coefficients))</span>
<span id="cb430-6"><a href="further-explanations.html#cb430-6" tabindex="-1"></a>  </span>
<span id="cb430-7"><a href="further-explanations.html#cb430-7" tabindex="-1"></a>  <span class="co"># Initialize placeholders for coefficients</span></span>
<span id="cb430-8"><a href="further-explanations.html#cb430-8" tabindex="-1"></a>  place_holder1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, max_coef)</span>
<span id="cb430-9"><a href="further-explanations.html#cb430-9" tabindex="-1"></a>  place_holder2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, max_coef)</span>
<span id="cb430-10"><a href="further-explanations.html#cb430-10" tabindex="-1"></a>  </span>
<span id="cb430-11"><a href="further-explanations.html#cb430-11" tabindex="-1"></a>  <span class="co"># Replace the placeholders with coefficients from the models</span></span>
<span id="cb430-12"><a href="further-explanations.html#cb430-12" tabindex="-1"></a>  place_holder1[<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(model1<span class="sc">$</span>coefficients)] <span class="ot">&lt;-</span> model1<span class="sc">$</span>coefficients</span>
<span id="cb430-13"><a href="further-explanations.html#cb430-13" tabindex="-1"></a>  place_holder2[<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(model2<span class="sc">$</span>coefficients)] <span class="ot">&lt;-</span> model2<span class="sc">$</span>coefficients</span>
<span id="cb430-14"><a href="further-explanations.html#cb430-14" tabindex="-1"></a>  </span>
<span id="cb430-15"><a href="further-explanations.html#cb430-15" tabindex="-1"></a>  <span class="co"># Create a data frame with coefficients from both models</span></span>
<span id="cb430-16"><a href="further-explanations.html#cb430-16" tabindex="-1"></a>  dt <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(place_holder1,place_holder2) </span>
<span id="cb430-17"><a href="further-explanations.html#cb430-17" tabindex="-1"></a>  </span>
<span id="cb430-18"><a href="further-explanations.html#cb430-18" tabindex="-1"></a>  <span class="co"># Set row and column names</span></span>
<span id="cb430-19"><a href="further-explanations.html#cb430-19" tabindex="-1"></a>  <span class="fu">colnames</span>(dt) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Model without Temperature&quot;</span>, <span class="st">&quot;Model with Temperature&quot;</span>)</span>
<span id="cb430-20"><a href="further-explanations.html#cb430-20" tabindex="-1"></a>  <span class="fu">rownames</span>(dt) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Intercept&quot;</span>, <span class="st">&quot;Ice Cream Sales&quot;</span>, <span class="st">&quot;Temperature&quot;</span>)</span>
<span id="cb430-21"><a href="further-explanations.html#cb430-21" tabindex="-1"></a>  </span>
<span id="cb430-22"><a href="further-explanations.html#cb430-22" tabindex="-1"></a><span class="co"># Display the table</span></span>
<span id="cb430-23"><a href="further-explanations.html#cb430-23" tabindex="-1"></a>  dt <span class="sc">%&gt;%</span></span>
<span id="cb430-24"><a href="further-explanations.html#cb430-24" tabindex="-1"></a>  <span class="fu">kbl</span>() <span class="sc">%&gt;%</span></span>
<span id="cb430-25"><a href="further-explanations.html#cb430-25" tabindex="-1"></a>  <span class="fu">kable_styling</span>()</span>
<span id="cb430-26"><a href="further-explanations.html#cb430-26" tabindex="-1"></a>  </span>
<span id="cb430-27"><a href="further-explanations.html#cb430-27" tabindex="-1"></a>}</span></code></pre></div>
<div id="probability-theory" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Probability Theory<a href="further-explanations.html#probability-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Before diving into Exploratory Data Analysis and Linear Regression, it’s worth taking a moment to look at Probability Theory.</strong> This is crucial because probability forms the foundation of statistics: it allows us to make statements that include uncertainty. Instead of saying something definite like <em>“It will rain tomorrow,”</em> probability allows us to say, <em>“There is a 70% chance of rain tomorrow.”</em> That means you might want to take an umbrella — but there’s still a 30% chance you won’t need it.</p>
<div id="random-variable" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Random Variable<a href="further-explanations.html#random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We are interested in Data! And Data is nothing more than a bunch of <strong>Random Variables.</strong> Technically speaking, a Random Variable is a numerical outcome of a random process or experiment. Informally, let us say you want to collect Data about your family, just image all of them and now randomly pick some information about them, their height, their age, gender, glasses or not, cooking abilities, anything, and now you just need to find a system of assigning a number (e.g. height in cm, age in years,…) and et violà you collected a random variable. Note that a random variable is the mutually exclusive outcome of a random process.</p>
<p>Rolling a dice and getting the result of the dice could be a random variable, let us role the dice and say we got a 3:</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="further-explanations.html#cb431-1" tabindex="-1"></a>dice_role <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb431-2"><a href="further-explanations.html#cb431-2" tabindex="-1"></a><span class="fu">print</span>(dice_role)</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>We now collected our first random variable. On its own, a random variable is not really insightful, but what if we do not collect one random variable, but two? Or three? Or thousand?</p>
<div id="discrete-and-continuous-variables" class="section level4 hasAnchor" number="7.1.1.1">
<h4><span class="header-section-number">7.1.1.1</span> Discrete and Continuous Variables<a href="further-explanations.html#discrete-and-continuous-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Before answering the question above, there is an important distinction to make between the types of random variables we can collect. There are <strong>discrete</strong> and <strong>continuous variables</strong>:</p>
<ul>
<li><p>A <strong>discrete variable</strong> can take on a <strong>finite or countably infinite</strong> set of distinct values, usually integers. These values are <strong>separated</strong> and cannot take on values in between. For example: Number of children, coin flips, number of goals.</p></li>
<li><p>A <strong>continuous variable</strong> can take on <strong>any value</strong> within a given range, including <strong>fractions and decimals</strong>. Between any two values, there are <strong>infinitely many</strong> possible values. For example: Height, weight, temperature, time.</p></li>
</ul>
</div>
<div id="population-vs-sample-distribution" class="section level4 hasAnchor" number="7.1.1.2">
<h4><span class="header-section-number">7.1.1.2</span> Population vs Sample Distribution<a href="further-explanations.html#population-vs-sample-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Another thing to clarify before going on with Probability Distributions, is the difference between Populations and Samples.</p>
<ul>
<li><p><strong>Population</strong>: The population is the entire group we are theoretically interested in studying, based on our research question. For example, if we want to know the <strong>average height of German women</strong>, then the <strong>population</strong> consists of <strong>all German women</strong>. The <strong>population mean</strong> is the true average height of all German women.</p></li>
<li><p><strong>Sample:</strong> Asking every <strong>German woman</strong> (42.3 million, according to the Federal Office of Statistics, 2023) would indeed be impractical. Instead, we use <strong>statistical sampling</strong> to gather data from a smaller, more manageable subset of the population. <strong>Probability theory</strong> helps ensure that this sample is representative of the entire population.</p>
<p>For the sample to be representative of the population, it needs to meet two key criteria:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Sample Size:</strong> The sample must be large enough to accurately reflect the population’s characteristics.</p></li>
<li><p><strong>Randomness:</strong> Every individual in the population must have an equal chance of being selected for the sample. This ensures that the sample is <strong>random</strong> and not biased in any way.</p></li>
</ol>
<p>When these conditions are met, we can confidently generalize the findings from the sample to the larger population. For example, we calculate the <strong>sample mean</strong> (the average of the sample). This is the true value for the sample itself, but we use it to make an inference about the <strong>population mean</strong>.</p>
<p>However, the sample mean will not always be exactly equal to the population mean. This is because of <strong>sampling variability</strong> — there’s always some random variation in the sample that can cause deviations from the population mean.</p>
<p>Despite this, <strong>if the sample is large enough</strong> and <strong>chosen randomly</strong>, the sample mean will tend to be <strong>close</strong> to the population mean on average, and the difference between them decreases as the sample size increases.</p></li>
</ul>
<p>The difference of population and sample is crucial and will be important in the later section of this chapter!</p>
</div>
</div>
<div id="probability-distributions" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Probability Distributions<a href="further-explanations.html#probability-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="probability-mass-functions-pmf" class="section level4 hasAnchor" number="7.1.2.1">
<h4><span class="header-section-number">7.1.2.1</span> Probability Mass Functions (PMF)<a href="further-explanations.html#probability-mass-functions-pmf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="uniform-distributions" class="section level5 hasAnchor" number="7.1.2.1.1">
<h5><span class="header-section-number">7.1.2.1.1</span> Uniform Distributions<a href="further-explanations.html#uniform-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We can let <strong>R roll the dice for us</strong> using the <code>sample()</code> function. This function takes in the range of possible outcomes, the number of observations to draw, and some additional options (which we won’t worry about for now):</p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a href="further-explanations.html#cb433-1" tabindex="-1"></a>dice_rolls <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb433-2"><a href="further-explanations.html#cb433-2" tabindex="-1"></a>  <span class="at">roll =</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>), <span class="dv">1000</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb433-3"><a href="further-explanations.html#cb433-3" tabindex="-1"></a>)</span></code></pre></div>
<p>This simulates rolling a fair six-sided die 1,000 times.</p>
<p>Let us plot it:</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="further-explanations.html#cb434-1" tabindex="-1"></a><span class="co"># Plot with ggplot</span></span>
<span id="cb434-2"><a href="further-explanations.html#cb434-2" tabindex="-1"></a><span class="fu">ggplot</span>(dice_rolls, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(roll))) <span class="sc">+</span></span>
<span id="cb434-3"><a href="further-explanations.html#cb434-3" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">fill =</span> <span class="st">&quot;#89CFF0&quot;</span>, <span class="at">color =</span> <span class="st">&quot;gray&quot;</span>) <span class="sc">+</span></span>
<span id="cb434-4"><a href="further-explanations.html#cb434-4" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb434-5"><a href="further-explanations.html#cb434-5" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Distribution of 1,000 Dice Rolls&quot;</span>,</span>
<span id="cb434-6"><a href="further-explanations.html#cb434-6" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Dice Face&quot;</span>,</span>
<span id="cb434-7"><a href="further-explanations.html#cb434-7" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Frequency&quot;</span></span>
<span id="cb434-8"><a href="further-explanations.html#cb434-8" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb434-9"><a href="further-explanations.html#cb434-9" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/simple%20dice%20rolls-1.png" width="672" /></p>
<p>What we’ve created here is called a <strong>distribution of a discrete variable</strong>. A <strong>discrete variable</strong> has a countable number of distinct outcomes. In this case, our die can only land on one of six values: 1 through 6 — nothing in between.</p>
<p>This explicit type of distribution is called uniform distribution. A <strong>uniform distribution</strong> is one in which <strong>all outcomes are equally likely</strong> — in our case, each face of the die (1 through 6) has the same probability of occurring. Thus, the probability of one outcome is just 1/n, thus 1/6 for each outcome. Let us plot that as well:</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="further-explanations.html#cb435-1" tabindex="-1"></a><span class="co"># Define the outcomes and their corresponding probabilities</span></span>
<span id="cb435-2"><a href="further-explanations.html#cb435-2" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb435-3"><a href="further-explanations.html#cb435-3" tabindex="-1"></a>  <span class="at">Outcome =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>),</span>
<span id="cb435-4"><a href="further-explanations.html#cb435-4" tabindex="-1"></a>  <span class="at">Probability =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb435-5"><a href="further-explanations.html#cb435-5" tabindex="-1"></a>)</span>
<span id="cb435-6"><a href="further-explanations.html#cb435-6" tabindex="-1"></a></span>
<span id="cb435-7"><a href="further-explanations.html#cb435-7" tabindex="-1"></a><span class="co"># Plot it</span></span>
<span id="cb435-8"><a href="further-explanations.html#cb435-8" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> Outcome, <span class="at">y =</span> Probability)) <span class="sc">+</span></span>
<span id="cb435-9"><a href="further-explanations.html#cb435-9" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb435-10"><a href="further-explanations.html#cb435-10" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb435-11"><a href="further-explanations.html#cb435-11" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Uniform Probability Distribution of a Fair Die&quot;</span>,</span>
<span id="cb435-12"><a href="further-explanations.html#cb435-12" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Die Face&quot;</span>,</span>
<span id="cb435-13"><a href="further-explanations.html#cb435-13" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Probability&quot;</span></span>
<span id="cb435-14"><a href="further-explanations.html#cb435-14" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb435-15"><a href="further-explanations.html#cb435-15" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/generating%20probabilities-1.png" width="672" /></p>
<p>This a so-called Probability Mass Function (PMF). This is the term for probability distributions of discrete variables. It is useful since we can determine the probability of an event with it, for example the probability of getting 3:</p>
<p><span class="math display">\[
P(X = 3) = \frac{1}{6}
\]</span></p>
<p>What if we start to cumulate the probabilities, like in the following table:</p>
<table>
<thead>
<tr class="header">
<th>Outcome</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Probability</strong></td>
<td>1/6</td>
<td>1/6</td>
<td>1/6</td>
<td>1/6</td>
<td>1/6</td>
<td>1/6</td>
</tr>
<tr class="even">
<td><strong>Cumulative Probability</strong></td>
<td>1/6</td>
<td>2/6</td>
<td>3/6</td>
<td>4/6</td>
<td>5/6</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>We can plot the cumulative Probability by calculating it with the <code>cumsum()</code> function:</p>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="further-explanations.html#cb436-1" tabindex="-1"></a><span class="co"># Calculating the cumsum</span></span>
<span id="cb436-2"><a href="further-explanations.html#cb436-2" tabindex="-1"></a>df<span class="sc">$</span>cumsum <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(df<span class="sc">$</span>Probability)</span>
<span id="cb436-3"><a href="further-explanations.html#cb436-3" tabindex="-1"></a></span>
<span id="cb436-4"><a href="further-explanations.html#cb436-4" tabindex="-1"></a><span class="co"># Plot it</span></span>
<span id="cb436-5"><a href="further-explanations.html#cb436-5" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> Outcome, <span class="at">y =</span> cumsum)) <span class="sc">+</span></span>
<span id="cb436-6"><a href="further-explanations.html#cb436-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb436-7"><a href="further-explanations.html#cb436-7" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb436-8"><a href="further-explanations.html#cb436-8" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Cumulative Distribution of a Fair Die&quot;</span>,</span>
<span id="cb436-9"><a href="further-explanations.html#cb436-9" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Die Face&quot;</span>,</span>
<span id="cb436-10"><a href="further-explanations.html#cb436-10" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Probability&quot;</span></span>
<span id="cb436-11"><a href="further-explanations.html#cb436-11" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb436-12"><a href="further-explanations.html#cb436-12" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb436-13"><a href="further-explanations.html#cb436-13" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/cumsum-1.png" width="672" /></p>
<p>With this graph we can directly see the cumulative distribution for several outcomes, for example the cumulative probability for the outcome 3 is 0.5, thus 50%, so with a 50% probability I will get either 1, 2 or 3.</p>
</div>
<div id="bernoulli-distributions" class="section level5 hasAnchor" number="7.1.2.1.2">
<h5><span class="header-section-number">7.1.2.1.2</span> Bernoulli Distributions<a href="further-explanations.html#bernoulli-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In this part I want to introduce you a new type of distribution: The bernoulli distribution. It is super easy it takes on the value 1 with probability <span class="math inline">\(p\)</span> and the value 0 with the probability <span class="math inline">\(q = 1 -p\)</span>. In more simple terms, it displays the outcomes for any experiment that has yes/no answers. The classical example is head and tails, you throw a coin and it either shows head or tails. Let us simulate data and plot it:</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="further-explanations.html#cb437-1" tabindex="-1"></a><span class="co"># Simulate 1000 tosses of a fair coin (1 = Head, 0 = Tail)</span></span>
<span id="cb437-2"><a href="further-explanations.html#cb437-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb437-3"><a href="further-explanations.html#cb437-3" tabindex="-1"></a>fair_coin <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb437-4"><a href="further-explanations.html#cb437-4" tabindex="-1"></a>  <span class="at">outcome =</span> <span class="fu">factor</span>(<span class="fu">rbinom</span>(<span class="dv">10</span>, <span class="dv">1</span>, <span class="fl">0.5</span>), <span class="at">levels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), </span>
<span id="cb437-5"><a href="further-explanations.html#cb437-5" tabindex="-1"></a>                   <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Tail&quot;</span>, <span class="st">&quot;Head&quot;</span>))</span>
<span id="cb437-6"><a href="further-explanations.html#cb437-6" tabindex="-1"></a>)</span>
<span id="cb437-7"><a href="further-explanations.html#cb437-7" tabindex="-1"></a></span>
<span id="cb437-8"><a href="further-explanations.html#cb437-8" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb437-9"><a href="further-explanations.html#cb437-9" tabindex="-1"></a><span class="fu">ggplot</span>(fair_coin, <span class="fu">aes</span>(<span class="at">x =</span> outcome)) <span class="sc">+</span></span>
<span id="cb437-10"><a href="further-explanations.html#cb437-10" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">fill =</span> <span class="st">&quot;#89CFF0&quot;</span>, <span class="at">width =</span> <span class="fl">0.35</span>) <span class="sc">+</span></span>
<span id="cb437-11"><a href="further-explanations.html#cb437-11" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb437-12"><a href="further-explanations.html#cb437-12" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Simulation of 10 Tosses of a Fair Coin&quot;</span>,</span>
<span id="cb437-13"><a href="further-explanations.html#cb437-13" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Outcome&quot;</span>,</span>
<span id="cb437-14"><a href="further-explanations.html#cb437-14" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Count&quot;</span></span>
<span id="cb437-15"><a href="further-explanations.html#cb437-15" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb437-16"><a href="further-explanations.html#cb437-16" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/bernoulli%20fair%20coin-1.png" width="672" /></p>
<p>As we probably expected, if we throw a fair coin ten times we will get approximately 5 Tails and 5 Heads. Furthermore, we plotted our first Bernoulli Distribution.</p>
<p>This is an example for a fair coin, what if our coin was “not fair” or “unfair”, thus a “biased” coin, we could plot that as well:</p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="further-explanations.html#cb438-1" tabindex="-1"></a><span class="co"># Simulate 1000 tosses of a fair coin (1 = Head, 0 = Tail)</span></span>
<span id="cb438-2"><a href="further-explanations.html#cb438-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">102</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb438-3"><a href="further-explanations.html#cb438-3" tabindex="-1"></a>unfair_coin <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb438-4"><a href="further-explanations.html#cb438-4" tabindex="-1"></a>  <span class="at">outcome =</span> <span class="fu">factor</span>(<span class="fu">rbinom</span>(<span class="dv">10</span>, <span class="dv">1</span>, <span class="fl">0.28</span>), <span class="at">levels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), </span>
<span id="cb438-5"><a href="further-explanations.html#cb438-5" tabindex="-1"></a>                   <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Tail&quot;</span>, <span class="st">&quot;Head&quot;</span>))</span>
<span id="cb438-6"><a href="further-explanations.html#cb438-6" tabindex="-1"></a>)</span>
<span id="cb438-7"><a href="further-explanations.html#cb438-7" tabindex="-1"></a></span>
<span id="cb438-8"><a href="further-explanations.html#cb438-8" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb438-9"><a href="further-explanations.html#cb438-9" tabindex="-1"></a><span class="fu">ggplot</span>(unfair_coin, <span class="fu">aes</span>(<span class="at">x =</span> outcome)) <span class="sc">+</span></span>
<span id="cb438-10"><a href="further-explanations.html#cb438-10" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">fill =</span> <span class="st">&quot;#89CFF0&quot;</span>, <span class="at">width =</span> <span class="fl">0.35</span>) <span class="sc">+</span></span>
<span id="cb438-11"><a href="further-explanations.html#cb438-11" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb438-12"><a href="further-explanations.html#cb438-12" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Simulation of 1000 Tosses of an unfair Coin&quot;</span>,</span>
<span id="cb438-13"><a href="further-explanations.html#cb438-13" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Outcome&quot;</span>,</span>
<span id="cb438-14"><a href="further-explanations.html#cb438-14" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Count&quot;</span></span>
<span id="cb438-15"><a href="further-explanations.html#cb438-15" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb438-16"><a href="further-explanations.html#cb438-16" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/bernoulli%20biased%20coin-1.png" width="672" /></p>
<p>We can see that the bins are not equal the coin has a tendency for Tail. The Probability for getting Tail is higher than getting Head with this Unfair Coin.</p>
<p>How does the Probability Distribution of a Bernoulli Distribution looks like? Before answering this question, we have to calculate the probability of getting exact 7 Tails and 3 Heads as we got. I leave the maths out, thankfully R can do that with the <code>dbinom()</code> function, it takes in “x” which is the number of outcomes we got, thus our Tail = 7. Then we have to put in the size, which is the number of trials, thus 10 and lastly the probability, if it is a fair coin 0.5. I computed the biased coin with a probability of 0.28.</p>
<p>Let us calculate for the fair and the biased coin, the probability of our results:</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a href="further-explanations.html#cb439-1" tabindex="-1"></a><span class="co"># fair coin</span></span>
<span id="cb439-2"><a href="further-explanations.html#cb439-2" tabindex="-1"></a><span class="fu">dbinom</span>(</span>
<span id="cb439-3"><a href="further-explanations.html#cb439-3" tabindex="-1"></a>  <span class="at">x =</span> <span class="dv">5</span>,</span>
<span id="cb439-4"><a href="further-explanations.html#cb439-4" tabindex="-1"></a>  <span class="at">size =</span> <span class="dv">10</span>,</span>
<span id="cb439-5"><a href="further-explanations.html#cb439-5" tabindex="-1"></a>  <span class="at">prob =</span> <span class="fl">0.5</span></span>
<span id="cb439-6"><a href="further-explanations.html#cb439-6" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## [1] 0.2460938</code></pre>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb441-1"><a href="further-explanations.html#cb441-1" tabindex="-1"></a><span class="co"># biased coin</span></span>
<span id="cb441-2"><a href="further-explanations.html#cb441-2" tabindex="-1"></a><span class="fu">dbinom</span>(</span>
<span id="cb441-3"><a href="further-explanations.html#cb441-3" tabindex="-1"></a>  <span class="at">x =</span> <span class="dv">3</span>,</span>
<span id="cb441-4"><a href="further-explanations.html#cb441-4" tabindex="-1"></a>  <span class="at">size =</span> <span class="dv">10</span>,</span>
<span id="cb441-5"><a href="further-explanations.html#cb441-5" tabindex="-1"></a>  <span class="at">prob =</span> <span class="fl">0.28</span></span>
<span id="cb441-6"><a href="further-explanations.html#cb441-6" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## [1] 0.2642304</code></pre>
<p>What you can see here is the probability of getting exact 5 heads for the fair coin and 3 heads for the unfair coin. So to get exact 5 heads with the fair coin has a probability of 24.6% and the probability of getting exact 3 heads has a probability of 26.4%.</p>
</div>
<div id="binomial-distribution" class="section level5 hasAnchor" number="7.1.2.1.3">
<h5><span class="header-section-number">7.1.2.1.3</span> Binomial Distribution<a href="further-explanations.html#binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>How does it come, that although we have compute a probability of 50% of a success in each trial, that we only get a probability of 5 heads with 24.6% probability and 26.4% of getting 3 heads with a biased coin?</p>
<p>We can solve this problem by plotting a so-called Binomial Distribution. It displays the number of <strong>successes</strong> in a fixed number of independent <strong>yes/no, thus head and tails (Bernoulli)</strong> trials.</p>
<p>It gets clear, when we plot all Probabilities for both coins for each possible outcome for head, thus 10, since we have 10 trials:</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="further-explanations.html#cb443-1" tabindex="-1"></a><span class="co"># Create theoretical distributions</span></span>
<span id="cb443-2"><a href="further-explanations.html#cb443-2" tabindex="-1"></a>theoretical_probs <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb443-3"><a href="further-explanations.html#cb443-3" tabindex="-1"></a>  <span class="at">heads =</span> <span class="fu">rep</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">2</span>),</span>
<span id="cb443-4"><a href="further-explanations.html#cb443-4" tabindex="-1"></a>  <span class="at">coin_type =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;unbiased&quot;</span>, <span class="st">&quot;biased&quot;</span>), <span class="at">each =</span> <span class="dv">11</span>),</span>
<span id="cb443-5"><a href="further-explanations.html#cb443-5" tabindex="-1"></a>  <span class="at">prob =</span> <span class="fu">c</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>, <span class="at">size =</span> <span class="dv">10</span>, <span class="at">prob =</span> <span class="fl">0.5</span>),</span>
<span id="cb443-6"><a href="further-explanations.html#cb443-6" tabindex="-1"></a>           <span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>, <span class="at">size =</span> <span class="dv">10</span>, <span class="at">prob =</span> <span class="fl">0.28</span>))</span>
<span id="cb443-7"><a href="further-explanations.html#cb443-7" tabindex="-1"></a>)</span>
<span id="cb443-8"><a href="further-explanations.html#cb443-8" tabindex="-1"></a></span>
<span id="cb443-9"><a href="further-explanations.html#cb443-9" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb443-10"><a href="further-explanations.html#cb443-10" tabindex="-1"></a><span class="fu">ggplot</span>(theoretical_probs, <span class="fu">aes</span>(<span class="at">x =</span> heads, <span class="at">y =</span> prob)) <span class="sc">+</span></span>
<span id="cb443-11"><a href="further-explanations.html#cb443-11" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">1.5</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb443-12"><a href="further-explanations.html#cb443-12" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> coin_type, </span>
<span id="cb443-13"><a href="further-explanations.html#cb443-13" tabindex="-1"></a>             <span class="at">labeller =</span> <span class="fu">as_labeller</span>(<span class="fu">c</span>(</span>
<span id="cb443-14"><a href="further-explanations.html#cb443-14" tabindex="-1"></a>               <span class="at">biased =</span> <span class="st">&quot;Biased Coin (p = 0.28)&quot;</span>, </span>
<span id="cb443-15"><a href="further-explanations.html#cb443-15" tabindex="-1"></a>               <span class="at">unbiased =</span> <span class="st">&quot;Unbiased Coin (p = 0.5)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb443-16"><a href="further-explanations.html#cb443-16" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Theoretical Binomial Distribution of Number of Heads in 10 Flips&quot;</span>,</span>
<span id="cb443-17"><a href="further-explanations.html#cb443-17" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Number of Heads&quot;</span>,</span>
<span id="cb443-18"><a href="further-explanations.html#cb443-18" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Probability&quot;</span>) <span class="sc">+</span></span>
<span id="cb443-19"><a href="further-explanations.html#cb443-19" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb443-20"><a href="further-explanations.html#cb443-20" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/binomial%20distributions-1.png" width="672" /></p>
<p>Now, the reason behind the probabilities for the outcome for heads becomes clear, the computed probability deviates from the outcome probability because in some cases the trials follow the computed probability but in some cases it deviates and head becomes 2 or 4 for the biased coin and 4 or 6 for the unbiased coin.</p>
<p>At this point, one thing becomes clear: the <strong>computed probability</strong> represents the <em>expected likelihood</em> of obtaining a specific number of heads <strong>on average</strong> over a large number of repeated experiments. Why “on average”? Because probability theory accounts for randomness — meaning that in any single simulation (say, flipping a coin 10 times), we might observe different outcomes: sometimes 5 heads, sometimes 2, or even 9.</p>
<p>For a fair coin, the theoretical expectation is 5 heads in 10 flips. However, this doesn’t mean we will always get exactly 5. The actual results vary due to chance. In the case of a biased coin with a 0.28 probability of heads, we might expect around 2.8 heads on average — but again, we will only ever see whole numbers like 2 or 3 in practice.</p>
<p>In short, these deviations from the expected value arise from randomness. While the true probability (e.g., 0.5 or 0.28) describes the long-term pattern, it does not guarantee specific outcomes in short runs. Instead, it tells us what to expect <strong>most frequently</strong> over a large number of repetitions.</p>
<p>This concept will become important later again, but before that let us have a look at the cumulated distribution function (CDF) of the probability distribution function (PDF):</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="further-explanations.html#cb444-1" tabindex="-1"></a><span class="co"># Fix cumulative sum: compute it within each coin_type group</span></span>
<span id="cb444-2"><a href="further-explanations.html#cb444-2" tabindex="-1"></a>theoretical_probs <span class="ot">&lt;-</span> theoretical_probs <span class="sc">%&gt;%</span></span>
<span id="cb444-3"><a href="further-explanations.html#cb444-3" tabindex="-1"></a>  <span class="fu">group_by</span>(coin_type) <span class="sc">%&gt;%</span></span>
<span id="cb444-4"><a href="further-explanations.html#cb444-4" tabindex="-1"></a>  <span class="fu">arrange</span>(heads, <span class="at">.by_group =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb444-5"><a href="further-explanations.html#cb444-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">cumsum_prob =</span> <span class="fu">cumsum</span>(prob)) <span class="sc">%&gt;%</span></span>
<span id="cb444-6"><a href="further-explanations.html#cb444-6" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span>
<span id="cb444-7"><a href="further-explanations.html#cb444-7" tabindex="-1"></a></span>
<span id="cb444-8"><a href="further-explanations.html#cb444-8" tabindex="-1"></a><span class="co"># Plot cumulative probabilities</span></span>
<span id="cb444-9"><a href="further-explanations.html#cb444-9" tabindex="-1"></a><span class="fu">ggplot</span>(theoretical_probs, <span class="fu">aes</span>(<span class="at">x =</span> heads, <span class="at">y =</span> cumsum_prob)) <span class="sc">+</span></span>
<span id="cb444-10"><a href="further-explanations.html#cb444-10" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">1.5</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb444-11"><a href="further-explanations.html#cb444-11" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> coin_type,</span>
<span id="cb444-12"><a href="further-explanations.html#cb444-12" tabindex="-1"></a>             <span class="at">labeller =</span> <span class="fu">as_labeller</span>(<span class="fu">c</span>(</span>
<span id="cb444-13"><a href="further-explanations.html#cb444-13" tabindex="-1"></a>               <span class="at">biased =</span> <span class="st">&quot;Biased Coin (p = 0.28)&quot;</span>,</span>
<span id="cb444-14"><a href="further-explanations.html#cb444-14" tabindex="-1"></a>               <span class="at">unbiased =</span> <span class="st">&quot;Unbiased Coin (p = 0.5)&quot;</span>)),</span>
<span id="cb444-15"><a href="further-explanations.html#cb444-15" tabindex="-1"></a>             <span class="at">nrow =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb444-16"><a href="further-explanations.html#cb444-16" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb444-17"><a href="further-explanations.html#cb444-17" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Cumulative Distribution of Number of Heads in 10 Flips&quot;</span>,</span>
<span id="cb444-18"><a href="further-explanations.html#cb444-18" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Number of Heads&quot;</span>,</span>
<span id="cb444-19"><a href="further-explanations.html#cb444-19" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Cumulative Probability&quot;</span>) <span class="sc">+</span></span>
<span id="cb444-20"><a href="further-explanations.html#cb444-20" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb444-21"><a href="further-explanations.html#cb444-21" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/cumulative%20probabilities-1.png" width="672" /></p>
<p>The CDF is quite interesting, we see that in round about 70% of all experiments of the biased coin will result in 3 or less heads and on the other side the unbiased coin will result in round about 80% in 6 or less heads.</p>
</div>
</div>
<div id="probability-density-functions-pdf" class="section level4 hasAnchor" number="7.1.2.2">
<h4><span class="header-section-number">7.1.2.2</span> Probability Density Functions (PDF)<a href="further-explanations.html#probability-density-functions-pdf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="normal-distributions" class="section level5 hasAnchor" number="7.1.2.2.1">
<h5><span class="header-section-number">7.1.2.2.1</span> <strong>Normal Distributions</strong><a href="further-explanations.html#normal-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Thus far we looked at the two typical distributions for <strong>discrete variables</strong>. In the following, we will look at distributions for <strong>continuous variables</strong>.</p>
<p>The probability distributions of the Bernoulli trials we saw in the part follow the form of the arguably most important distribution: The normal distribution. This distribution is also called a Gaussian distribution after its inventor Carl-Friedrich Gauss, or Bell Curve due to its shape. Let us have a look at it at the Probability Distribution Function (PDF):</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="further-explanations.html#cb445-1" tabindex="-1"></a><span class="co"># simulating the data</span></span>
<span id="cb445-2"><a href="further-explanations.html#cb445-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb445-3"><a href="further-explanations.html#cb445-3" tabindex="-1"></a>snd <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb445-4"><a href="further-explanations.html#cb445-4" tabindex="-1"></a>  <span class="at">sample =</span> <span class="fu">rnorm</span>(<span class="dv">1000000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb445-5"><a href="further-explanations.html#cb445-5" tabindex="-1"></a>  )</span>
<span id="cb445-6"><a href="further-explanations.html#cb445-6" tabindex="-1"></a></span>
<span id="cb445-7"><a href="further-explanations.html#cb445-7" tabindex="-1"></a><span class="co"># Plot it</span></span>
<span id="cb445-8"><a href="further-explanations.html#cb445-8" tabindex="-1"></a><span class="fu">ggplot</span>(snd, <span class="fu">aes</span>(<span class="at">x =</span> sample)) <span class="sc">+</span></span>
<span id="cb445-9"><a href="further-explanations.html#cb445-9" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb445-10"><a href="further-explanations.html#cb445-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Normal Distribution&quot;</span>,</span>
<span id="cb445-11"><a href="further-explanations.html#cb445-11" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Value of Random Variable&quot;</span>,</span>
<span id="cb445-12"><a href="further-explanations.html#cb445-12" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb445-13"><a href="further-explanations.html#cb445-13" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="sc">-</span><span class="dv">5</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb445-14"><a href="further-explanations.html#cb445-14" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb445-15"><a href="further-explanations.html#cb445-15" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/pdf-1.png" width="672" /></p>
<p>The normal distribution has a lot of properties:</p>
<ul>
<li><p>The distribution is perfectly symmetrical around the mean</p></li>
<li><p>The mean equals the median equals the mode</p></li>
<li><p>The shape is bell-like, with most values clustered around the mean and tails tapering off in both directions.</p></li>
<li><p>About 68% of the data fall within 1 standard deviation of the mean, about 95% fall within 2 standard deviations and around 99.7% fall within 3 standard deviations.</p></li>
<li><p>The distribution is defined by its mean (<span class="math inline">\(\mu\)</span>) and its standard deviation (<span class="math inline">\(\sigma\)</span>).</p></li>
<li><p>The tails extend infinitely in both directions but never touch the x-axis (approach zero asymptotically).</p></li>
<li><p>The area under the density curve represents probability, and it sums up to exactly 1. That is the reason the reason why we talk about Probability Density Functions, because we can determine the probability for each data point by calculating the density of the curve.</p></li>
<li><p>It is unimodal, thus has only one peak.</p></li>
<li><p>As Standard Normal Distributions is a special case where the mean, <span class="math inline">\(\mu\)</span> = 0 and the standard deviation, <span class="math inline">\(\sigma\)</span> = 1</p></li>
</ul>
<p>We already saw the Cumulative Density Function in the Bernoulli Trials, but let us have a look again:</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="further-explanations.html#cb446-1" tabindex="-1"></a><span class="co"># Plot empirical CDF</span></span>
<span id="cb446-2"><a href="further-explanations.html#cb446-2" tabindex="-1"></a><span class="fu">ggplot</span>(snd, <span class="fu">aes</span>(<span class="at">x =</span> sample)) <span class="sc">+</span></span>
<span id="cb446-3"><a href="further-explanations.html#cb446-3" tabindex="-1"></a>  <span class="fu">stat_ecdf</span>(<span class="at">geom =</span> <span class="st">&quot;step&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb446-4"><a href="further-explanations.html#cb446-4" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Cumulative Distribution Function (CDF)&quot;</span>,</span>
<span id="cb446-5"><a href="further-explanations.html#cb446-5" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Value of Random Variable&quot;</span>,</span>
<span id="cb446-6"><a href="further-explanations.html#cb446-6" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Cumulative Probability&quot;</span>) <span class="sc">+</span></span>
<span id="cb446-7"><a href="further-explanations.html#cb446-7" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/cdf-1.png" width="672" /></p>
<p>We can use the cumulative distribution function (CDF) to determine the probability that a random variable takes on a value <strong>less than or equal to</strong> a specific value. For example, with a standard normal distribution, the CDF tells us there’s a 50% chance that the random variable will take on a value <strong>less than or equal to 0</strong>.</p>
</div>
<div id="central-limit-theorem-ctl" class="section level5 hasAnchor" number="7.1.2.2.2">
<h5><span class="header-section-number">7.1.2.2.2</span> Central Limit Theorem (CTL)<a href="further-explanations.html#central-limit-theorem-ctl" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In the examples of the Chapter <a href="further-explanations.html#working-with-distributions">7.1.3</a> we will use normal distributions due to its significance for natural and social sciences, here is why:</p>
<ul>
<li><p>When we collect random variables such as height, IQ, sizes of snowflakes, milk production of cows follows a normal distribution.</p></li>
<li><p>Due to its elegant mathematical properties like the symmetry around the mean, the 68-95-99.7 rule, and it is defined by only two parameters, the mean and the standard deviation.</p></li>
<li><p>Statistical Analysis assumes in many models a normal distributions, because its simplicity, for example the it is the building block for machine learning, finance, etc., and for statistical models such as regression analysis assumes also normality.</p></li>
</ul>
<p>But besides these, let’s say “convenient reasons”, there is also one mathematical reason why it is so frequently used in statistics, <strong>the central limit theorem</strong>. I will not dive into its maths, but it is quite easy to understand:</p>
<p>The Central Limit Theorem states that, under certain conditions, the distribution of the <strong>sample means</strong> of a large number of independent and identically distributed random variables will approximate a normal distribution—<strong>regardless of the shape of the original population distribution</strong>. In other words, as the sample size increases, the distribution of the <strong>means of these samples</strong> becomes increasingly normal.</p>
<p>Let us visualize that! Remember the sample of our dice? What if we draw many, many samples and collect their sample means, and then plot the sample means? We would expect a uniform distribution, right? Because every outcome has the same probability to happen, exactly 1/6?</p>
<p>Let us draw samples and look what happens. In the following I first define a function that rolls the dice 1000 times. In the next step, I use the function to draw a sample. I first take one sample, thus role the dice 1000 times and save the result, and plot it, I do the same for 2, 3, and 4 samples:</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="further-explanations.html#cb447-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb447-2"><a href="further-explanations.html#cb447-2" tabindex="-1"></a></span>
<span id="cb447-3"><a href="further-explanations.html#cb447-3" tabindex="-1"></a><span class="co"># Function to simulate rolling a die n_rolls times, repeated n_sim times</span></span>
<span id="cb447-4"><a href="further-explanations.html#cb447-4" tabindex="-1"></a>simulate_dice_means <span class="ot">&lt;-</span> <span class="cf">function</span>(n_rolls, <span class="at">n_sim =</span> <span class="dv">1000</span>) {</span>
<span id="cb447-5"><a href="further-explanations.html#cb447-5" tabindex="-1"></a>  <span class="fu">replicate</span>(n_sim, <span class="fu">mean</span>(<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, n_rolls, <span class="at">replace =</span> <span class="cn">TRUE</span>)))</span>
<span id="cb447-6"><a href="further-explanations.html#cb447-6" tabindex="-1"></a>}</span>
<span id="cb447-7"><a href="further-explanations.html#cb447-7" tabindex="-1"></a></span>
<span id="cb447-8"><a href="further-explanations.html#cb447-8" tabindex="-1"></a><span class="co"># Simulate sample means for different numbers of rolls</span></span>
<span id="cb447-9"><a href="further-explanations.html#cb447-9" tabindex="-1"></a>means_1 <span class="ot">&lt;-</span> <span class="fu">simulate_dice_means</span>(<span class="dv">1</span>)</span>
<span id="cb447-10"><a href="further-explanations.html#cb447-10" tabindex="-1"></a>means_2 <span class="ot">&lt;-</span> <span class="fu">simulate_dice_means</span>(<span class="dv">2</span>)</span>
<span id="cb447-11"><a href="further-explanations.html#cb447-11" tabindex="-1"></a>means_3 <span class="ot">&lt;-</span> <span class="fu">simulate_dice_means</span>(<span class="dv">3</span>)</span>
<span id="cb447-12"><a href="further-explanations.html#cb447-12" tabindex="-1"></a>means_4 <span class="ot">&lt;-</span> <span class="fu">simulate_dice_means</span>(<span class="dv">4</span>)</span>
<span id="cb447-13"><a href="further-explanations.html#cb447-13" tabindex="-1"></a></span>
<span id="cb447-14"><a href="further-explanations.html#cb447-14" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb447-15"><a href="further-explanations.html#cb447-15" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">mean =</span> means_1, <span class="at">rolls =</span> <span class="st">&quot;1000 Rolls&quot;</span>),</span>
<span id="cb447-16"><a href="further-explanations.html#cb447-16" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">mean =</span> means_2, <span class="at">rolls =</span> <span class="st">&quot;2000 Rolls&quot;</span>),</span>
<span id="cb447-17"><a href="further-explanations.html#cb447-17" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">mean =</span> means_3, <span class="at">rolls =</span> <span class="st">&quot;3000 Rolls&quot;</span>),</span>
<span id="cb447-18"><a href="further-explanations.html#cb447-18" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">mean =</span> means_4, <span class="at">rolls =</span> <span class="st">&quot;4000 Rolls&quot;</span>)</span>
<span id="cb447-19"><a href="further-explanations.html#cb447-19" tabindex="-1"></a>)</span>
<span id="cb447-20"><a href="further-explanations.html#cb447-20" tabindex="-1"></a></span>
<span id="cb447-21"><a href="further-explanations.html#cb447-21" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb447-22"><a href="further-explanations.html#cb447-22" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> mean)) <span class="sc">+</span></span>
<span id="cb447-23"><a href="further-explanations.html#cb447-23" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb447-24"><a href="further-explanations.html#cb447-24" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> rolls, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="at">ncol =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb447-25"><a href="further-explanations.html#cb447-25" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Central Limit Theorem Demonstration with Dice Rolls&quot;</span>,</span>
<span id="cb447-26"><a href="further-explanations.html#cb447-26" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Sample Mean&quot;</span>,</span>
<span id="cb447-27"><a href="further-explanations.html#cb447-27" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb447-28"><a href="further-explanations.html#cb447-28" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb447-29"><a href="further-explanations.html#cb447-29" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">6</span>)) <span class="sc">+</span></span>
<span id="cb447-30"><a href="further-explanations.html#cb447-30" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/central%20limit%20theorem-1.png" width="672" /></p>
<p>Here we can clearly see how the distribution of the sample means converge to the form of a normal distribution. That leaves only one question, why does the mean of the sample means converge to 3.5?</p>
</div>
<div id="law-of-large-numbers" class="section level5 hasAnchor" number="7.1.2.2.3">
<h5><span class="header-section-number">7.1.2.2.3</span> <strong>Law of Large Numbers</strong><a href="further-explanations.html#law-of-large-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The phenomenon that the sample converges to 3.5 is due to the Law of large numbers. It is a mathematical law that states that if the sample size increases, the sample mean gets closer population mean. The true population mean for our dice example is 3.5 (1+2+3+4+5+6/6 = 3.5), thus our expected value is our true population mean. According to the Law of large numbers, the more the dice is rolled the more the sample mean converges to 3.5 and that is what we see in the density plot in the bottom, right corner. When the dice is rolled 4000 times in total the curve gets closer to the sample mean.</p>
</div>
</div>
<div id="different-types-of-distributions" class="section level4 hasAnchor" number="7.1.2.3">
<h4><span class="header-section-number">7.1.2.3</span> Different Types of Distributions<a href="further-explanations.html#different-types-of-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Alright, that was a lot of theory, but it is crucial to understand, because probability distributions are the basic ground for the statistical models to analyse our data. And here is the deal, knowing the distributions of our data leads to configuration possibilities later in the models, so that our models can better fit our data! In the next chapter, I hope that will become clear!</p>
<p>To conclude this chapter, I will briefly introduce you in different types of distributions and their interpretations:</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="further-explanations.html#cb448-1" tabindex="-1"></a><span class="co"># Create a data frame with different distributions</span></span>
<span id="cb448-2"><a href="further-explanations.html#cb448-2" tabindex="-1"></a>x_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb448-3"><a href="further-explanations.html#cb448-3" tabindex="-1"></a></span>
<span id="cb448-4"><a href="further-explanations.html#cb448-4" tabindex="-1"></a>dist_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb448-5"><a href="further-explanations.html#cb448-5" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">rep</span>(x_vals, <span class="dv">4</span>),  <span class="co"># only 4 now, Poisson removed from here</span></span>
<span id="cb448-6"><a href="further-explanations.html#cb448-6" tabindex="-1"></a>  <span class="at">distribution =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Exponential (λ=1)&quot;</span>, </span>
<span id="cb448-7"><a href="further-explanations.html#cb448-7" tabindex="-1"></a>                       <span class="st">&quot;Gamma (shape=2, rate=1)&quot;</span>, </span>
<span id="cb448-8"><a href="further-explanations.html#cb448-8" tabindex="-1"></a>                       <span class="st">&quot;Chi-Square (df=3)&quot;</span>, </span>
<span id="cb448-9"><a href="further-explanations.html#cb448-9" tabindex="-1"></a>                       <span class="st">&quot;t-Distribution (df=10)&quot;</span>), <span class="at">each =</span> <span class="fu">length</span>(x_vals)),</span>
<span id="cb448-10"><a href="further-explanations.html#cb448-10" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">c</span>(</span>
<span id="cb448-11"><a href="further-explanations.html#cb448-11" tabindex="-1"></a>    <span class="fu">dexp</span>(x_vals, <span class="at">rate =</span> <span class="dv">1</span>),</span>
<span id="cb448-12"><a href="further-explanations.html#cb448-12" tabindex="-1"></a>    <span class="fu">dgamma</span>(x_vals, <span class="at">shape =</span> <span class="dv">2</span>, <span class="at">rate =</span> <span class="dv">1</span>),</span>
<span id="cb448-13"><a href="further-explanations.html#cb448-13" tabindex="-1"></a>    <span class="fu">dchisq</span>(x_vals, <span class="at">df =</span> <span class="dv">3</span>),</span>
<span id="cb448-14"><a href="further-explanations.html#cb448-14" tabindex="-1"></a>    <span class="fu">dt</span>(x_vals <span class="sc">-</span> <span class="dv">5</span>, <span class="at">df =</span> <span class="dv">10</span>)  <span class="co"># shift t-distribution for better display</span></span>
<span id="cb448-15"><a href="further-explanations.html#cb448-15" tabindex="-1"></a>  )</span>
<span id="cb448-16"><a href="further-explanations.html#cb448-16" tabindex="-1"></a>)</span>
<span id="cb448-17"><a href="further-explanations.html#cb448-17" tabindex="-1"></a></span>
<span id="cb448-18"><a href="further-explanations.html#cb448-18" tabindex="-1"></a><span class="co"># Add Poisson and F-distribution</span></span>
<span id="cb448-19"><a href="further-explanations.html#cb448-19" tabindex="-1"></a>poisson_vals <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb448-20"><a href="further-explanations.html#cb448-20" tabindex="-1"></a>  <span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb448-21"><a href="further-explanations.html#cb448-21" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">dpois</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>, <span class="at">lambda =</span> <span class="dv">3</span>),</span>
<span id="cb448-22"><a href="further-explanations.html#cb448-22" tabindex="-1"></a>  <span class="at">distribution =</span> <span class="st">&quot;Poisson (λ=3)&quot;</span></span>
<span id="cb448-23"><a href="further-explanations.html#cb448-23" tabindex="-1"></a>)</span>
<span id="cb448-24"><a href="further-explanations.html#cb448-24" tabindex="-1"></a></span>
<span id="cb448-25"><a href="further-explanations.html#cb448-25" tabindex="-1"></a>f_vals <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb448-26"><a href="further-explanations.html#cb448-26" tabindex="-1"></a>  <span class="at">x =</span> x_vals,</span>
<span id="cb448-27"><a href="further-explanations.html#cb448-27" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">df</span>(x_vals, <span class="at">df1 =</span> <span class="dv">5</span>, <span class="at">df2 =</span> <span class="dv">10</span>),</span>
<span id="cb448-28"><a href="further-explanations.html#cb448-28" tabindex="-1"></a>  <span class="at">distribution =</span> <span class="st">&quot;F-Distribution (df1=5, df2=10)&quot;</span></span>
<span id="cb448-29"><a href="further-explanations.html#cb448-29" tabindex="-1"></a>)</span>
<span id="cb448-30"><a href="further-explanations.html#cb448-30" tabindex="-1"></a></span>
<span id="cb448-31"><a href="further-explanations.html#cb448-31" tabindex="-1"></a><span class="co"># Combine all distributions</span></span>
<span id="cb448-32"><a href="further-explanations.html#cb448-32" tabindex="-1"></a>plot_df <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(dist_df, poisson_vals, f_vals)</span>
<span id="cb448-33"><a href="further-explanations.html#cb448-33" tabindex="-1"></a></span>
<span id="cb448-34"><a href="further-explanations.html#cb448-34" tabindex="-1"></a><span class="co"># Plot using facet_wrap</span></span>
<span id="cb448-35"><a href="further-explanations.html#cb448-35" tabindex="-1"></a><span class="fu">ggplot</span>(plot_df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb448-36"><a href="further-explanations.html#cb448-36" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> <span class="fu">filter</span>(plot_df, <span class="sc">!</span>distribution <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;Poisson (λ=3)&quot;</span>)),</span>
<span id="cb448-37"><a href="further-explanations.html#cb448-37" tabindex="-1"></a>            <span class="at">color =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb448-38"><a href="further-explanations.html#cb448-38" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> <span class="fu">filter</span>(plot_df, distribution <span class="sc">==</span> <span class="st">&quot;Poisson (λ=3)&quot;</span>),</span>
<span id="cb448-39"><a href="further-explanations.html#cb448-39" tabindex="-1"></a>             <span class="at">color =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb448-40"><a href="further-explanations.html#cb448-40" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> distribution, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="at">ncol =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb448-41"><a href="further-explanations.html#cb448-41" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Overview of Common Statistical Distributions&quot;</span>,</span>
<span id="cb448-42"><a href="further-explanations.html#cb448-42" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;x&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Density / Probability&quot;</span>) <span class="sc">+</span></span>
<span id="cb448-43"><a href="further-explanations.html#cb448-43" tabindex="-1"></a>  <span class="fu">theme_minimal</span>(<span class="at">base_size =</span> <span class="dv">14</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Let us go shortly through these distributions and their meanings:</p>
<ul>
<li><p>Chi-Square: This distribution arises when squaring standard normal values. It’s fundamental in hypothesis testing (e.g., the Chi-Square Test for independence) and constructing confidence intervals for population variances.</p></li>
<li><p>Exponential: It describes the time between two independent events in a Poisson process. Essential for survival analysis and modeling waiting times.</p></li>
<li><p>F-Distribution: It describes the ratio of two sample variances, each from normally distributed populations. Used in comparing model fits.</p></li>
<li><p>Gamma: A flexible distribution that models the waiting time until multiple independent events occur. It generalizes the exponential distribution and is widely used in Bayesian statistics, reliability engineering, and insurance modeling.</p></li>
<li><p>Poisson: Models the number of events occurring in a fixed time or space interval when events happen independently at a constant average rate—like the number of emails per hour. Commonly used for count data and in event-based modeling.</p></li>
<li><p>t-Distribution: Used when estimating the mean of a normally distributed population in small samples. It accounts for the added uncertainty from estimating the standard deviation. Essential in hypothesis testing and regression analysis.</p></li>
</ul>
</div>
</div>
<div id="working-with-distributions" class="section level3 hasAnchor" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Working with Distributions<a href="further-explanations.html#working-with-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After the fairly theoretical part before, let us work with distributions hands-on in <code>R</code>: For this purpose let us simulate data. For the following part, I will simulate a normal distribution and introduce you to the first central function <code>rnorm()</code>. This functions takes in three arguments, the number of observations <strong>n</strong>, the <strong>mean</strong> and the standard deviation, <strong>sd</strong>. And again, the beauty of the normal distribution becomes clear, we do not need to tell R more than that, since normal distributions can be generated with only the mean and the standard deviation and on top we can define the number of observations.</p>
<p>We will simulate the normal distribution for IQ. Let the mean be 100 and the standard deviation 15. These are not randomly selected, most samples of the IQ result in this mean and sd:</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="further-explanations.html#cb449-1" tabindex="-1"></a><span class="co">#Setting seed for reproduciability</span></span>
<span id="cb449-2"><a href="further-explanations.html#cb449-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb449-3"><a href="further-explanations.html#cb449-3" tabindex="-1"></a></span>
<span id="cb449-4"><a href="further-explanations.html#cb449-4" tabindex="-1"></a><span class="co">#Simulating sample</span></span>
<span id="cb449-5"><a href="further-explanations.html#cb449-5" tabindex="-1"></a>sample_iq <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb449-6"><a href="further-explanations.html#cb449-6" tabindex="-1"></a>  <span class="at">height =</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>))</span>
<span id="cb449-7"><a href="further-explanations.html#cb449-7" tabindex="-1"></a></span>
<span id="cb449-8"><a href="further-explanations.html#cb449-8" tabindex="-1"></a><span class="co">#Plotting it</span></span>
<span id="cb449-9"><a href="further-explanations.html#cb449-9" tabindex="-1"></a><span class="fu">ggplot</span>(sample_iq, <span class="fu">aes</span>(<span class="at">x=</span>height)) <span class="sc">+</span></span>
<span id="cb449-10"><a href="further-explanations.html#cb449-10" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;#E35335&quot;</span>) <span class="sc">+</span></span>
<span id="cb449-11"><a href="further-explanations.html#cb449-11" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb449-12"><a href="further-explanations.html#cb449-12" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;IQ&quot;</span>, </span>
<span id="cb449-13"><a href="further-explanations.html#cb449-13" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Frequency&quot;</span></span>
<span id="cb449-14"><a href="further-explanations.html#cb449-14" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb449-15"><a href="further-explanations.html#cb449-15" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">40</span>, <span class="dv">160</span>, <span class="dv">20</span>),</span>
<span id="cb449-16"><a href="further-explanations.html#cb449-16" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">40</span>, <span class="dv">160</span>)) <span class="sc">+</span></span>
<span id="cb449-17"><a href="further-explanations.html#cb449-17" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/simulating%20sample-1.png" width="672" /></p>
<div id="the-68-95-99.7-rule" class="section level4 hasAnchor" number="7.1.3.1">
<h4><span class="header-section-number">7.1.3.1</span> The 68-95-99.7 Rule<a href="further-explanations.html#the-68-95-99.7-rule" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Remember this Rule? Without any complex mathematics, we can determine, where the most values are located, just by by subtracting the standard deviation from the mean:</p>
<ul>
<li><p>68% of all values falls within one standard deviation of the mean:</p>
<p><span class="math display">\[
\mu + \sigma² = 100 + 15 = 115
\]</span></p>
<p><span class="math display">\[
\mu - \sigma² = 100 + 15 = 85
\]</span></p>
<p>We can interpret these values as follows, when your IQ falls between 115 and 115 you share the fate of 68% of our sample.</p></li>
<li><p>95% of all values falls within two standard deviations of the mean:</p>
<p><span class="math display">\[
\mu + 2\sigma² = 100 + 2 \cdot 15 = 130
\]</span></p>
<p><span class="math display">\[
\mu - 2\sigma² = 100 - 2 \cdot 15 = 70
\]</span></p>
<p>If an IQ is not within 70 and 130 then it is either in the 2.5 % lowest or 2.5 highest values. An IQ smaller than 70 means being among the 2.5% lowest IQs and on the other side, if the IQ is above 130 than your IQ is higher than 97.5% !</p></li>
<li><p>99.7% of all values falls within three standard deviations of the mean:</p>
<p><span class="math display">\[
\mu + 3\sigma² = 100 + 3 \cdot 15 = 145
\]</span></p>
<p><span class="math display">\[
\mu + 3\sigma² = 100 - 3 = 55
\]</span></p></li>
</ul>
<p>If the IQ value is outside of 145 or 55, the value is either at the top 0.15 or if it is below 55 it is at the bottom 0.15.</p>
</div>
<div id="determining-the-probability-of-single-values" class="section level4 hasAnchor" number="7.1.3.2">
<h4><span class="header-section-number">7.1.3.2</span> Determining the probability of single values?<a href="further-explanations.html#determining-the-probability-of-single-values" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The 68-95-99.7 rule is a helpful rule of thumb for understanding normal distributions. However, it’s not very precise. If we want to determine the <strong>exact probability</strong> of a value in our distribution, we need to use the <strong>Probability Density Function (PDF)</strong> of the normal distribution.</p>
<p>Now, because a continuous distribution has <strong>infinitely many possible values</strong>, the probability of observing <strong>any exact value</strong> (like exactly 100) is actually <strong>zero</strong>. That’s why we work with <strong>probability densities</strong>, not exact probabilities at a single point.</p>
<p>Instead of asking “What is the probability of exactly 100?”, we ask “What is the <strong>density</strong> at 100?” — which reflects how likely it is to observe values <strong>around</strong> 100.</p>
<p>In R, we can use the <code>dnorm()</code> function to compute this <strong>density</strong>. It takes three arguments:</p>
<ul>
<li><p><strong>x</strong>: the value we’re interested in</p></li>
<li><p><strong>mean</strong>: the mean of the distribution</p></li>
<li><p><strong>sd</strong>: the standard deviation</p></li>
</ul>
<p>Let us calculate the probability of having an IQ of exactly 100, 87 and 140:</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="further-explanations.html#cb450-1" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="at">x =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co">#Probability of 100 IQ</span></span></code></pre></div>
<pre><code>## [1] 0.02659615</code></pre>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="further-explanations.html#cb452-1" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="at">x =</span> <span class="dv">87</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co">#Probability of 65 IQ</span></span></code></pre></div>
<pre><code>## [1] 0.0182691</code></pre>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="further-explanations.html#cb454-1" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="at">x =</span> <span class="dv">140</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co">#Probability of 135 IQ</span></span></code></pre></div>
<pre><code>## [1] 0.0007597324</code></pre>
<p>So the probability of having an IQ of 100 is 2.6%, getting an IQ of 87 is 1.8% and lastly the probability of having an IQ of 140 is 0.07%.</p>
</div>
<div id="getting-the-probabilities-until-a-certain-value" class="section level4 hasAnchor" number="7.1.3.3">
<h4><span class="header-section-number">7.1.3.3</span> Getting the probabilities until a certain value<a href="further-explanations.html#getting-the-probabilities-until-a-certain-value" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The function <code>pnorm()</code> in R gives the <strong>cumulative probability</strong> up to a certain value under the normal distribution. In simpler terms, it tells you <strong>how likely it is that a value drawn from a normal distribution is less than or equal to a given number</strong>.</p>
<p>It takes the following arguments:</p>
<ul>
<li><p><code>q</code>: the value you are interested in (the quantile)</p></li>
<li><p><code>mean</code>: the mean (μ) of the distribution</p></li>
<li><p><code>sd</code>: the standard deviation (σ) of the distribution</p></li>
</ul>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="further-explanations.html#cb456-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="at">q =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co">#Probability of 100 IQ</span></span></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="further-explanations.html#cb458-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="at">q =</span> <span class="dv">87</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co">#Probability of 65 IQ</span></span></code></pre></div>
<pre><code>## [1] 0.1930623</code></pre>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="further-explanations.html#cb460-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="at">q =</span> <span class="dv">140</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co">#Probability of 135 IQ</span></span></code></pre></div>
<pre><code>## [1] 0.9961696</code></pre>
<p>Now what do those values mean? The interpretation is quite easy, 0.5 means that an IQ of 100 is higher than 0.5, thus 50% of our distribution has lower values than 100. In simpler terms. If a person has an IQ of 100, the person has an IQ smarter than 50% of all other people. If a person has an IQ of 87, the person has an IQ higher than 19.3% of the sample/population. For an IQ of 140 we can say that the person has a higher IQ than 99.6% of all other persons in the sample. In other ways, with an IQ of 140, a person is part of the top 1%.</p>
<p>What if we want to know which value we need, to be in a certain percentile? Well, for this case R has the <code>qnorm()</code> function.</p>
<ul>
<li>It takes in the argument <strong>p</strong>, which takes probability, we want the corresponding value and of course our two parameters mean and standard deviation</li>
</ul>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="further-explanations.html#cb462-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="at">p =</span> <span class="fl">0.5</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co">#Probability of 100 IQ</span></span></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="further-explanations.html#cb464-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="at">p =</span> <span class="fl">0.1930623</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co">#Probability of 65 IQ</span></span></code></pre></div>
<pre><code>## [1] 87</code></pre>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="further-explanations.html#cb466-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="at">p =</span> <span class="fl">0.9961696</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co">#Probability of 135 IQ</span></span></code></pre></div>
<pre><code>## [1] 140</code></pre>
<p>We see that we get our estimated values before. This function is especially useful when setting thresholds (e.g., what IQ is needed to qualify as a “genius”) or determining cutoffs in standardized testing.</p>
</div>
<div id="conclusion-1" class="section level4 hasAnchor" number="7.1.3.4">
<h4><span class="header-section-number">7.1.3.4</span> Conclusion<a href="further-explanations.html#conclusion-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This concludes our chapter on probability theory.
We explored different types of variables, learned about <strong>Probability Mass Functions (PMFs)</strong> and <strong>Probability Density Functions (PDFs)</strong>, and finally saw how to work with probability distributions in R.</p>
<p>At first glance, this might seem like basic theory. However, a solid understanding of distributions is essential: it allows us to <strong>detect and correct mistakes</strong> when building statistical models, which often <strong>rely on distributional assumptions</strong>.</p>
<p>Moreover, understanding distributions shows us <strong>why</strong> statistical analysis works in the first place—even in the presence of uncertainty. It allows us to assign <strong>probabilities to predictions</strong>, interpret results meaningfully, and quantify how likely certain outcomes are.</p>
</div>
</div>
</div>
<div id="regression-diagnostics" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Regression Diagnostics<a href="further-explanations.html#regression-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="model-fit-bivariate-regression" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Model Fit: Bivariate Regression<a href="further-explanations.html#model-fit-bivariate-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now the linear regression has a lot of assumptions, it is not like we can run the model every time how we want. Since it is a model, it makes assumptions and instead of just assuming them to be right, we can test them. To techniques to test them are called <strong>Measures of Fit</strong>. Because they test how much our data fits the data. Let us have a look at the assumptions and how we can test them:</p>
<div id="measures-of-fit" class="section level4 hasAnchor" number="7.2.1.1">
<h4><span class="header-section-number">7.2.1.1</span> Measures of Fit<a href="further-explanations.html#measures-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="residuals" class="section level5 hasAnchor" number="7.2.1.1.1">
<h5><span class="header-section-number">7.2.1.1.1</span> Residuals<a href="further-explanations.html#residuals" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>1.) Calculating Residuals:</p>
<p><span class="math display">\[
Residuals = y_i - \hat{y_i} = y_i - (\hat{\beta_0} - \hat{\beta_1}x_i)
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(y_i\)</span> = our actual observed values of our dependent variable (<code>df$y</code>)</p></li>
<li><p><span class="math inline">\(\hat{y_i}\)</span> = are our predicted values based on our OLS estimator</p></li>
</ul>
<p>Reconsider the graph at 2.2.1, the residuals are basically the red lines, thus the distance from the line to the point. We can calculate the residuals for our graph:</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="further-explanations.html#cb468-1" tabindex="-1"></a><span class="co">#Getting the data</span></span>
<span id="cb468-2"><a href="further-explanations.html#cb468-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># For reproducibility</span></span>
<span id="cb468-3"><a href="further-explanations.html#cb468-3" tabindex="-1"></a></span>
<span id="cb468-4"><a href="further-explanations.html#cb468-4" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">30</span> </span>
<span id="cb468-5"><a href="further-explanations.html#cb468-5" tabindex="-1"></a></span>
<span id="cb468-6"><a href="further-explanations.html#cb468-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n) <span class="sc">*</span> <span class="dv">10</span> </span>
<span id="cb468-7"><a href="further-explanations.html#cb468-7" tabindex="-1"></a></span>
<span id="cb468-8"><a href="further-explanations.html#cb468-8" tabindex="-1"></a>categorical_variable <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), n, <span class="at">replace =</span> <span class="cn">TRUE</span>))</span>
<span id="cb468-9"><a href="further-explanations.html#cb468-9" tabindex="-1"></a></span>
<span id="cb468-10"><a href="further-explanations.html#cb468-10" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fl">0.8</span> <span class="sc">+</span> <span class="fl">1.6</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">3</span>)</span>
<span id="cb468-11"><a href="further-explanations.html#cb468-11" tabindex="-1"></a></span>
<span id="cb468-12"><a href="further-explanations.html#cb468-12" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x,y, categorical_variable)</span>
<span id="cb468-13"><a href="further-explanations.html#cb468-13" tabindex="-1"></a></span>
<span id="cb468-14"><a href="further-explanations.html#cb468-14" tabindex="-1"></a><span class="co"># And or Model</span></span>
<span id="cb468-15"><a href="further-explanations.html#cb468-15" tabindex="-1"></a><span class="co">#running a linear regression</span></span>
<span id="cb468-16"><a href="further-explanations.html#cb468-16" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, </span>
<span id="cb468-17"><a href="further-explanations.html#cb468-17" tabindex="-1"></a>             <span class="at">data =</span> df) </span></code></pre></div>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="further-explanations.html#cb469-1" tabindex="-1"></a><span class="co">#First, we calculate the predictions for y</span></span>
<span id="cb469-2"><a href="further-explanations.html#cb469-2" tabindex="-1"></a>df<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> <span class="fl">1.6821</span> <span class="sc">+</span> <span class="fl">1.5394</span><span class="sc">*</span>df<span class="sc">$</span>x </span>
<span id="cb469-3"><a href="further-explanations.html#cb469-3" tabindex="-1"></a></span>
<span id="cb469-4"><a href="further-explanations.html#cb469-4" tabindex="-1"></a><span class="co">#We get the Residuals by subtracting our actual y from y_hat</span></span>
<span id="cb469-5"><a href="further-explanations.html#cb469-5" tabindex="-1"></a>df<span class="sc">$</span>residuals <span class="ot">&lt;-</span> df<span class="sc">$</span>y <span class="sc">-</span> df<span class="sc">$</span>y_hat </span>
<span id="cb469-6"><a href="further-explanations.html#cb469-6" tabindex="-1"></a></span>
<span id="cb469-7"><a href="further-explanations.html#cb469-7" tabindex="-1"></a><span class="co">#cheking it</span></span>
<span id="cb469-8"><a href="further-explanations.html#cb469-8" tabindex="-1"></a><span class="fu">head</span>(df) </span></code></pre></div>
<pre><code>##          x         y categorical_variable
## 1 2.875775  6.680633                    0
## 2 7.883051 12.527668                    1
## 3 4.089769 10.029008                    0
## 4 8.830174 17.562679                    1
## 5 9.404673 18.312220                    1
## 6 0.455565  3.594825                    0
##       y_hat  residuals
## 1  6.109068  0.5715646
## 2 13.817269 -1.2896015
## 3  7.977891  2.0511170
## 4 15.275270  2.2874090
## 5 16.159653  2.1525664
## 6  2.383397  1.2114280</code></pre>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="further-explanations.html#cb471-1" tabindex="-1"></a><span class="co">#We could have done that automatically with R as well !  </span></span>
<span id="cb471-2"><a href="further-explanations.html#cb471-2" tabindex="-1"></a>df<span class="sc">$</span>residuals_auto <span class="ot">&lt;-</span> <span class="fu">residuals</span>(model1)</span>
<span id="cb471-3"><a href="further-explanations.html#cb471-3" tabindex="-1"></a></span>
<span id="cb471-4"><a href="further-explanations.html#cb471-4" tabindex="-1"></a><span class="co">#Checking it</span></span>
<span id="cb471-5"><a href="further-explanations.html#cb471-5" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>##          x         y categorical_variable
## 1 2.875775  6.680633                    0
## 2 7.883051 12.527668                    1
## 3 4.089769 10.029008                    0
## 4 8.830174 17.562679                    1
## 5 9.404673 18.312220                    1
## 6 0.455565  3.594825                    0
##       y_hat  residuals residuals_auto
## 1  6.109068  0.5715646      0.5716538
## 2 13.817269 -1.2896015     -1.2892991
## 3  7.977891  2.0511170      2.0512579
## 4 15.275270  2.2874090      2.2877518
## 5 16.159653  2.1525664      2.1529337
## 6  2.383397  1.2114280      1.2114141</code></pre>
<p>Let us have a look at a so-called <strong>Residual Plot</strong>: On the x-axis you plot the <strong>fitted values</strong>, thus our <code>y_hat</code>. On the y-axis you plot the residuals, thus <span class="math inline">\(y-\hat{y}\)</span>. Then you plot a horizontal line at y = 0. All dots on those lines show us the values correctly predicted by our model.</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="further-explanations.html#cb473-1" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(x, residuals_auto)) <span class="sc">+</span> </span>
<span id="cb473-2"><a href="further-explanations.html#cb473-2" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb473-3"><a href="further-explanations.html#cb473-3" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb473-4"><a href="further-explanations.html#cb473-4" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Residuals&quot;</span>, <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">1</span>), </span>
<span id="cb473-5"><a href="further-explanations.html#cb473-5" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="dv">6</span>)) <span class="sc">+</span></span>
<span id="cb473-6"><a href="further-explanations.html#cb473-6" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Fitted Values&quot;</span>, <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">1</span>), </span>
<span id="cb473-7"><a href="further-explanations.html#cb473-7" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb473-8"><a href="further-explanations.html#cb473-8" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/residual%20plot-1.png" width="672" /></p>
</div>
<div id="homoskedasticity-and-heteroskedasticity" class="section level5 hasAnchor" number="7.2.1.1.2">
<h5><span class="header-section-number">7.2.1.1.2</span> Homoskedasticity and Heteroskedasticity<a href="further-explanations.html#homoskedasticity-and-heteroskedasticity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>One assumption of linear regression is that the variance of the error term is not correlated with our independent variable. Well, that is quite technocratic and means basically, that the residuals are distributed equally over the independent variables. Let us plot it to get a visual intuition:</p>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb474-1"><a href="further-explanations.html#cb474-1" tabindex="-1"></a><span class="co">#setting seed for reproduciability</span></span>
<span id="cb474-2"><a href="further-explanations.html#cb474-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb474-3"><a href="further-explanations.html#cb474-3" tabindex="-1"></a></span>
<span id="cb474-4"><a href="further-explanations.html#cb474-4" tabindex="-1"></a><span class="co"># Generate some data</span></span>
<span id="cb474-5"><a href="further-explanations.html#cb474-5" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">150</span>, <span class="fl">0.05</span>, <span class="dv">1</span>)</span>
<span id="cb474-6"><a href="further-explanations.html#cb474-6" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">150</span>, <span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb474-7"><a href="further-explanations.html#cb474-7" tabindex="-1"></a></span>
<span id="cb474-8"><a href="further-explanations.html#cb474-8" tabindex="-1"></a><span class="co">#homoskedastic data </span></span>
<span id="cb474-9"><a href="further-explanations.html#cb474-9" tabindex="-1"></a>y_homo <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> x <span class="sc">+</span> e </span>
<span id="cb474-10"><a href="further-explanations.html#cb474-10" tabindex="-1"></a><span class="co">#heteroskedastic data </span></span>
<span id="cb474-11"><a href="further-explanations.html#cb474-11" tabindex="-1"></a>y_hetero <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> x <span class="sc">+</span> e<span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb474-12"><a href="further-explanations.html#cb474-12" tabindex="-1"></a><span class="co">#making a data frame with both data</span></span>
<span id="cb474-13"><a href="further-explanations.html#cb474-13" tabindex="-1"></a>df_homo_hetero <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y_homo, y_hetero)</span>
<span id="cb474-14"><a href="further-explanations.html#cb474-14" tabindex="-1"></a></span>
<span id="cb474-15"><a href="further-explanations.html#cb474-15" tabindex="-1"></a><span class="co"># Scatterplot with homoscedasticity</span></span>
<span id="cb474-16"><a href="further-explanations.html#cb474-16" tabindex="-1"></a>homoskedastic_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df_homo_hetero, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_homo)) <span class="sc">+</span></span>
<span id="cb474-17"><a href="further-explanations.html#cb474-17" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb474-18"><a href="further-explanations.html#cb474-18" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span><span class="co"># Add linear regression line</span></span>
<span id="cb474-19"><a href="further-explanations.html#cb474-19" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Y&quot;</span>, <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="fl">0.5</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>)) <span class="sc">+</span></span>
<span id="cb474-20"><a href="further-explanations.html#cb474-20" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Homoskedastic Plot&quot;</span>) <span class="sc">+</span></span>
<span id="cb474-21"><a href="further-explanations.html#cb474-21" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb474-22"><a href="further-explanations.html#cb474-22" tabindex="-1"></a></span>
<span id="cb474-23"><a href="further-explanations.html#cb474-23" tabindex="-1"></a><span class="co"># Scatterplot with heteroscedasticity</span></span>
<span id="cb474-24"><a href="further-explanations.html#cb474-24" tabindex="-1"></a>heteroskedastic_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df_homo_hetero, </span>
<span id="cb474-25"><a href="further-explanations.html#cb474-25" tabindex="-1"></a>                               <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_hetero)) <span class="sc">+</span></span>
<span id="cb474-26"><a href="further-explanations.html#cb474-26" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb474-27"><a href="further-explanations.html#cb474-27" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span><span class="co">#Add linear regression line</span></span>
<span id="cb474-28"><a href="further-explanations.html#cb474-28" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Heteroskedastic Plot&quot;</span>) <span class="sc">+</span></span>
<span id="cb474-29"><a href="further-explanations.html#cb474-29" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Y&quot;</span>, <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="fl">0.5</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>)) <span class="sc">+</span></span>
<span id="cb474-30"><a href="further-explanations.html#cb474-30" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb474-31"><a href="further-explanations.html#cb474-31" tabindex="-1"></a></span>
<span id="cb474-32"><a href="further-explanations.html#cb474-32" tabindex="-1"></a><span class="co"># Combine plots using facet_wrap</span></span>
<span id="cb474-33"><a href="further-explanations.html#cb474-33" tabindex="-1"></a>facet_plots <span class="ot">&lt;-</span> <span class="fu">ggarrange</span>(homoskedastic_plot, heteroskedastic_plot, <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb474-34"><a href="further-explanations.html#cb474-34" tabindex="-1"></a></span>
<span id="cb474-35"><a href="further-explanations.html#cb474-35" tabindex="-1"></a><span class="co"># Print the combined plots</span></span>
<span id="cb474-36"><a href="further-explanations.html#cb474-36" tabindex="-1"></a><span class="fu">print</span>(facet_plots)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/homo%20and%20hetero%20plot-1.png" width="672" /></p>
<p>In the left plot, you see the homoskedastic data. The dots are equally and constantly distributed around the fitted line. However, the right plot shows that the more the independent variable <strong>x</strong> increases, the more the observations are increasing. The dots are not constantly distributed over the line. In the case, that the data is heteroskedastic, then this is a problem. You could try to transform the independent variable by taking the logarithm (We will look into that later). You could also use so-called heteroskedastic regression, but this an advanced model.</p>
</div>
</div>
<div id="tss-ess-and-r2" class="section level4 hasAnchor" number="7.2.1.2">
<h4><span class="header-section-number">7.2.1.2</span> TSS, ESS and <span class="math inline">\(R^2\)</span><a href="further-explanations.html#tss-ess-and-r2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now that we have the Residuals, we can calculate the Total Sum of Square (TSS), the Explained Sum of Squared ESS, and <span class="math inline">\(R^2\)</span>:</p>
<ul>
<li><p>TSS (Variation in the DV): <span class="math inline">\(TSS =\sum(y_i - \bar{y})^2\)</span> , we just subtract our actual values (<code>df$y</code>) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable.</p></li>
<li><p>ESS (Variation we explain in the DV): <span class="math inline">\(ESS = \sum(\hat{y_i} - \bar{y})^2\)</span> , now we use our predicted values (<code>df$y_hat</code>) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model.</p></li>
<li><p><span class="math inline">\(R^2\)</span> (The Variation we can predict from our model): <span class="math inline">\(R^2 = \frac{ESS}{TSS}\)</span> , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this <span class="math inline">\(R^2\)</span> is 1. If our model could not explain anything the variation would be 0, since the values of both cannot be negative. Let us calculate them:</p></li>
</ul>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="further-explanations.html#cb475-1" tabindex="-1"></a><span class="co">#total sum of squares</span></span>
<span id="cb475-2"><a href="further-explanations.html#cb475-2" tabindex="-1"></a>tss <span class="ot">&lt;-</span> <span class="fu">sum</span>((df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb475-3"><a href="further-explanations.html#cb475-3" tabindex="-1"></a><span class="co">#explained sum of squares</span></span>
<span id="cb475-4"><a href="further-explanations.html#cb475-4" tabindex="-1"></a>ess <span class="ot">&lt;-</span> <span class="fu">sum</span>((df<span class="sc">$</span>y_hat <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb475-5"><a href="further-explanations.html#cb475-5" tabindex="-1"></a><span class="co">#caculating r squared</span></span>
<span id="cb475-6"><a href="further-explanations.html#cb475-6" tabindex="-1"></a>r_squared <span class="ot">&lt;-</span> ess<span class="sc">/</span>tss</span>
<span id="cb475-7"><a href="further-explanations.html#cb475-7" tabindex="-1"></a></span>
<span id="cb475-8"><a href="further-explanations.html#cb475-8" tabindex="-1"></a><span class="co">#Printing it</span></span>
<span id="cb475-9"><a href="further-explanations.html#cb475-9" tabindex="-1"></a>r_squared</span></code></pre></div>
<pre><code>## [1] 0.7631199</code></pre>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="further-explanations.html#cb477-1" tabindex="-1"></a><span class="co">#Summarizing it</span></span>
<span id="cb477-2"><a href="further-explanations.html#cb477-2" tabindex="-1"></a><span class="fu">summary</span>(model1)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.7630777</code></pre>
</div>
<div id="influential-outliers" class="section level4 hasAnchor" number="7.2.1.3">
<h4><span class="header-section-number">7.2.1.3</span> Influential Outliers<a href="further-explanations.html#influential-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Outliers are extremely deviating values, which can impact our analysis and bias it. Therefore, we have to check, if our data contains such values. But first let us see how they can impact our data:</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="further-explanations.html#cb479-1" tabindex="-1"></a><span class="co">#set seed </span></span>
<span id="cb479-2"><a href="further-explanations.html#cb479-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">069</span>)</span>
<span id="cb479-3"><a href="further-explanations.html#cb479-3" tabindex="-1"></a></span>
<span id="cb479-4"><a href="further-explanations.html#cb479-4" tabindex="-1"></a><span class="co">#generate fake data with outlier </span></span>
<span id="cb479-5"><a href="further-explanations.html#cb479-5" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(<span class="dv">10</span>, <span class="at">min =</span> <span class="dv">30</span>, <span class="at">max =</span> <span class="dv">70</span>))</span>
<span id="cb479-6"><a href="further-explanations.html#cb479-6" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10</span> , <span class="at">mean =</span> <span class="dv">200</span>, <span class="at">sd =</span> <span class="dv">50</span>)</span>
<span id="cb479-7"><a href="further-explanations.html#cb479-7" tabindex="-1"></a>y1[<span class="dv">9</span>] <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb479-8"><a href="further-explanations.html#cb479-8" tabindex="-1"></a>data_outlier <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, y1)</span>
<span id="cb479-9"><a href="further-explanations.html#cb479-9" tabindex="-1"></a></span>
<span id="cb479-10"><a href="further-explanations.html#cb479-10" tabindex="-1"></a><span class="co">#Model with Outlier </span></span>
<span id="cb479-11"><a href="further-explanations.html#cb479-11" tabindex="-1"></a>model_outlier <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1 <span class="sc">~</span> x1) </span>
<span id="cb479-12"><a href="further-explanations.html#cb479-12" tabindex="-1"></a></span>
<span id="cb479-13"><a href="further-explanations.html#cb479-13" tabindex="-1"></a><span class="co">#Model without Outlier</span></span>
<span id="cb479-14"><a href="further-explanations.html#cb479-14" tabindex="-1"></a>model_without_outlier <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1[<span class="sc">-</span><span class="dv">9</span>] <span class="sc">~</span> x1[<span class="sc">-</span><span class="dv">9</span>]) </span>
<span id="cb479-15"><a href="further-explanations.html#cb479-15" tabindex="-1"></a></span>
<span id="cb479-16"><a href="further-explanations.html#cb479-16" tabindex="-1"></a><span class="co">#Plotting the Data </span></span>
<span id="cb479-17"><a href="further-explanations.html#cb479-17" tabindex="-1"></a></span>
<span id="cb479-18"><a href="further-explanations.html#cb479-18" tabindex="-1"></a><span class="co"># Scatter plot with points</span></span>
<span id="cb479-19"><a href="further-explanations.html#cb479-19" tabindex="-1"></a><span class="fu">ggplot</span>(data_outlier, <span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> y1)) <span class="sc">+</span></span>
<span id="cb479-20"><a href="further-explanations.html#cb479-20" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">20</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb479-21"><a href="further-explanations.html#cb479-21" tabindex="-1"></a>  <span class="co"># Regression line for the model with outlier</span></span>
<span id="cb479-22"><a href="further-explanations.html#cb479-22" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">slope =</span> model_outlier<span class="sc">$</span>coefficients[<span class="dv">2</span>], <span class="at">intercept =</span></span>
<span id="cb479-23"><a href="further-explanations.html#cb479-23" tabindex="-1"></a>              model_outlier<span class="sc">$</span>coefficients[<span class="dv">1</span>], </span>
<span id="cb479-24"><a href="further-explanations.html#cb479-24" tabindex="-1"></a>              <span class="at">color =</span> <span class="st">&quot;Model with Outlier&quot;</span>), <span class="at">linewidth =</span> <span class="fl">0.75</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb479-25"><a href="further-explanations.html#cb479-25" tabindex="-1"></a>  <span class="co"># Regression line for the model without outlier</span></span>
<span id="cb479-26"><a href="further-explanations.html#cb479-26" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">slope =</span> model_without_outlier<span class="sc">$</span>coefficients[<span class="dv">2</span>], </span>
<span id="cb479-27"><a href="further-explanations.html#cb479-27" tabindex="-1"></a>              <span class="at">intercept =</span> model_without_outlier<span class="sc">$</span>coefficients[<span class="dv">1</span>], </span>
<span id="cb479-28"><a href="further-explanations.html#cb479-28" tabindex="-1"></a>              <span class="at">color =</span> <span class="st">&quot;Model without Outlier&quot;</span>), <span class="at">linewidth =</span> <span class="fl">0.75</span>, </span>
<span id="cb479-29"><a href="further-explanations.html#cb479-29" tabindex="-1"></a>              <span class="at">show.legend =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb479-30"><a href="further-explanations.html#cb479-30" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Independent Variable&quot;</span>) <span class="sc">+</span></span>
<span id="cb479-31"><a href="further-explanations.html#cb479-31" tabindex="-1"></a>  <span class="co"># Adding legend</span></span>
<span id="cb479-32"><a href="further-explanations.html#cb479-32" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> </span>
<span id="cb479-33"><a href="further-explanations.html#cb479-33" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="fu">c</span>(<span class="fl">0.15</span>,<span class="fl">0.9</span>), </span>
<span id="cb479-34"><a href="further-explanations.html#cb479-34" tabindex="-1"></a>        <span class="at">legend.title =</span> <span class="fu">element_blank</span>()) </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/plotting%20bias%20through%20outliers-1.png" width="672" /></p>
<p>We see that this one observation completely biases our sample. But how do we find out, which observation is an influential outlier? There is a metric called <strong>Cook’s Distance</strong>, we can use. Let us do it and plot it in R.</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="further-explanations.html#cb480-1" tabindex="-1"></a><span class="co">#Cooks Distance can be calculated with a built-in function</span></span>
<span id="cb480-2"><a href="further-explanations.html#cb480-2" tabindex="-1"></a>data_outlier<span class="sc">$</span>cooks_distance <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(model_outlier) </span>
<span id="cb480-3"><a href="further-explanations.html#cb480-3" tabindex="-1"></a></span>
<span id="cb480-4"><a href="further-explanations.html#cb480-4" tabindex="-1"></a><span class="co">#Plotting it</span></span>
<span id="cb480-5"><a href="further-explanations.html#cb480-5" tabindex="-1"></a><span class="fu">ggplot</span>(data_outlier, <span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> cooks_distance)) <span class="sc">+</span> </span>
<span id="cb480-6"><a href="further-explanations.html#cb480-6" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">colour =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb480-7"><a href="further-explanations.html#cb480-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Cook&#39;s Distance&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Independent Variables&quot;</span>) <span class="sc">+</span> </span>
<span id="cb480-8"><a href="further-explanations.html#cb480-8" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span> </span>
<span id="cb480-9"><a href="further-explanations.html#cb480-9" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/plotting%20cooks%20distance-1.png" width="672" /></p>
<p>We can clearly see that <strong>Cook’s Distance</strong> detected the outlier. The rule is that values with a cooks distance bigger than 1 have to be eliminated. You can do that with the <code>filter()</code> function and run the model afterward again without the outliers.</p>
</div>
<div id="functional-form" class="section level4 hasAnchor" number="7.2.1.4">
<h4><span class="header-section-number">7.2.1.4</span> Functional Form<a href="further-explanations.html#functional-form" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>First let us get our data:</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="further-explanations.html#cb481-1" tabindex="-1"></a><span class="co">#Simulate further data</span></span>
<span id="cb481-2"><a href="further-explanations.html#cb481-2" tabindex="-1"></a>X_quadratic <span class="ot">&lt;-</span> X <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">50</span>, <span class="at">min =</span> <span class="sc">-</span><span class="dv">5</span>, <span class="at">max =</span> <span class="dv">5</span>)</span>
<span id="cb481-3"><a href="further-explanations.html#cb481-3" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, <span class="at">sd =</span> <span class="dv">1</span>)  </span>
<span id="cb481-4"><a href="further-explanations.html#cb481-4" tabindex="-1"></a></span>
<span id="cb481-5"><a href="further-explanations.html#cb481-5" tabindex="-1"></a><span class="co">#True relation</span></span>
<span id="cb481-6"><a href="further-explanations.html#cb481-6" tabindex="-1"></a>Y_quadratic <span class="ot">&lt;-</span> X<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> X <span class="sc">+</span> u</span>
<span id="cb481-7"><a href="further-explanations.html#cb481-7" tabindex="-1"></a></span>
<span id="cb481-8"><a href="further-explanations.html#cb481-8" tabindex="-1"></a><span class="co">#Making a data frame out of it</span></span>
<span id="cb481-9"><a href="further-explanations.html#cb481-9" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X_quadratic, Y_quadratic)</span></code></pre></div>
<p>Linear regression is a mathematical model. Therefore it is based on assumptions. But we should not just assume them, we should test them! One assumption is that linearity is assumed between X and Y. But that can be problematic consider following example:</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="further-explanations.html#cb482-1" tabindex="-1"></a><span class="co"># estimate a simple regression model </span></span>
<span id="cb482-2"><a href="further-explanations.html#cb482-2" tabindex="-1"></a>model_simple <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y_quadratic <span class="sc">~</span> X_quadratic, <span class="at">data =</span> df2)</span>
<span id="cb482-3"><a href="further-explanations.html#cb482-3" tabindex="-1"></a></span>
<span id="cb482-4"><a href="further-explanations.html#cb482-4" tabindex="-1"></a><span class="co"># Summarize it</span></span>
<span id="cb482-5"><a href="further-explanations.html#cb482-5" tabindex="-1"></a><span class="fu">summary</span>(model_simple)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y_quadratic ~ X_quadratic, data = df2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.703 -6.508 -1.478  5.277 17.464 
## 
## Coefficients:
##             Estimate Std. Error t value
## (Intercept)   7.5912     1.0914   6.955
## X_quadratic   2.0220     0.3948   5.122
##             Pr(&gt;|t|)    
## (Intercept) 8.61e-09 ***
## X_quadratic 5.32e-06 ***
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 7.709 on 48 degrees of freedom
## Multiple R-squared:  0.3534, Adjusted R-squared:  0.3399 
## F-statistic: 26.23 on 1 and 48 DF,  p-value: 5.321e-06</code></pre>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="further-explanations.html#cb484-1" tabindex="-1"></a><span class="co">#Plot it</span></span>
<span id="cb484-2"><a href="further-explanations.html#cb484-2" tabindex="-1"></a><span class="fu">ggplot</span>(df2, <span class="fu">aes</span>(<span class="at">x =</span> X_quadratic, <span class="at">y =</span> Y_quadratic)) <span class="sc">+</span> </span>
<span id="cb484-3"><a href="further-explanations.html#cb484-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">20</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb484-4"><a href="further-explanations.html#cb484-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> </span>
<span id="cb484-5"><a href="further-explanations.html#cb484-5" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdownproj_files/figure-html/plotting%20linear%20fit%20through%20quadratic%20form-1.png" width="672" /></p>
<p>As you can see something look wrong. There seems to be a correlation between the two variables, but it does not seem linear. In such a case it does make sense to square the independent variable and run the regression again:</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="further-explanations.html#cb486-1" tabindex="-1"></a><span class="co"># estimate a simple regression model </span></span>
<span id="cb486-2"><a href="further-explanations.html#cb486-2" tabindex="-1"></a>model_quadratic <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y_quadratic <span class="sc">~</span> X_quadratic<span class="sc">^</span><span class="dv">2</span>, <span class="at">data =</span> df2)</span>
<span id="cb486-3"><a href="further-explanations.html#cb486-3" tabindex="-1"></a></span>
<span id="cb486-4"><a href="further-explanations.html#cb486-4" tabindex="-1"></a><span class="co">#Summarize it </span></span>
<span id="cb486-5"><a href="further-explanations.html#cb486-5" tabindex="-1"></a><span class="fu">summary</span>(model_quadratic)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y_quadratic ~ X_quadratic^2, data = df2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.703 -6.508 -1.478  5.277 17.464 
## 
## Coefficients:
##             Estimate Std. Error t value
## (Intercept)   7.5912     1.0914   6.955
## X_quadratic   2.0220     0.3948   5.122
##             Pr(&gt;|t|)    
## (Intercept) 8.61e-09 ***
## X_quadratic 5.32e-06 ***
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 7.709 on 48 degrees of freedom
## Multiple R-squared:  0.3534, Adjusted R-squared:  0.3399 
## F-statistic: 26.23 on 1 and 48 DF,  p-value: 5.321e-06</code></pre>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="further-explanations.html#cb488-1" tabindex="-1"></a><span class="co">#Plot it</span></span>
<span id="cb488-2"><a href="further-explanations.html#cb488-2" tabindex="-1"></a><span class="fu">ggplot</span>(df2, <span class="fu">aes</span>(<span class="at">x =</span> X_quadratic, <span class="at">y =</span> Y_quadratic)) <span class="sc">+</span> </span>
<span id="cb488-3"><a href="further-explanations.html#cb488-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">20</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb488-4"><a href="further-explanations.html#cb488-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">2</span>), </span>
<span id="cb488-5"><a href="further-explanations.html#cb488-5" tabindex="-1"></a>              <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, </span>
<span id="cb488-6"><a href="further-explanations.html#cb488-6" tabindex="-1"></a>              <span class="at">se =</span> <span class="cn">FALSE</span>,) <span class="sc">+</span> </span>
<span id="cb488-7"><a href="further-explanations.html#cb488-7" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;X&quot;</span>, <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">1</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb488-8"><a href="further-explanations.html#cb488-8" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Y&quot;</span>) <span class="sc">+</span></span>
<span id="cb488-9"><a href="further-explanations.html#cb488-9" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/plotting%20quadratic%20fit%20with%20quadratic%20form-1.png" width="672" /></p>
<p>Well that looks better and is the proper way to deal with quadratic relationships in linear regression. Well, data can take on not only a quadratic form, it could also take on a form of a square-root function. I will show the most classical example of such a functional form. The <code>gapminder</code> data is loaded. It contains data about the average life expectancy (<code>lifeExp</code>)and the GDP per capita (<code>gdpPercap</code>) of countries in different years. Let us look if the GDP per Capita is correlated with Life Expectancy:</p>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="further-explanations.html#cb489-1" tabindex="-1"></a><span class="co">#checking the data</span></span>
<span id="cb489-2"><a href="further-explanations.html#cb489-2" tabindex="-1"></a><span class="fu">head</span>(gapminder)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 6
##   country     continent  year lifeExp    pop
##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;
## 1 Afghanistan Asia       1952    28.8 8.43e6
## 2 Afghanistan Asia       1957    30.3 9.24e6
## 3 Afghanistan Asia       1962    32.0 1.03e7
## 4 Afghanistan Asia       1967    34.0 1.15e7
## 5 Afghanistan Asia       1972    36.1 1.31e7
## 6 Afghanistan Asia       1977    38.4 1.49e7
## # ℹ 1 more variable: gdpPercap &lt;dbl&gt;</code></pre>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="further-explanations.html#cb491-1" tabindex="-1"></a><span class="co">#Plotting it</span></span>
<span id="cb491-2"><a href="further-explanations.html#cb491-2" tabindex="-1"></a><span class="fu">ggplot</span>(gapminder, <span class="fu">aes</span>(gdpPercap, lifeExp)) <span class="sc">+</span> </span>
<span id="cb491-3"><a href="further-explanations.html#cb491-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb491-4"><a href="further-explanations.html#cb491-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> </span>
<span id="cb491-5"><a href="further-explanations.html#cb491-5" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Life Expectancy&quot;</span>, <span class="fu">seq</span>(<span class="dv">30</span>, <span class="dv">80</span>, <span class="dv">10</span>), </span>
<span id="cb491-6"><a href="further-explanations.html#cb491-6" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">30</span>, <span class="dv">80</span>)) <span class="sc">+</span> </span>
<span id="cb491-7"><a href="further-explanations.html#cb491-7" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdownproj_files/figure-html/unlogged%20fit-1.png" width="672" /></p>
<p>Well, that looks terrible. What can we do? I already mentioned that the fitted line looks like a square-root function (<span class="math inline">\(y = \beta{\sqrt{x}}\)</span> ). When you take the logarithm of square root, you neutralize the square root and only x remains <span class="math inline">\(\log{(\sqrt{x})} = x\)</span>. When you do that the functional form changes to <span class="math inline">\(y = \beta{x}\)</span>. Well, that is exactly the systematic component we are after:</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="further-explanations.html#cb493-1" tabindex="-1"></a><span class="fu">ggplot</span>(gapminder, <span class="fu">aes</span>(<span class="fu">log</span>(gdpPercap), lifeExp)) <span class="sc">+</span> </span>
<span id="cb493-2"><a href="further-explanations.html#cb493-2" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb493-3"><a href="further-explanations.html#cb493-3" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> </span>
<span id="cb493-4"><a href="further-explanations.html#cb493-4" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Life Expectancy&quot;</span>, <span class="fu">seq</span>(<span class="dv">30</span>, <span class="dv">80</span>, <span class="dv">10</span>), </span>
<span id="cb493-5"><a href="further-explanations.html#cb493-5" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">30</span>, <span class="dv">80</span>)) <span class="sc">+</span> </span>
<span id="cb493-6"><a href="further-explanations.html#cb493-6" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;GDP per Capita&quot;</span>) <span class="sc">+</span></span>
<span id="cb493-7"><a href="further-explanations.html#cb493-7" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdownproj_files/figure-html/logged%20fit-1.png" width="672" /></p>
<p>This looks more like what we want to achieve. What can we see in that plot? When we run a model we want to take the logarithm of the independent variable, when we expect the following: If the most observations of a variable are low, but some observations are extremely high such functional forms can occur. In our example, most of the countries have a low GDP per Capita, but some countries such as the Western European countries or the USA have such a a high level of GDP per Capita, they change the functional form of the fitted line. These are influential outliers, but too much to delete them. It could bias the representativeness of the sample, therefore we can deal with them by taking the logarithm.</p>
</div>
<div id="independent-observation" class="section level4 hasAnchor" number="7.2.1.5">
<h4><span class="header-section-number">7.2.1.5</span> Independent Observation<a href="further-explanations.html#independent-observation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Another assumption of the linear regression model is the independent, identically distributed (i.i.d) assumption. That sounds complicated but it really is not. Consider following plot:</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="further-explanations.html#cb495-1" tabindex="-1"></a><span class="co"># Getting the Data</span></span>
<span id="cb495-2"><a href="further-explanations.html#cb495-2" tabindex="-1"></a><span class="co"># set seed</span></span>
<span id="cb495-3"><a href="further-explanations.html#cb495-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb495-4"><a href="further-explanations.html#cb495-4" tabindex="-1"></a></span>
<span id="cb495-5"><a href="further-explanations.html#cb495-5" tabindex="-1"></a><span class="co"># generate a date vector</span></span>
<span id="cb495-6"><a href="further-explanations.html#cb495-6" tabindex="-1"></a>date <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">as.Date</span>(<span class="st">&quot;1960/1/1&quot;</span>), <span class="fu">as.Date</span>(<span class="st">&quot;2020/1/1&quot;</span>), <span class="st">&quot;years&quot;</span>)</span>
<span id="cb495-7"><a href="further-explanations.html#cb495-7" tabindex="-1"></a></span>
<span id="cb495-8"><a href="further-explanations.html#cb495-8" tabindex="-1"></a><span class="co"># initialize the employment vector</span></span>
<span id="cb495-9"><a href="further-explanations.html#cb495-9" tabindex="-1"></a>y_time <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5000</span>, <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="fu">length</span>(date)<span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb495-10"><a href="further-explanations.html#cb495-10" tabindex="-1"></a></span>
<span id="cb495-11"><a href="further-explanations.html#cb495-11" tabindex="-1"></a><span class="co"># generate time series observations with random influences</span></span>
<span id="cb495-12"><a href="further-explanations.html#cb495-12" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(date)) {</span>
<span id="cb495-13"><a href="further-explanations.html#cb495-13" tabindex="-1"></a>  </span>
<span id="cb495-14"><a href="further-explanations.html#cb495-14" tabindex="-1"></a>    y_time[i] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">50</span> <span class="sc">+</span> <span class="fl">0.98</span> <span class="sc">*</span> y_time[i<span class="dv">-1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">200</span>)</span>
<span id="cb495-15"><a href="further-explanations.html#cb495-15" tabindex="-1"></a>}</span>
<span id="cb495-16"><a href="further-explanations.html#cb495-16" tabindex="-1"></a></span>
<span id="cb495-17"><a href="further-explanations.html#cb495-17" tabindex="-1"></a><span class="co"># Plot it</span></span>
<span id="cb495-18"><a href="further-explanations.html#cb495-18" tabindex="-1"></a>df_time_series <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y_time, date)</span>
<span id="cb495-19"><a href="further-explanations.html#cb495-19" tabindex="-1"></a><span class="fu">ggplot</span>(df_time_series, <span class="fu">aes</span>(date, y_time)) <span class="sc">+</span> </span>
<span id="cb495-20"><a href="further-explanations.html#cb495-20" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb495-21"><a href="further-explanations.html#cb495-21" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Y&quot;</span>) <span class="sc">+</span></span>
<span id="cb495-22"><a href="further-explanations.html#cb495-22" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Year&quot;</span>) <span class="sc">+</span></span>
<span id="cb495-23"><a href="further-explanations.html#cb495-23" tabindex="-1"></a>  <span class="fu">theme_bw</span>()  </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/time%20trend-1.png" width="672" /></p>
<p>If you look at the plot, can we assume that the observation Year = 2000 is independent from the Years before? No, the observation in the years are correlated to each other, thus the assumption is violated. This is basically the huge problem of working with longitudinal data (time-series cross-sectional or panel). If you face such problems there are plenty of other methods to use: Interrupted time series, Difference-in-Difference Designs, Panel-Matching, Fixed-Effects Models etc.</p>
</div>
</div>
<div id="model-fit-multivariate-regression" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Model Fit: Multivariate Regression<a href="further-explanations.html#model-fit-multivariate-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="model-fit-adjusted-r-squared" class="section level4 hasAnchor" number="7.2.2.1">
<h4><span class="header-section-number">7.2.2.1</span> Model Fit: Adjusted R-squared<a href="further-explanations.html#model-fit-adjusted-r-squared" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The last important aspect of Multivariate Regression is the Adjusted R-squared measure. Reconsider, the calculation of the classical R-squared:</p>
<ul>
<li><p>TSS (Variation in the DV): <span class="math inline">\(TSS =\sum(y_i - \bar{y})^2\)</span> , we just subtract our actual values (<code>df$y</code>) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable.</p></li>
<li><p>ESS (Variation we explain in the DV): <span class="math inline">\(ESS = \sum(\hat{y_i} - \bar{y})^2\)</span> , now we use our predicted values (<code>df$y_hat</code>) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model.</p></li>
<li><p><span class="math inline">\(R^2\)</span> (The Variation we can predict from our model): <span class="math inline">\(R^2 = \frac{ESS}{TSS}\)</span> , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this <span class="math inline">\(R^2\)</span> is 1.</p></li>
</ul>
<p>The problem with the classical R-squared is, that if you would add useless independent variables to it, the classical R-squared would decrease, although your model did not increase in explanatory power. This is called <strong>overfitting</strong>. However, adjusted R-squared will account for that problem by introducing a “penalty” for every additional variable. Mathematically, it looks like this:</p>
<p><span class="math display">\[ Adj.R^2 = 1 - \frac{(1 - R^2)*(N - 1)}{N - k - 1} \]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(R^2\)</span> is our classical R-squared calculated (<span class="math inline">\(\frac{TSS}{ESS}\)</span>)</p></li>
<li><p><span class="math inline">\(N\)</span> is the number of observations in our sample</p></li>
<li><p><span class="math inline">\(k\)</span> is the number of independent variables</p></li>
</ul>
<p>In <code>R</code>, we can extract the adjusted R-squared simply from our model in chunk, multivariate regression:</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="further-explanations.html#cb496-1" tabindex="-1"></a><span class="co">#running multivariate model</span></span>
<span id="cb496-2"><a href="further-explanations.html#cb496-2" tabindex="-1"></a>multivariate_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> categorical_variable, <span class="at">data =</span> df)</span>
<span id="cb496-3"><a href="further-explanations.html#cb496-3" tabindex="-1"></a></span>
<span id="cb496-4"><a href="further-explanations.html#cb496-4" tabindex="-1"></a><span class="co">#Getting summary</span></span>
<span id="cb496-5"><a href="further-explanations.html#cb496-5" tabindex="-1"></a><span class="fu">summary</span>(multivariate_model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x + categorical_variable, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5183 -1.3840 -0.5014  1.2393  5.5808 
## 
## Coefficients:
##                       Estimate Std. Error
## (Intercept)             1.9799     1.0679
## x                       1.5557     0.1621
## categorical_variable1  -1.0667     0.9637
##                       t value Pr(&gt;|t|)    
## (Intercept)             1.854   0.0747 .  
## x                       9.596 3.42e-10 ***
## categorical_variable1  -1.107   0.2781    
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 2.533 on 27 degrees of freedom
## Multiple R-squared:  0.7734, Adjusted R-squared:  0.7566 
## F-statistic: 46.07 on 2 and 27 DF,  p-value: 1.982e-09</code></pre>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="further-explanations.html#cb498-1" tabindex="-1"></a><span class="co">#Extract Adjusted R-squared</span></span>
<span id="cb498-2"><a href="further-explanations.html#cb498-2" tabindex="-1"></a><span class="fu">summary</span>(multivariate_model)<span class="sc">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.7565721</code></pre>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="further-explanations.html#cb500-1" tabindex="-1"></a><span class="co">#Calculating by hand</span></span>
<span id="cb500-2"><a href="further-explanations.html#cb500-2" tabindex="-1"></a>adj_r_squared <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> (((<span class="dv">1</span><span class="sc">-</span><span class="fu">summary</span>(multivariate_model)<span class="sc">$</span>r.squared) <span class="sc">*</span> (<span class="fu">nrow</span>(df) <span class="sc">-</span> <span class="dv">1</span>))<span class="sc">/</span>(<span class="fu">nrow</span>(df) <span class="sc">-</span> <span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb500-3"><a href="further-explanations.html#cb500-3" tabindex="-1"></a></span>
<span id="cb500-4"><a href="further-explanations.html#cb500-4" tabindex="-1"></a><span class="co">#printing it</span></span>
<span id="cb500-5"><a href="further-explanations.html#cb500-5" tabindex="-1"></a><span class="fu">print</span>(adj_r_squared)</span></code></pre></div>
<pre><code>## [1] 0.7565721</code></pre>
</div>
<div id="omitted-variable-bias" class="section level4 hasAnchor" number="7.2.2.2">
<h4><span class="header-section-number">7.2.2.2</span> Omitted Variable Bias<a href="further-explanations.html#omitted-variable-bias" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>I already mentioned one (abstract) reason why we should include other variables in our model. But there is more to it: You could find effects between two variables X and Y, but it could be that in Reality there is not an association. For example, let us say you collect data about ice cream and shark attacks. Ice cream sales is your independent variable and you want to explain the number of shark attacks, here is your data:</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="further-explanations.html#cb502-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility </span></span>
<span id="cb502-2"><a href="further-explanations.html#cb502-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)  </span>
<span id="cb502-3"><a href="further-explanations.html#cb502-3" tabindex="-1"></a></span>
<span id="cb502-4"><a href="further-explanations.html#cb502-4" tabindex="-1"></a><span class="co"># Number of data points </span></span>
<span id="cb502-5"><a href="further-explanations.html#cb502-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb502-6"><a href="further-explanations.html#cb502-6" tabindex="-1"></a></span>
<span id="cb502-7"><a href="further-explanations.html#cb502-7" tabindex="-1"></a><span class="co"># Simulate diet data (assuming a normal distribution) </span></span>
<span id="cb502-8"><a href="further-explanations.html#cb502-8" tabindex="-1"></a>temperature <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">1500</span>, <span class="at">sd =</span> <span class="dv">200</span>)  </span>
<span id="cb502-9"><a href="further-explanations.html#cb502-9" tabindex="-1"></a></span>
<span id="cb502-10"><a href="further-explanations.html#cb502-10" tabindex="-1"></a><span class="co"># Simulate exercise data (assuming a normal distribution) </span></span>
<span id="cb502-11"><a href="further-explanations.html#cb502-11" tabindex="-1"></a>ice_cream_sales <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">3</span>, <span class="at">sd =</span> <span class="dv">1</span>)  </span>
<span id="cb502-12"><a href="further-explanations.html#cb502-12" tabindex="-1"></a></span>
<span id="cb502-13"><a href="further-explanations.html#cb502-13" tabindex="-1"></a><span class="co"># Simulate weight loss data </span></span>
<span id="cb502-14"><a href="further-explanations.html#cb502-14" tabindex="-1"></a>violence_crime_true <span class="ot">&lt;-</span> <span class="fl">0.2</span> <span class="sc">*</span> temperature <span class="sc">-</span> </span>
<span id="cb502-15"><a href="further-explanations.html#cb502-15" tabindex="-1"></a>  <span class="fl">0.5</span> <span class="sc">*</span> ice_cream_sales <span class="sc">+</span> </span>
<span id="cb502-16"><a href="further-explanations.html#cb502-16" tabindex="-1"></a>  <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">5</span>) </span>
<span id="cb502-17"><a href="further-explanations.html#cb502-17" tabindex="-1"></a></span>
<span id="cb502-18"><a href="further-explanations.html#cb502-18" tabindex="-1"></a><span class="co"># Create a data frame </span></span>
<span id="cb502-19"><a href="further-explanations.html#cb502-19" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">temperature =</span> temperature,<span class="at">ice_cream_sales =</span> ice_cream_sales,                     <span class="at">violence_crime_true =</span> violence_crime_true)  </span>
<span id="cb502-20"><a href="further-explanations.html#cb502-20" tabindex="-1"></a></span>
<span id="cb502-21"><a href="further-explanations.html#cb502-21" tabindex="-1"></a><span class="co"># Fit a model without including the diet variable </span></span>
<span id="cb502-22"><a href="further-explanations.html#cb502-22" tabindex="-1"></a>model_without_temperature <span class="ot">&lt;-</span> <span class="fu">lm</span>(violence_crime_true <span class="sc">~</span> ice_cream_sales, <span class="at">data =</span> data)</span>
<span id="cb502-23"><a href="further-explanations.html#cb502-23" tabindex="-1"></a></span>
<span id="cb502-24"><a href="further-explanations.html#cb502-24" tabindex="-1"></a><span class="co">#Fit a model with only the temperature variable</span></span>
<span id="cb502-25"><a href="further-explanations.html#cb502-25" tabindex="-1"></a></span>
<span id="cb502-26"><a href="further-explanations.html#cb502-26" tabindex="-1"></a>model_with_only_temperature <span class="ot">&lt;-</span> <span class="fu">lm</span>(violence_crime_true <span class="sc">~</span>temperature,                        <span class="at">data =</span> data)</span>
<span id="cb502-27"><a href="further-explanations.html#cb502-27" tabindex="-1"></a></span>
<span id="cb502-28"><a href="further-explanations.html#cb502-28" tabindex="-1"></a><span class="co"># Fit a model including both diet and exercise variables </span></span>
<span id="cb502-29"><a href="further-explanations.html#cb502-29" tabindex="-1"></a>model_with_temperature <span class="ot">&lt;-</span> <span class="fu">lm</span>(violence_crime_true <span class="sc">~</span> ice_cream_sales <span class="sc">+</span> temperature,                        <span class="at">data =</span> data)  </span>
<span id="cb502-30"><a href="further-explanations.html#cb502-30" tabindex="-1"></a></span>
<span id="cb502-31"><a href="further-explanations.html#cb502-31" tabindex="-1"></a><span class="co"># Output the summary of both models </span></span>
<span id="cb502-32"><a href="further-explanations.html#cb502-32" tabindex="-1"></a><span class="fu">summary</span>(model_without_temperature) </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = violence_crime_true ~ ice_cream_sales, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -94.788 -23.736  -2.348  22.264  98.754 
## 
## Coefficients:
##                 Estimate Std. Error t value
## (Intercept)      287.681     11.623  24.751
## ice_cream_sales    4.090      3.741   1.093
##                 Pr(&gt;|t|)    
## (Intercept)       &lt;2e-16 ***
## ice_cream_sales    0.277    
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 35.94 on 98 degrees of freedom
## Multiple R-squared:  0.01205,    Adjusted R-squared:  0.001969 
## F-statistic: 1.195 on 1 and 98 DF,  p-value: 0.2769</code></pre>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="further-explanations.html#cb504-1" tabindex="-1"></a><span class="fu">summary</span>(model_with_only_temperature)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = violence_crime_true ~ temperature, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q 
## -13.9969  -3.8454   0.1718   2.9121 
##      Max 
##  11.7502 
## 
## Coefficients:
##              Estimate Std. Error t value
## (Intercept) -3.535495   4.576293  -0.773
## temperature  0.201591   0.003021  66.727
##             Pr(&gt;|t|)    
## (Intercept)    0.442    
## temperature   &lt;2e-16 ***
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 5.307 on 98 degrees of freedom
## Multiple R-squared:  0.9785, Adjusted R-squared:  0.9782 
## F-statistic:  4452 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="further-explanations.html#cb506-1" tabindex="-1"></a><span class="fu">summary</span>(model_with_temperature)  </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = violence_crime_true ~ ice_cream_sales + temperature, 
##     data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q 
## -14.4770  -3.6734  -0.0914   2.9449 
##      Max 
##  12.1843 
## 
## Coefficients:
##                  Estimate Std. Error
## (Intercept)     -2.396657   4.694786
## ice_cream_sales -0.596143   0.556470
## temperature      0.202005   0.003043
##                 t value Pr(&gt;|t|)    
## (Intercept)      -0.510    0.611    
## ice_cream_sales  -1.071    0.287    
## temperature      66.373   &lt;2e-16 ***
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 5.303 on 97 degrees of freedom
## Multiple R-squared:  0.9787, Adjusted R-squared:  0.9783 
## F-statistic:  2230 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="further-explanations.html#cb508-1" tabindex="-1"></a><span class="co">#Let us display both models next to each other</span></span>
<span id="cb508-2"><a href="further-explanations.html#cb508-2" tabindex="-1"></a><span class="co">#EDIT: I created this function specifically, the code for the function is at the top.   </span></span>
<span id="cb508-3"><a href="further-explanations.html#cb508-3" tabindex="-1"></a><span class="fu">table_ovb</span>(model_without_temperature, model_with_temperature)</span></code></pre></div>
<table class="table" style="color: black; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Model without Temperature
</th>
<th style="text-align:right;">
Model with Temperature
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
287.680876
</td>
<td style="text-align:right;">
-2.3966572
</td>
</tr>
<tr>
<td style="text-align:left;">
Ice Cream Sales
</td>
<td style="text-align:right;">
4.090356
</td>
<td style="text-align:right;">
-0.5961434
</td>
</tr>
<tr>
<td style="text-align:left;">
Temperature
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
0.2020051
</td>
</tr>
</tbody>
</table>
<p>We can see that the coefficient changes dramatically. What happened? Well, one important assumption of linear regression is that the error term captures all variance not explained by our model and is <strong>not correlated</strong> with the independent variable(s) nor the dependent variable. But if there is unexplained variation in our model that is correlated with our independent variable, then this assumption is violated. In our example, we can see that ice cream sales coefficient in the first model is biased, because ice cream sales and is correlated to temperature. The warmer it gets, the more ice cream is sold. But, the warmer it gets, the more violent people get, therefore we have an omitted variable and that is temperature. When we include temperature in the model, we see the problem of omitted variable bias: It biases our coefficients, by either overestimating (like in our example) or by underestimating it. What we should do in such a case, is to delete the omitted variable, which is the drastically changing variable (ice cream sales in our case). This is also the reason, why people talk about additional variables as control variables in a multiple linear model. This way you can control if an association between two variables is due to omitted variable bias or other variables, which can explain the variation better.</p>
</div>
<div id="multicollinearity" class="section level4 hasAnchor" number="7.2.2.3">
<h4><span class="header-section-number">7.2.2.3</span> Multicollinearity<a href="further-explanations.html#multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Another, and I promise, the last OLS assumption, which has to tested is that <strong>there is no Multicollinearity</strong>. The concept is simple: The independent variables should not be correlated. In our previous example, ice cream sales and temperature were correlated. This would have hurt these assumption. In strong cases, multicollinearity can bias our estimates, so that they gain statistical significance and lead us to wrong conclusions. Let us look at an obvious example. You want to find out how the grades of children is affected by different factors. You choose 2 factors: The time they spent on doing their homework (<em>learning time</em>) and the time they spent on playing video games (<em>gaming time</em>). The systematic component looks like this:</p>
<p><span class="math display">\[
Grades_i = \beta_0 + \beta_1*\text{learning time}_i + \beta_2 *\text{gaming time}_i + \epsilon_i
\]</span></p>
<p>Let us compute them:</p>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="further-explanations.html#cb509-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb509-2"><a href="further-explanations.html#cb509-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb509-3"><a href="further-explanations.html#cb509-3" tabindex="-1"></a></span>
<span id="cb509-4"><a href="further-explanations.html#cb509-4" tabindex="-1"></a><span class="co"># Number of samples</span></span>
<span id="cb509-5"><a href="further-explanations.html#cb509-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb509-6"><a href="further-explanations.html#cb509-6" tabindex="-1"></a></span>
<span id="cb509-7"><a href="further-explanations.html#cb509-7" tabindex="-1"></a><span class="co"># True coefficients</span></span>
<span id="cb509-8"><a href="further-explanations.html#cb509-8" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="dv">80</span></span>
<span id="cb509-9"><a href="further-explanations.html#cb509-9" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> <span class="fl">1.5</span></span>
<span id="cb509-10"><a href="further-explanations.html#cb509-10" tabindex="-1"></a>beta_2 <span class="ot">&lt;-</span> <span class="fl">1.5</span></span>
<span id="cb509-11"><a href="further-explanations.html#cb509-11" tabindex="-1"></a></span>
<span id="cb509-12"><a href="further-explanations.html#cb509-12" tabindex="-1"></a><span class="co"># Generate independent variables</span></span>
<span id="cb509-13"><a href="further-explanations.html#cb509-13" tabindex="-1"></a>learning_time <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb509-14"><a href="further-explanations.html#cb509-14" tabindex="-1"></a>gaming_time <span class="ot">&lt;-</span> <span class="fl">0.7</span> <span class="sc">*</span> learning_time <span class="sc">+</span> </span>
<span id="cb509-15"><a href="further-explanations.html#cb509-15" tabindex="-1"></a>  <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.7</span><span class="sc">^</span><span class="dv">2</span>) <span class="sc">*</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="dv">1</span>) </span>
<span id="cb509-16"><a href="further-explanations.html#cb509-16" tabindex="-1"></a></span>
<span id="cb509-17"><a href="further-explanations.html#cb509-17" tabindex="-1"></a><span class="co">#generate error term</span></span>
<span id="cb509-18"><a href="further-explanations.html#cb509-18" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">3</span>)</span>
<span id="cb509-19"><a href="further-explanations.html#cb509-19" tabindex="-1"></a></span>
<span id="cb509-20"><a href="further-explanations.html#cb509-20" tabindex="-1"></a><span class="co"># Generate grades</span></span>
<span id="cb509-21"><a href="further-explanations.html#cb509-21" tabindex="-1"></a>grades <span class="ot">&lt;-</span> beta_0 <span class="sc">+</span> beta_1 <span class="sc">*</span> learning_time <span class="sc">+</span> beta_2 <span class="sc">*</span> gaming_time <span class="sc">+</span> epsilon</span>
<span id="cb509-22"><a href="further-explanations.html#cb509-22" tabindex="-1"></a></span>
<span id="cb509-23"><a href="further-explanations.html#cb509-23" tabindex="-1"></a><span class="co"># Create a data frame</span></span>
<span id="cb509-24"><a href="further-explanations.html#cb509-24" tabindex="-1"></a>df_grades <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(learning_time,</span>
<span id="cb509-25"><a href="further-explanations.html#cb509-25" tabindex="-1"></a>                   gaming_time,</span>
<span id="cb509-26"><a href="further-explanations.html#cb509-26" tabindex="-1"></a>                   grades)</span>
<span id="cb509-27"><a href="further-explanations.html#cb509-27" tabindex="-1"></a></span>
<span id="cb509-28"><a href="further-explanations.html#cb509-28" tabindex="-1"></a><span class="co"># Display first few rows of the data frame</span></span>
<span id="cb509-29"><a href="further-explanations.html#cb509-29" tabindex="-1"></a>grades_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(grades <span class="sc">~</span> learning_time <span class="sc">+</span> gaming_time, </span>
<span id="cb509-30"><a href="further-explanations.html#cb509-30" tabindex="-1"></a>                   <span class="at">data =</span> df_grades)</span>
<span id="cb509-31"><a href="further-explanations.html#cb509-31" tabindex="-1"></a></span>
<span id="cb509-32"><a href="further-explanations.html#cb509-32" tabindex="-1"></a><span class="co">#Getting the summary</span></span>
<span id="cb509-33"><a href="further-explanations.html#cb509-33" tabindex="-1"></a><span class="fu">summary</span>(grades_model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = grades ~ learning_time + gaming_time, data = df_grades)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.2868 -1.9842 -0.0991  1.9482  6.2446 
## 
## Coefficients:
##               Estimate Std. Error t value
## (Intercept)    80.1718     0.6668 120.231
## learning_time   1.2665     0.3335   3.798
## gaming_time     1.7860     0.4312   4.142
##               Pr(&gt;|t|)    
## (Intercept)    &lt; 2e-16 ***
## learning_time 0.000255 ***
## gaming_time   7.36e-05 ***
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 2.822 on 97 degrees of freedom
## Multiple R-squared:  0.8661, Adjusted R-squared:  0.8634 
## F-statistic: 313.8 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>By looking at the model, we could conclude that the more a student learns, the better its grades on average, holding all else constant. So far, so clear, but the same goes for gaming time. Why is that the case? Because if we think that a student has per day 3 hours, which the student can assign to either learning or gaming, than both are correlated, because assigning 2 hours to learning means 1 hour for gaming, 0.5 hours for learning means 2.5 hours for gaming and so forth. This means both coefficients are explaining each other and bias each other. How to detect them?</p>
<div id="testing-correlations-to-each-other" class="section level5 hasAnchor" number="7.2.2.3.1">
<h5><span class="header-section-number">7.2.2.3.1</span> Testing Correlations to each other<a href="further-explanations.html#testing-correlations-to-each-other" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The first technique is to check the correlations of the variables to each other beforehand. You can do that two-ways: Just print out a correlation table:</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="further-explanations.html#cb511-1" tabindex="-1"></a><span class="co">#First store the variables you need in a seperate data frame </span></span>
<span id="cb511-2"><a href="further-explanations.html#cb511-2" tabindex="-1"></a>cormatrix_data <span class="ot">&lt;-</span> df_grades <span class="sc">%&gt;%</span> </span>
<span id="cb511-3"><a href="further-explanations.html#cb511-3" tabindex="-1"></a>  <span class="fu">select</span>(learning_time, gaming_time)</span>
<span id="cb511-4"><a href="further-explanations.html#cb511-4" tabindex="-1"></a></span>
<span id="cb511-5"><a href="further-explanations.html#cb511-5" tabindex="-1"></a></span>
<span id="cb511-6"><a href="further-explanations.html#cb511-6" tabindex="-1"></a><span class="co">#Second, calculate the table, the 2 at the end are the dimensions</span></span>
<span id="cb511-7"><a href="further-explanations.html#cb511-7" tabindex="-1"></a>cormatrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(cormatrix_data) <span class="co">#Calculate the correlations</span></span>
<span id="cb511-8"><a href="further-explanations.html#cb511-8" tabindex="-1"></a><span class="fu">round</span>(cormatrix, <span class="dv">2</span>) <span class="co">#round it to the second digit and display it </span></span></code></pre></div>
<pre><code>##               learning_time gaming_time
## learning_time          1.00        0.95
## gaming_time            0.95        1.00</code></pre>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="further-explanations.html#cb513-1" tabindex="-1"></a><span class="co">#We could have also done this code in one step </span></span>
<span id="cb513-2"><a href="further-explanations.html#cb513-2" tabindex="-1"></a></span>
<span id="cb513-3"><a href="further-explanations.html#cb513-3" tabindex="-1"></a><span class="co">#df_grades %&gt;% </span></span>
<span id="cb513-4"><a href="further-explanations.html#cb513-4" tabindex="-1"></a><span class="co">#  select(learning_time, gaming_time) %&gt;% </span></span>
<span id="cb513-5"><a href="further-explanations.html#cb513-5" tabindex="-1"></a><span class="co">#  cor() %&gt;% </span></span>
<span id="cb513-6"><a href="further-explanations.html#cb513-6" tabindex="-1"></a><span class="co">#  round(2) </span></span></code></pre></div>
<p>We can see that the correlation between both variables is way to high. Now, with only two variables the table works fine, but what if we have, let us say, 20 variables? It could get messy, therefore you could also use the correlation matrix, which I introduced in the chapter before. In this case, it does not make sense, since it would just print out one block. But keep it nevertheless in mind for the future.</p>
</div>
<div id="variance-of-inflation-vif" class="section level5 hasAnchor" number="7.2.2.3.2">
<h5><span class="header-section-number">7.2.2.3.2</span> Variance of Inflation (VIF)<a href="further-explanations.html#variance-of-inflation-vif" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Another measure for multicolinearity and probably the most famous one, is the Variance-of-Inflation (VIF) factor. Intuitively spoken, this measure fits models with multiple variables, by calculating the variance of each variable. Then it fits a model with only one independent variable and calculates the variance of it. The result is a measure that displays high values if the variance of a variable increases, when other variables are added. That is exactly what this measure does.. The formula of it is really simple:</p>
<p><span class="math display">\[
VIF_i = \frac{1}{1 - R^2_i}
\]</span></p>
<p>where, the Variance of inflation (VIF) of variable <em>i</em> is calculated by 1 divided by 1 - the R-squared (<span class="math inline">\(R^2_i\)</span>) of the regression with only that variable.</p>
<p>In <code>R</code>, we can use the <code>VIF()</code> function from the <code>car</code> - package to do it.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="further-explanations.html#cb514-1" tabindex="-1"></a><span class="co">#We only have to use the function VIF() on our model </span></span>
<span id="cb514-2"><a href="further-explanations.html#cb514-2" tabindex="-1"></a><span class="fu">vif</span>(grades_model)</span></code></pre></div>
<pre><code>## learning_time   gaming_time 
##      10.21358      10.21358</code></pre>
<p>The rule is that if a value exceeds 10, it is considered critical. We should always test for multicolinearity, and if we detect it, run the regression separately without the correlated variables.</p>

</div>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="loops-and-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="solutions-exercises.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": {}
},
"fontsettings": null,
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
