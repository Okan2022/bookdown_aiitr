[["index.html", "An intuitive Introduction to R Chapter 1 An Intuitive Introduction to R 1.1 Welcome ! 1.2 About this Course 1.3 About Me", " An intuitive Introduction to R Okan Sarioglu 2024-07-28 Chapter 1 An Intuitive Introduction to R 1.1 Welcome ! Welcome to this R course! In this course I will teach you the workflow from raw data to your very first analyses in R. When I was a beginner myself, I tried a lot of courses like this, but there were two points, which I think slowed down my learning process, I do want to make different with this course: The courses taught way too much unnecessary stuff for the beginning. It does not matter to know several variations of a command for example, the intuition is way more important. This course was originally a tutorial for my juniors at my university and I asked myself the question: What do they need to know, to conduct data analyses themselves with R? Well, they have to get an intuition of the workflow. So I decided to design a course, that instead of showing unnecessary variations of functions aim to show how to analyse a question of interest. The courses were not reproducible! That is not the directly the fault of the courses. There are tools to make R scripts reproducible, however they are not beginner friendly, so the authors of those courses are facing a trade-off: Teaching unnecessary complicated stuff at to beginners, or to leave it out and make the course not reproducible. This course is different! In this course, you can download R-scripts and load them directly on your own device and follow the course by executing the codes for yourself to have the original experience of working with R. 1.2 About this Course 1.2.1 Prerequisities R and RStudio. You should download the most current version of R and RStudio. How to do that easily is described here. 1.2.2 What is R and RStudio ? R is a programming language specifically developed for statistical analysis. RStudio is a nice graphical interface to work with R. 1.2.3 Why R ? Free of cost and open-source Functionalities for all steps of research process from Data Collection to Data Analysis Programming language specifically developed for statistical analysis Very active Community: e.g. R community on \\[StackOverflow\\] (https://stackoverflow.blog/2017/10/10/impressive-growth-r/) e.g. #rstats on twitter 1.2.4 What expects you and what not In this course you will learn: To get familiar with R and its basic language Core commands from the tidyverse package Data Wrangling, Data Bridging, Data Munging, Data Manipulation An efficient Workflow A brief introduction into basic Data Analysis You will not learn: Advanced R usage (Webscraping, Quantitative Text Analysis etc…) 1.2.5 Overview of the course structure: 1. The R environment Basic Functionality (Calculations, Vectors, Matrices, Lists) Object classes Accessing, Subsetting and Naming Objects 2. Data Manipulation Pipelines or Piping The tidyverse - Dplyr Loading and Storing Data Ordering your Data: Renaming, Re-Ordering, Subsetting and Selecting Transforming Variables Merging Data Missing Values 3. Exploratory Data Analysis/Descriptives Summary Statistics - the Psych package Frequency Tables Cross-Tabulations, Correlation Matrice 4. Data Visualization The Tidyverse - ggplot 2 Constructing Plots Plotting anything 5. R Programming For loops Apply function Functions 1.3 About Me My name is Okan and I am a Data Scientist. To be more precise, my profession is Data Science, but originally I graduated in Political Science. I know, what kind of transition is that? From Politics to Data? Well, my university was specialized in empirical research and Political Science was no exception. To explain political phenomena, I analysed large data sets and it was the most fun part of my studies. So I decided to become a Data Scientist and I could not be happier about that decision! Be part of this Course! Please report errors, bugs and problems with the code. If you have any ideas how to improve this course please contact me on GitHub or via E-Mail. In general, let us stay in touch, follow me on GitHub and LinkedIn and if you like this course share your experience and recommend it to others! "],["fundamentals.html", "Chapter 2 Fundamentals 2.1 Getting familiar with RStudio and establishing a Workflow 2.2 Lets get started: R as a fancy calculator 2.3 ifelse statements and the ifelse() function 2.4 Outlook 2.5 Exercise Section", " Chapter 2 Fundamentals 2.1 Getting familiar with RStudio and establishing a Workflow 2.1.1 The user interface RStudio has four main components and each of them has a purpose: Figure 1: Standard RStudio Interface On the top left you can see the Source window. In this window you write and run your code. The console opens after you choose, which type of Script you want to use: The standard R Script (Ctrl + Shift + N): In this file you can just write your code and run it. You can also make commenting lines with putting a # in front of it. The R Markdown File: In contrast to the standard R Script not everything written in a R Markdown File is automatically considered as Code (if not written after an #). A R Markdown File has options to design an HTML Output and a PDF Output. This can increase your efficiency in terms of working with partners. Further, you can write your code in chunks and have plenty options to work with those chunks. In QM and AQM you will exclusively use R Markdown and over time you will see the advantages of R Markdown. On the top right you can see the Environment. Here you have an overview over all the objects currently loaded in your environment. You will learn more about objects later in the course. On the bottom left you have the Console: This is where the results appear once you execute your R-code. You can also directly type R-code into the console and execute it. However, we cannot save this code which is why we usually work in the Editor. On the bottom right you can see the Output: Plots (if you use the standard R Script) and other things will appear here, don’t worry too much about it for the moment. 2.1.2 How to design it according to your preferences (optional) You can change the appearance of RStudio: Tools &gt; Global Options &gt; Appearance. Here you can change the zoom of the Console, the font size of the your letters and the style of your code. Further, you can change RStudio to a dark theme. Play around with it and find out how it is the most comfortable for you and of course you can change it over time. You can change the Pane Layout meaning where the four components of RStudio should be: RStudio: Tools &gt; Global Options &gt; Pane Layout. You should use key shortcuts. There are pre-installed short-cuts of RStudio, which are really helpful. You should get familiar with them. Tools &gt; Key Shortcut Helps or directly with Ctrl + Shift + K. You can add your own Key Shortcuts and we will do that in this course. 2.2 Lets get started: R as a fancy calculator 2.2.1 Mathmetical Operations in R You can use R for basic calculations: #You can use hashtags to comment 1 + 1 #Addition ## [1] 2 1 - 1 #Substraction ## [1] 0 1 * 1 #Multiplication ## [1] 1 1 / 1 #Division ## [1] 1 2^(1 / 2) #Mixed Terms ## [1] 1.414214 You can use R also for TRUE/FALSE statements for logical statements via the comparison operators: &gt; greater than &lt; smaller than == equal != not equal &gt;= greater than or equal to &lt;= less than or equal to 1 &lt; 3 #TRUE ## [1] TRUE 5 &gt;= 8 #FALSE ## [1] FALSE 11 != 10 #TRUE ## [1] TRUE 22 == 22 #TRUE ## [1] TRUE 7 &lt; 3 #FALSE ## [1] FALSE 5 &lt;= 2+3 #TRUE ## [1] TRUE You can also use logical operators: - &amp; element-wise AND operator. It returns TRUE if both elements are true - | element-wise OR operator. It returns TRUE if one of the statements is TRUE - ! Logical NOT operator. It returns FALSE if statement is TRUE 5 &amp; 4 &lt; 8 #TRUE ## [1] TRUE 5 | 4 &lt; 8 #TRUE ## [1] TRUE !5 &gt; 2 #FALSE ## [1] FALSE 2.2.2 Using Commands For more advanced operations you can and should use functions. Functios are mostly a word with brackets. Within a function, there are so called arguments. These arguments specify the function and give it the information it needs + optional information. e.g. sqrt(x) taking the square root, x is any number. exp(x) the constant e, x is any number. mean(x) for the mean, x is any number. median(x) for the median, x is any number. sqrt(x = 36) #square root exp(x = 0) # exponential of 1 print(&quot;U can stay under my umbrella&quot;) #with this command you can print what you want I explicitly choose easy examples, but sometimes commands can be complicated, because they demand special inputs. To get help, our first step should be to ask R itself: You can put a question mark in front of a function and execute it. In your output under the tab “Help” an explanation with examples will pop up and explain the function. You can also call the help() function and put the function you want to learn about without brackets in the help() function. ?exp() #questionmark help(exp) #help command 2.2.3 Assigning objects and printing them Most of the time you will store your results in objects. You can do so by using the &lt;- operator. Afterwards you can work with the objects. Let us assign numbers to out objects. The objects will be saved and you can see them in your environment. Pizza &lt;- 7.50 #pizza object Cola &lt;- 3.50 #cola object Pizza + Cola #addition of objects ## [1] 11 We can then go on work with the content of the objects. For beginners, let us add the object Pizza and the object Cola together. We can also save the result of those object in a new object and work with this object and this can go on forever technically. Offer &lt;- Pizza + Cola #assigning addition Offer #printing the object ## [1] 11 Offer^2 #square the term with ^2 ## [1] 121 2.2.4 Vectors In R a vector contains more than one information. You use the c() command, and divide the information with a ,. Let us compare food prices: food &lt;- c(&quot;Pizza&quot;, &quot;Kebab&quot;, &quot;Curry&quot;, &quot;Fish&quot;, &quot;Burrito&quot;) #food vector print(food) #printing it ## [1] &quot;Pizza&quot; &quot;Kebab&quot; &quot;Curry&quot; &quot;Fish&quot; &quot;Burrito&quot; prices &lt;- c(7.50, 6.00, 8.50, 3.00, 11.00) #price vector print(prices) #printing it ## [1] 7.5 6.0 8.5 3.0 11.0 cola_prices &lt;- c(3.50, 3, 4, 2.50, 3) #cola prices vector print(cola_prices) #printing it ## [1] 3.5 3.0 4.0 2.5 3.0 Now we can calculate the prices for a decent meal in one step by adding the two vectors together. The vector prices_combined will give us the prices for a meal plus a cola: prices_combined &lt;- prices + cola_prices #prices combined print(prices_combined) #printing it ## [1] 11.0 9.0 12.5 5.5 14.0 2.2.5 Object Classes Objects can contain information of different data types: Numeric Numbers c(1, 2.4, 3.14, 4) Character Text c(\"1\", \"blue\", \"fun\", \"monster\") Logical True or false c(TRUE, FALSE, TRUE, FALSE) Factor Category c(\"Strongly disagree\", \"Agree\", \"Neutral\") For data analysis commands sometimes require special object classes. With the class() command we can find out the class. And with as.numeric for example we can change classes by assigning it to itself, by it is common to assign it to a new object: #Let us find out the classes class(prices) #numeric ## [1] &quot;numeric&quot; class(food) #character ## [1] &quot;character&quot; class(cola_prices) #numeric ## [1] &quot;numeric&quot; We can also change the classes of variables. To do so, we can use as.factor(), as.numeric(), as.character() and so forth. You can do that for every class. Let us change the cola_prices to a vector. To do so, we change call as.character() and put the object in it. Then we assign it to another object called cola_prices_character. This object will have the class \"character\". #We want the cola_prices vector to be a character cola_prices_character &lt;- as.character(cola_prices) #Checking it class(cola_prices_character) ## [1] &quot;character&quot; print(cola_prices_character) ## [1] &quot;3.5&quot; &quot;3&quot; &quot;4&quot; &quot;2.5&quot; &quot;3&quot; 2.2.6 Matrices 2.2.6.1 Making Matrices There are different ways of building a matrix. Let us start by just binding the vectors as columns together. You can do that cbind() if you want to bind columns together. rbind() is therefore the command to bind rows together. You have to call cbind() and include the vectors you want to bind together The same with rbind() price_index &lt;- cbind(food, prices, cola_prices) #We bind it together print(price_index) #We print it ## food prices cola_prices ## [1,] &quot;Pizza&quot; &quot;7.5&quot; &quot;3.5&quot; ## [2,] &quot;Kebab&quot; &quot;6&quot; &quot;3&quot; ## [3,] &quot;Curry&quot; &quot;8.5&quot; &quot;4&quot; ## [4,] &quot;Fish&quot; &quot;3&quot; &quot;2.5&quot; ## [5,] &quot;Burrito&quot; &quot;11&quot; &quot;3&quot; #Let&#39;s do the same by binding the rows together price_index2 &lt;- rbind(food, prices, cola_prices) #We bind it together print(price_index2) #We print it ## [,1] [,2] [,3] [,4] [,5] ## food &quot;Pizza&quot; &quot;Kebab&quot; &quot;Curry&quot; &quot;Fish&quot; &quot;Burrito&quot; ## prices &quot;7.5&quot; &quot;6&quot; &quot;8.5&quot; &quot;3&quot; &quot;11&quot; ## cola_prices &quot;3.5&quot; &quot;3&quot; &quot;4&quot; &quot;2.5&quot; &quot;3&quot; We can also generate a matrix by simulating it. There are a lot of things to care about, simply because a lot is possible: You first call the matrix() command. The first argument is an interval of numbers. These are our total observations if you want. The second and third argument are our number of rows nrow() and our number of columns ncol(). If you multiply them, they have to result in the number of observations you defined before. We have 20 numbers and 4 multiplied by 5 is 20, thus this is fine. Lastly you have to define if the numbers should be included from left to right, thus by row or if they should be ordered from top to bottom. We go through both examples and then it should become clear. The dim() command is helpful, because it shows us to inspect the dimensions. # Create a matrix matrix_example &lt;- matrix(1:20, nrow = 4, ncol = 5, byrow = T) # # Checking it print(matrix_example) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 # Checking the dimensions dim(matrix_example) ## [1] 4 5 # What happens if byrow is set to FALSE? matrix_example2 &lt;- matrix(1:20, nrow = 4, ncol = 5, byrow = F) # Checking it print(matrix_example2) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 9 13 17 ## [2,] 2 6 10 14 18 ## [3,] 3 7 11 15 19 ## [4,] 4 8 12 16 20 dim(matrix_example2) ## [1] 4 5 2.2.6.2 Working with Matrices We want to work with matrices. The first tool to learn is how to inspect the matrices: First, you call the matrix. In our example, matrix_example with square brackets. In these brackets you can call single rows by entering the number of the row you want to inspect, and then you put a comma behind it to signal R that you want to have the row. If you put a number behind the comma, you tell R to give you the column with the number. If you want a single number then you have to define a row and a column. #Let us get used to work with objects row &lt;- 1 column &lt;- 1 #Printing it print(object1 &lt;- matrix_example[row, ]) #printing the first row ## [1] 1 2 3 4 5 print(object2 &lt;- matrix_example[, column]) #printing the first column ## [1] 1 6 11 16 print(object3 &lt;- matrix_example[row, column]) #printing first row and column ## [1] 1 print(matrix_example) #printing the matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 #More Information nrow(matrix_example) #How many rows ## [1] 4 ncol(matrix_example) #How many columns ## [1] 5 dim(matrix_example) #Overall dimensions ## [1] 4 5 2.2.7 Data Frames The next type of data storage are data frames. These are the standard storage objects for data in R. The reason is simple, matrices are only able to contain one type of variables (numeric, character, etc…). In this example, we have a dataset with the variable country (character), the capital (character), the population in mio (numeric), and a if the country is in europe (logical) # making an example df df_example &lt;- data.frame( country = c(&quot;Austria&quot;, &quot;England&quot;, &quot;Brazil&quot;, &quot;Germany&quot;), capital = c(&quot;Vienna&quot;, &quot;London&quot;, &quot;Brasilia&quot;, &quot;Berlin&quot;), pop = c(9.04, 55.98, 215.3, 83.8), europe = c(TRUE, FALSE, TRUE, TRUE) ) # Checking it print(df_example) ## country capital pop europe ## 1 Austria Vienna 9.04 TRUE ## 2 England London 55.98 FALSE ## 3 Brazil Brasilia 215.30 TRUE ## 4 Germany Berlin 83.80 TRUE If you want to work with data frames you can call columns by calling the name of the data frame putting a dollar sign behind it and then calling the name of the column, you want to inspect: df_example$country ## [1] &quot;Austria&quot; &quot;England&quot; &quot;Brazil&quot; &quot;Germany&quot; You see that you get the vector of the column. We can go further and work more with data frames Let us call a single observation in the data frame: You call the column you want to inspect the same way as before, this time you also put square brackets behind and call the number of the observation in the column. We want to have “Brazil”. “Brazil” is the third element of the df_example$country, thus we include 3 in the square brackets: df_example$country[3] ## [1] &quot;Brazil&quot; The last thing to do with data frames is to get columns based on conditions. In the next part of this chapter we will get a method to do so, but we can do so as well with the data frame. Imagine you want to have a vector of df$country, but only with countries that have a df$pop bigger than 60. Meaning a population bigger than 60. To do so we call the df$country column and put square brackets behind it Further we need to call in the square brackets the condition. Thus, the variable df_example$pop and set it to bigger than 60. Et voila, we get the columns of df_example$country bigger than 60. df_example$country[df_example$pop &gt; 60] ## [1] &quot;Brazil&quot; &quot;Germany&quot; 2.3 ifelse statements and the ifelse() function One of the most frequently used and therefore most important logical operators in programming in general are ifelse commands. You probably know them from Excel, but every programming language includes them, because of their usefulness. A quick reminder of their logic. 2.3.1 If else statement with only one condition First, you define an if statement. The if statement is a logical statement for example bigger, smaller than X. The logical statement is your test expression. With this you tell the program to test the condition for an object, in excel a cell or whatever object in your programming language can be tested. The program then checks if the condition is TRUE or FALSE. Until this point every if else is the same and now we will look at some variations: Figure 2: Logic of if-else statements Figure ?? shows the logic of an if else statement with one condition. We say that if the test expression is true something should happen. If not nothing happens, because nothing was defined. We want R to judge our grades in school. For this reason we define an object called grade and assign 1.7 to it. In the next step we call if and open a bracket. We include the test expression in the bracket, which shall be if grade, our grade is smaller than 2 Then we open curly brackets to define what should happen if the test expression is TRUE. Meaning what should happen if the grade is better than 2. We define print(“Good Job”). Here is the general logic of a if else statement: if (test expression) { Body of if } grade &lt;- 1.7 if(grade &lt; 2) { print(&quot;Good Job&quot;) } # You write down if(test expression), and then the {body expression}, thus the body expression in fancy brackets. ## [1] &quot;Good Job&quot; grade &lt;- 2.5 if(grade &lt; 2) { print(&quot;Good Job&quot;) } #Since the condition is not met, nothing happens As you can see if the grade is bigger than 2 nothing happens. Otherwise “Good Job” is printed as intended. 2.3.2 if statements with else condition An if command on its own is quite useless. Things get interesting, when we also have a body for the else command. What happens now, is that instead of nothing being printed when the test expression is FALSE. Then the body of else is printed. Figure 3: Standard RStudio Interface Now we just add a body of else into the equation, in R that means we need to define it: We take the if else statement of the chunk before and now we add an else and open curly brackets, where we define the body of else. grade &lt;- 3.3 #assigning a grade if (grade &lt;= 2) { print(&quot;Good Job&quot;) } else { print(&quot;Life goes on&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 1.3 #assigning a grade if (grade &lt;= 2) { print(&quot;Good Job&quot;) } else { print(&quot;Life goes on&quot;) } ## [1] &quot;Good Job&quot; We see that in any case a result is printed. 2.3.3 The ifelse() command What you see above is the manual way of coding an if else condition. But there is also an ifelse() function in R, where you do not have different colors and fancy brackets everywhere. you call ifelse() The first argument is your test expression The second the body of if The third the body of else ifelse(test expression, body of if, body of else) ifelse(grade &lt;=2, &quot;Good Job&quot;, &quot;Life goes on&quot;) #ifelse command ## [1] &quot;Good Job&quot; 2.3.4 if else ladders/ if else with multiple conditions The world is a complex place. Sometimes things are not black and white. Thus we have to be prepared for different scenarios. To make it less melodramatic, if else statements are also possible with several bodies of else. Let us take the example that we want to have a statement of R regarding our grade for different number intervals. What R does is to check the first condition, afterward he checks the second test expression and so forth until he finds a hit and prints it. If no expression is found, you have to define a last else expression. You add else if with curly brackets where the test expression is included. Lastly, you define an else with curly brackets for the case there is not test expression grade &lt;- 3.3 #Assigning a grade if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } else { print(&quot;No expression found&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 1.7 #Assigning a grade if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } else { print(&quot;No expression found&quot;) } ## [1] &quot;Good Job&quot; grade &lt;- 5.0 #Assigning a grade if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } else { print(&quot;No expression found&quot;) } ## [1] &quot;No expression found&quot; Again we can do so-called nested ifelse function, using the ifelse() command: Instead of defining an else condition, we define another ifelse() command and do as long as we need to. The last ifelse() is then the only one with a clear else condition. grade &lt;- 1.7 #Assigning a grade ifelse(grade == 1.0, &quot;Amazing&quot;, ifelse(grade &gt; 1 &amp; grade &lt;= 2, &quot;Good Job&quot;, ifelse(grade &gt; 2 &amp; grade &lt;= 3, &quot;OK&quot;, ifelse(grade &lt; 3 &amp; grade &lt;=4, &quot;Life goes on&quot;, &quot;No expression found&quot; ) ) ) ) ## [1] &quot;Good Job&quot; grade &lt;- 3.3 #Assigning a grade ifelse(grade == 1.0, &quot;Amazing&quot;, ifelse(grade &gt; 1 &amp; grade &lt;= 2, &quot;Good Job&quot;, ifelse(grade &gt; 2 &amp; grade &lt;= 3, &quot;OK&quot;, ifelse(grade &lt; 3 &amp; grade &lt;=4, &quot;Life goes on&quot;, &quot;No expression found&quot;) ) ) ) ## [1] &quot;No expression found&quot; # The same logic: ifelse(test expression, body expression if, ifelse(test expression 2, body expression if 2)) etc.. 2.4 Outlook This section was a brief introduction to the fundamentals of R. It was kept simple to give you a feeling for a R. These are the absolute basics, which are needed to understand everything, which will be built based on it. 2.5 Exercise Section 2.5.1 Exercise 1: Create a vector called my_vector with the values 1,2,3 and check is class. #create the vector my_vector &lt;- #check the class 2.5.2 Exercise 2: Create a Matrix called student. This should contain information about the name, age and major. Make three vectors wiht three entries and bind them together to a the matrix student. Print the matrix. #Create the vectors name &lt;- age &lt;- major &lt;- #Create the matrix student &lt;- #Print the matrix 2.5.3 Exercise 3: Write an if-else statement that checks if a given number is positive or negative. If the number is positive, print “Number is positive”, otherwise print “Number is negative”. Feel free to decide if you want to use the ifelse function or the ifelse condition. #Assigning the number to the object &quot;number&quot; number &lt;- -4 2.5.4 Exercise 4: Write an if-else ladder that categorizes a student’s grade based on their score. The grading criteria are as follows: Score &gt;= 90: “A” Score &gt;= 80 and &lt; 90: “B” Score &gt;= 70 and &lt; 80: “C” Score &gt;= 60 and &lt; 70: “D” Score &lt; 60: “F” "],["data-manipulation.html", "Chapter 3 Data Manipulation 3.1 Packages 3.2 Working with packages 3.3 The Data we will work with: The European Social Survey (ESS) 3.4 Let’s wrangle the data 3.5 The dplyr package 3.6 Merging Datasets 3.7 Outlook 3.8 Exercise Section", " Chapter 3 Data Manipulation 3.1 Packages This far, we’ve only covered so-called “base R functions” or “built-in functions”, but R has an active community and sometimes further operations are needed, so we use packages. These are including further functions, which we will use heavily in the following section. 3.1.1 The tidyverse package One of the most influential and widely used package in R is the tidyverse package. This package includes several other packages, which are key for data manipulation e.g. dplyr, ggplot2, stringr, readr, tidyr. 3.2 Working with packages 3.2.1 Installing packages To install packages you use the very creative install.packages() command in R. Note that it is necessary to directly install a package in R. This step is only required once: Call the install.packages() command. You put the name of the package in quotation marks into the function. install.packages(\"pacman\") 3.2.2 Loading your packages While you only need to install a package once, you need to load it every time in your script, when you open it. You can do that with the library() function in R: Call the library() function. Put the name of library(pacman) It is always important to have an efficient workflow in R. Traditional R users, load all packages they need at the beginning of their page. Logically, so they just need to go back to the top of the script and need to load it every time they open the script. But there are way more elegant and pragmatic ways to do that. One way is the pacman package: First, if you use the name of package and put a “::” behind it you tell R to go into the package and to specifically get one command of the package, in our case the p_load command. Second, the p_load command, loads the packages in the brackets and checks if they are installed, if not, it automatically installs and loads them. pacman::p_load(&quot;tidyverse&quot;, &quot;psych&quot;, &quot;WDI&quot;, &quot;gapminder&quot;) #loading packages 3.3 The Data we will work with: The European Social Survey (ESS) For the following Data Manipulation Part, we will use the European Social Survey Round 10 with the topic “Democracy, Digital social contacts”. It is a high-quality survey conducted in 31 European countries. Round 10 was conducted in 2020 and is the most recent ESS. We will use it, since survey data is quite popular among students and further at some point everyone needs to work with it. But do not worry if you do not like survey data, we will also cover other prominent Datasets. You can freely download it via the website of the ESS. From there you need to go to the Data Portal and than you can download the Round you want, in the format you want. As already mentioned, use the .dta or .csv format. Variable Description Scales idnt Respondent’s identification number unique number from 1-9000 year The year when the survey was conducted only 2020 cntry Country BE, BG, CH, CZ, EE, FI, FR,GB, GR, HR, HU, IE, IS, IT, LT,NL, NO, PT, SI, SK agea Age of the Respondent, calculated Number of Age = 15-90 999 = Not available gndr Gender 1 = Male; 2 = Female; 9 = No answer happy How happy are you 0 (Extremly unhappy) - 10 (Extremly happy); 77 = Refusal; 88 = Don’t Know; 99 = No answer eisced Highest level of education, ES - ISCED 0 = Not possible to harmonise into ES-ISCED; 1 (ES-ISCED I , less than lower secondary) - 7 (ES-ISCED V2, higher tertiary education, =&gt; MA level; 55 = Other; 77 = Refusal; 88 = Don’t know; 99 = No answer netusoft Internet use, how often 1 (Never) - 5 (Every day); 7 = Refusal; 8 = Don’t know; 9 = No answer trstprl Most people can be trusted or you can’t be too careful 0 (You can’t be too careful) - 10 (Most people can be trusted); 77 = Refusal; 88 = Don’t Know; 99 = No answer lrscale Left-Right Placement 0 (Left) - 10 (Right); 77 = Refusal; 88 = Don’t know; 99 = No answer Note: In the following, I will simulate the ESS with the same names and same range values. Which means if you read in the actual ESS, you should be able to run the code as well without problems! I do so for reproducibility reasons. When downloading the script, users should be able to run the whole script with one click. This is because users, who are new to R might have problems with setting working directories, since working directories are prone to errors, especially at the beginning. 3.3.1 Load the data 3.3.1.1 How to load data The first thing to do, when cleaning data is to load the data into R. The first thing to ensure is to know, where R can take the data from and load it into its own environment. You can call data from your local devise or remote through e.g. an API: Remote: This means the dataset is laying around on some external Cloud or Server and you have the possibility to load it into R, without downloading it on your local devise. Depending on the dataset there are different ways, I will show you later how to get data from the World Bank. Local: To get data from your devise, you have to tell R, where to find it, meaning you tell him the path. Go to your file and click on it. The path will be shown in the address bar, just copy it and paste it in R with the responding command. The command is defined by the type of the file as you can see in the table below. Here is an example how it could look wit a .csv file data &lt;- read.csv(\"C:/Users/YourUsername/Documents/data.csv\") This table presents some of the most frequent data types, which you can download and load into R. There are of course more data types, and to find out if you can read them into R, you can just google the type and look for the command. Mostly you will find a package with a command. And mostly, those commands start with “read”. Examples of Different Data Types and how to load them into R File File Extension Package Command Stata .dta haven read_dta() CSV-Files .csv readr (is included in the tidyverse package) read_csv() Excel-Files .xlsx; .xls readxl read.rds() RData (also RDS Data) .RData;.rds base R functions, no package required load() , read.rds() 3.3.1.2 Working Directories and R-Projects If you have more data and files you want to load into R, it is not recommended to copy always the path of every file. Instead it is common to work with working directories or R-Projects. There is always a working directory you can find out, in which working directory you are currently at by using the command getwd(), just run it in your console and R will give you the path it currently is working at. Let us assume you saved all your data in a folder called “Intro_to_R_course”, then you can set the working directory separately with this command (do not forget to change the names in the paths): setwd(\"C:/Users/YourUsername/Intro_to_R_course\") After you set the working directory you can run again getwd() and the path now has to be changed to the content of the setwd(). The advantage is now that if the working directory is set, you can load in the data without specifying the path. If your data is in the path of the working directory you can load like that: data &lt;- read.csv(\"data.csv\") You can also create an R-Project. If you click on file &gt; New Project &gt; New Directory &gt; New Project you can determine the working directory and create a folder in it. In this folder, there will be an R-Project file, if you open this file, a blank R environment will appear. Now, you can create files in this folder. And every time you enter R through the R-Project file, you do not have to set any working directories. Because if the R-Projects opens R, it automatically sets the working directory to where you created the folder. I would not recommend to work with R-Projects in the beginning, but when you work with collaborators and want to make your code reproducible for others, then you will have to work with R-Projects one day. Again, this also the reason, I just simulate all the data, so you can work with the code without worrying about any working directories. 3.3.2 One last thing: Pipelines Sometimes codes have several dimensions, which could make it quite complicated leave_house(get_dressed(get_out_of_the_bed(wake_up(me)))) Well, as you can see there are too many dimensions and with tidyverse you can basically split it up into so-called Pipelines, for them you use a Pipe %&gt;% : me %&gt;% wake_up() %&gt;% get_out_of_the_bed() %&gt;% get_dressed() %&gt;% leave_house It is the same code, in R this code would do the same. But the advantage is that it is way more intuitive and makes the code clear. Here an example: First, I create a vector with three random numbers named q. Then I take the mean, then the exponential and lastly the square root. I could just wrap the codes around each other like this: sqrt(exp(mean(q))). But I could also use pipes, where I clearly see that first the mean, then the exponential and then the square root is taken: q %&gt;% mean() %&gt;% exp() %&gt;% sqrt(). Run both codes, they are producing the same result. #Making a vector with three random numbers q &lt;- c(6,3,8) #Taking first the mean, second the exponential and lastly the square root sqrt(exp(mean(q))) ## [1] 17.00204 ###With a Pipe q %&gt;% mean() %&gt;% exp() %&gt;% sqrt() ## [1] 17.00204 This was an easy and short example, but as you will see at the end of this chapter, the code can get really fast really messy and to have a clean code, we will use pipes. Furthermore, tidyverse users use pipes all the times, so even if you do not use them, you have to understand them. 3.4 Let’s wrangle the data 3.5 The dplyr package Dplyr is THE standard package, when it comes to data manipulation (next to Base R of course). It has essential functions, and helpful further functions. If you are able to understand the flexibility of these functions you can easily handle every data set. 3.5.1 The filter() command The first function I introduce you is the filter() function. Within the filter() function we can define certain conditions to cut our Data Set to. We need the filter() function and then a variable we want to filter based on. In my example, I only want to keep all observations, which have “HU” in their cntry - variable. Substantially this means, I cut down to all observations from Hungary. I use the == operator since I want to have all observations where the condition is true. Here is a quick reminder of logical operators in R: Logical Operator Meaning == equals &lt; smaller than &gt; greater than &lt;= less than or equal to &gt;= greater than or equal to ! (e.g. !=; &gt;!; &lt;!…) not equal, not greater than, not smaller than &amp; element-wise AND operator. It returns TRUE if both elements are true | element-wise OR operator. It returns TRUE if one of the statements is TRUE 3.5.1.1 Filtering for only one condition We start by defining a new object, let us call it d1. Then we take the data we want to filter, in our case ess. We define a pipe, write down filter and define a condition, in our case that only cases where cntry is equal to the iso2c code of Hungary, \"HU\". In the next code, we do the same and filter for cases that are equal or smaller than 40. Thus we get a dataset with observations who are 40 or younger. Note: Since the cntry variable is a character variable, we have to put the condition in square brackets. The variable agea is a numeric variable, therefore we only need the number. #filtering for cases only in Hungary d1 &lt;- ess %&gt;% filter(cntry == &quot;HU&quot;) #checking it head(d1) ## idnt year cntry agea gndr happy eisced netusoft ## 1 4501 2020 HU 21 2 3 6 4 ## 2 4502 2020 HU 48 1 5 5 3 ## 3 4503 2020 HU 80 1 6 3 4 ## 4 4504 2020 HU 38 1 6 3 2 ## 5 4505 2020 HU 40 2 10 3 3 ## 6 4506 2020 HU 40 1 1 7 2 ## trstprl lrscale ## 1 3 2 ## 2 5 3 ## 3 4 1 ## 4 1 3 ## 5 9 7 ## 6 2 3 #We only want participants younger than 40 d2 &lt;- ess %&gt;% filter(agea &lt;= 40) #checking it head(d2) ## idnt year cntry agea gndr happy eisced netusoft ## 1 3 2020 BE 28 2 3 4 5 ## 2 8 2020 BE 28 2 5 1 2 ## 3 9 2020 BE 39 2 8 3 1 ## 4 12 2020 BE 23 1 77 1 4 ## 5 14 2020 BE 40 2 3 5 2 ## 6 15 2020 BE 21 1 1 4 1 ## trstprl lrscale ## 1 4 7 ## 2 1 1 ## 3 4 6 ## 4 4 7 ## 5 3 2 ## 6 8 5 3.5.1.2 Filtering for multiple condition #filtering for cases in Hungary and France d1 &lt;- ess %&gt;% filter(cntry %in% c(&quot;HU&quot;, &quot;FR&quot;)) #Checking it head(d2) ## idnt year cntry agea gndr happy eisced netusoft ## 1 3 2020 BE 28 2 3 4 5 ## 2 8 2020 BE 28 2 5 1 2 ## 3 9 2020 BE 39 2 8 3 1 ## 4 12 2020 BE 23 1 77 1 4 ## 5 14 2020 BE 40 2 3 5 2 ## 6 15 2020 BE 21 1 1 4 1 ## trstprl lrscale ## 1 4 7 ## 2 1 1 ## 3 4 6 ## 4 4 7 ## 5 3 2 ## 6 8 5 #filtering for cases under 40 in Hungary and France d2 &lt;- ess %&gt;% filter(cntry %in% c(&quot;HU&quot;, &quot;FR&quot;) &amp; agea &lt;= 40) #Checking it head(d2) ## idnt year cntry agea gndr happy eisced netusoft ## 1 2704 2020 FR 23 1 9 3 3 ## 2 2705 2020 FR 30 2 8 7 2 ## 3 2708 2020 FR 17 1 1 2 2 ## 4 2709 2020 FR 26 2 10 1 4 ## 5 2710 2020 FR 34 1 8 3 1 ## 6 2711 2020 FR 30 1 5 3 5 ## trstprl lrscale ## 1 9 3 ## 2 7 9 ## 3 8 3 ## 4 9 6 ## 5 3 8 ## 6 9 10 #d2 &lt;- ess %&gt;% #Our dataset # filter(cntry %in% c(&quot;HU&quot;, &quot;FR&quot;), # agea &lt;= 40) #filtering for cases under 40 in Hungary and France with a comma #head(d2) 3.5.2 The select() function: We obviously do not care about all variables a dataset can offer (mostly). To select the variables we need, we can use the select() function. This of course depends on our research question. Let us say we want to select the year, the country, the happy variable, age, gender, income and education. 3.5.2.1 Selecting and deleting single Rows We only have to pass their column names to select(). That’s it. #Selecting relevant variables d1 &lt;- ess %&gt;% select(year, cntry, happy, agea, gndr, eisced) #Checking it head(d1) ## year cntry happy agea gndr eisced ## 1 2020 BE 1 45 1 6 ## 2 2020 BE 3 65 1 5 ## 3 2020 BE 3 28 2 4 ## 4 2020 BE 1 81 1 7 ## 5 2020 BE 4 56 2 7 ## 6 2020 BE 3 64 1 5 3.5.2.2 Deleting Rows To delete row, you just put a minus in front of the column, you want to delete. That’s it, the rest stays the same. #We delete columns by simply putting a comma before it d2 &lt;- d1 %&gt;% select(-agea) #Checking it head(d2) ## year cntry happy gndr eisced ## 1 2020 BE 1 1 6 ## 2 2020 BE 3 1 5 ## 3 2020 BE 3 2 4 ## 4 2020 BE 1 1 7 ## 5 2020 BE 4 2 7 ## 6 2020 BE 3 1 5 3.5.2.3 Combining select() with the filter() function One huge advantage of piping is, that we can clear our data in one step or at least big steps. The only thing you have to be aware of is what you put there. Remember the last command, is always the first to be executed. In this example, we select the 7 variables and then filter it for all observations under 40. The two commands are separated by a pipe. #Combining codes d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) #Checking it head(d1) ## year cntry happy agea gndr eisced ## 1 2020 BE 3 28 2 4 ## 2 2020 BE 5 28 2 1 ## 3 2020 BE 8 39 2 3 ## 4 2020 BE 77 23 1 1 ## 5 2020 BE 1 21 1 4 ## 6 2020 BE 5 23 2 6 3.5.3 The arrange() function If we want our data to be in a certain order, we arrange it with this function. 3.5.3.1 Arranging in ascending order The arrange() function is called, and afterwards we call the variable we want to arrange based on. The default function of arrange, orders the data always in ascending order. #Adding arrange() d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(agea) #Checking it head(d1) ## year cntry happy agea gndr eisced ## 1 2020 BE 6 15 2 4 ## 2 2020 BE 2 15 1 1 ## 3 2020 BG 3 15 9 99 ## 4 2020 BG 4 15 1 6 ## 5 2020 BG 2 15 2 2 ## 6 2020 BG 2 15 1 3 3.5.3.2 Arranging in descending order If we want to order them in ascending order, we have to call desc() inside the arrange() and put the name of the variable inside desc(). #arranging in descending order d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) #Checking it head(d1) ## year cntry happy agea gndr eisced ## 1 2020 BE 8 39 2 3 ## 2 2020 BE 1 39 1 2 ## 3 2020 BE 10 39 2 5 ## 4 2020 BE 2 39 1 3 ## 5 2020 BE 6 39 2 7 ## 6 2020 BE 3 39 1 1 3.5.4 The rename() and relocate() function Two functions to make our dataset structured more useful are rename() and relocate(), well they do what they basically named after: 3.5.4.1 Renaming Variables: rename() Rename() follows a simple logic, you call the function and write down the new name, thus the name you want to assign, then you put an equal sign, and put in the old name, thus the current name of the column. Note: If you have a variable, which is binary, thus has two discrete categories, you name it after the category which corresponds to the higher value. If male is 0 and female is 1, you name it after the higher category 1, therefore the variable is named female. #Renaming variables d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) %&gt;% rename(county = cntry, age = agea, education = eisced, female = gndr) #Checking it head(d1) ## year county happy age female education ## 1 2020 BE 8 39 2 3 ## 2 2020 BE 1 39 1 2 ## 3 2020 BE 10 39 2 5 ## 4 2020 BE 2 39 1 3 ## 5 2020 BE 6 39 2 7 ## 6 2020 BE 3 39 1 1 3.5.4.2 Relocating Variables: relocate() You call relocate and determine the order of the columns. You can also rearrange single columns, you call the name of the column, which you want to rarrange. and then you either call .before and a column or .after and a column. And the column you want to rearrange is placed before or after the column you want to place. You separate both arguments with a comma. #relocating variables d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) %&gt;% rename(country = cntry, age = agea, education = eisced, female = gndr) %&gt;% relocate(education, age, female, country, happy, year) #determine the order #Checking it head(d1) ## education age female country happy year ## 1 3 39 2 BE 8 2020 ## 2 2 39 1 BE 1 2020 ## 3 5 39 2 BE 10 2020 ## 4 3 39 1 BE 2 2020 ## 5 7 39 2 BE 6 2020 ## 6 1 39 1 BE 3 2020 #relocate after d2 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) %&gt;% rename(country = cntry, age = agea, education = eisced, female = gndr) %&gt;% relocate(country, .after = age) #Checking it head(d2) ## year happy age country female education ## 1 2020 8 39 BE 2 3 ## 2 2020 1 39 BE 1 2 ## 3 2020 10 39 BE 2 5 ## 4 2020 2 39 BE 1 3 ## 5 2020 6 39 BE 2 7 ## 6 2020 3 39 BE 1 1 #relocating before d3 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) %&gt;% rename(country = cntry, age = agea, education = eisced, female = gndr) %&gt;% relocate(country, .before = age) #Checking it head(d3) ## year happy country age female education ## 1 2020 8 BE 39 2 3 ## 2 2020 1 BE 39 1 2 ## 3 2020 10 BE 39 2 5 ## 4 2020 2 BE 39 1 3 ## 5 2020 6 BE 39 2 7 ## 6 2020 3 BE 39 1 1 3.5.5 The mutate() function The next function is the powerful mutate() command. For the start, just think about mutate as a Variable with which you can transform or mutate variables to other variables as you please. You call mutate() and first you define the name of a new column with a name that is not existent in your dataset. You could also use the name of an existing column, but be careful! Then the new values are overwriting the old ones and we do not want that necessarily, therefore I recommend to always define new columns. After defining the new name you put an equal sign after it and define the calculation. In our case we just multiply happy, so all values in the happy column by 10 #mutating variables d1 &lt;- ess %&gt;% mutate(happy_10 = happy*10) #checking it head(d1) ## idnt year cntry agea gndr happy eisced netusoft ## 1 1 2020 BE 45 1 1 6 2 ## 2 2 2020 BE 65 1 3 5 4 ## 3 3 2020 BE 28 2 3 4 5 ## 4 4 2020 BE 81 1 1 7 3 ## 5 5 2020 BE 56 2 4 7 5 ## 6 6 2020 BE 64 1 3 5 4 ## trstprl lrscale happy_10 ## 1 2 3 10 ## 2 3 6 30 ## 3 4 7 30 ## 4 2 6 10 ## 5 6 4 40 ## 6 6 10 30 #mutating variables d2 &lt;- ess %&gt;% mutate(new_variable = happy*10/eisced+67, female_char = as.character(gndr)) %&gt;% select(female_char, new_variable) #checking it head(d2) ## female_char new_variable ## 1 1 68.66667 ## 2 1 73.00000 ## 3 2 74.50000 ## 4 1 68.42857 ## 5 2 72.71429 ## 6 1 73.00000 We can also use mutate for more than calculations. We can also define new columns by mutating existing columns into different classes: The variable new_variable is just a random mathematical operation including two variables. The second call in mutate() changes the class of the gndr variable to a character and saves it as such in the dataset. #more mutating d2 &lt;- ess %&gt;% mutate(new_variable = happy*10/eisced+67, female_char = as.character(gndr)) %&gt;% select(female_char, new_variable) #Checking it head(d2) ## female_char new_variable ## 1 1 68.66667 ## 2 1 73.00000 ## 3 2 74.50000 ## 4 1 68.42857 ## 5 2 72.71429 ## 6 1 73.00000 What makes mutate() so powerful is that you can use other functions in it to define your variables as you want. Image you have the happy variable. Quick reminder, this variable contains the answers to the question “How happy are you?” in the questionnaire of the European Social Survey. It is scaled on a 0 (not happy at all) to 10(very happy). Let us say we want to change that. We want to make a variable with only three categories (unhappy, neutral, happy). We decide that all values from 0-4 should be classifies as unhappy, 5 should be classified as neutral and everything above 5 as happy. How can we do that in R? There are different ways and I will show you some of them: 3.5.5.1 Recoding with mutate() using recode(). You can use the recode() function: First, we define a new column name, in our example happy_cat. Then we call recode() and inside of it we call the variable we want to transform, in our case happy. We put call the category we want to transform in these brackets ``. We put an equal sign after it and define the new value we want to assign to the category. Note: We can assign different values, do not worry about the “NA_real_” value I will come back to that later #Get an overview of the variable table(ess$happy) ## ## 1 2 3 4 5 6 7 8 9 10 77 88 99 ## 849 918 888 897 888 852 883 877 908 895 49 46 50 #recoding variables d1 &lt;- ess %&gt;% mutate( gndr_fac = as.factor(gndr), #always check the class happy_cat = dplyr::recode(happy, `1` = 0, `2` = 0, `3` = 0, `4` = 0, `5` = 1, `6` = 2, `7` = 2, `8` = 2, `9` = 2, `10` = 2, `77` = NA_real_, `88` = NA_real_, `99` = NA_real_), female = dplyr::recode(gndr_fac, `1` = &quot;Male&quot;, `2` = &quot;Female&quot;)) #Let us check how it worked out table(d1$happy) ## ## 1 2 3 4 5 6 7 8 9 10 77 88 99 ## 849 918 888 897 888 852 883 877 908 895 49 46 50 table(d1$happy_cat) ## ## 0 1 2 ## 3552 888 4415 table(d1$gender) ## &lt; table of extent 0 &gt; 3.5.5.2 Recoding with mutate() using case_when() As you see, the recode() command is quite extensive. The tidyverse offers a way more intuitive command, the case_when() function. The function case_when is a generalized ifelse function. Which means we can use logical operators. The recode() function is way too extensive, it gives you full control over the data, but we do not that much control: We again define happy_cat. We call case_when() and inside we call our variable we want to transform. We define a logical statements, in our case that happy smaller than 5, meaning that we tell R that all values under 5 should be transformed. We call the wave ~ and tell R what value should substitute all values which are TRUE for the logical statement. Note: What is not explicitly stated in the case_when() function will be coded as NA. #recoding with case_when d1 &lt;- ess %&gt;% mutate(gndr_fac = as.factor(gndr), happy_cat = case_when( happy &lt; 5 ~ 0, happy == 5 ~ 1, happy &gt; 5 ~ 2), female = case_when( gndr == 1 ~ &quot;Male&quot;, gndr == 2 ~ &quot;Female&quot; )) #Checking it table(d1$female) ## ## Female Male ## 4404 4381 table(d1$happy_cat) ## ## 0 1 2 ## 3552 888 4560 3.5.5.3 Recoding with mutate() using ifelse() Do you remember the ifelse() function? As already mentioned, the case_when() command is a generalized ifelse() function. If you want to keep it old school, we can also recode with the ifelse() function: #recoding with ifelse function d1 &lt;- ess %&gt;% mutate(gndr_fac = as.factor(gndr), happy_cat = ifelse(happy &lt; 5, 0, ifelse(happy == 5, 1, ifelse(happy &gt; 5, 2, NA ))), female = ifelse(gndr_fac == 1, &quot;Male&quot;, ifelse(gndr_fac == 2, &quot;Female&quot;, NA)) ) #Check it table(d1$happy_cat) ## ## 0 1 2 ## 3552 888 4560 table(d1$female) ## ## Female Male ## 4404 4381 3.5.6 Handling Missing Values/Incomplete Data As you saw right now, not all data in a dataset is complete. Of course not, there are several sources, which can lead to incomplete/missing data. Can you think of reasons why? In Data Sciences we need to deal with missing values directly. If we look into the codebook, the ESS declares different types of missing values with high numbers: 7(7) means “Refusal”, so the respondent refused to answer, 8(8) means “dont know”, and 9(9) “No answer”. Note that the ESS does so, since some researchers are interested in missing values, and why they happen, so they can investigate it. For us, this is a problem, because we cannot run an analysis with missing values. There are two options: Using statistics to artificially fill them out, this called multiple imputation techniques, but this requires advanced data science knowledge so I do not recommend that for beginners. Just delete incomplete observations to have a dataset without missing values The ESS assigns values to missing values. First, we have to tell R that we do want those values to be missing values otherwise it biases our analyses. In the following, we have to recode the variables and tell R, that the missing values are declared as such: I already showed you how to do so. In the following I will use case_when() We will do the second one, and I already showed how to recode useless values to NAs so this should be clear by now. Remember, the ifelse() and recode() explicitly need input to turn values into NAs. #Creating missing values and showing a mutating workflow d1 &lt;- ess %&gt;% filter(agea &gt;=40) %&gt;% select(year, cntry, netusoft, agea, eisced, gndr, happy) %&gt;% arrange(desc(agea)) %&gt;% rename( internet_use = netusoft, age = agea, education = eisced, female = gndr) %&gt;% mutate( internet_use = case_when( internet_use &lt; 5 ~ NA_real_, TRUE ~ internet_use), age = case_when( age == 999 ~ NA_real_, TRUE ~ age), education = case_when( education %in% c(55, 77, 88, 99) ~ NA_real_, TRUE ~ education), female = case_when( female == 1 ~ 0, female == 2 ~ 1, female == 9 ~ NA_real_, TRUE ~ female), happy = case_when( happy %in% c(77, 88, 99) ~ NA_real_, TRUE ~ happy) ) #Checking it head(d1) ## year cntry internet_use age education female happy ## 1 2020 BE NA NA 6 1 10 ## 2 2020 BE NA NA 2 1 5 ## 3 2020 BG NA NA 2 1 3 ## 4 2020 BG NA NA 2 0 3 ## 5 2020 CH NA NA 1 1 5 ## 6 2020 CH NA NA 5 0 1 When you print the dataset, you see that now there are some values named “NA”. That stands for not available. If we want to delete NAs, we can use the tidyverse way by simply piping to drop_na() The base R way would be to put the dataset name na.omit(). We assign both to a new data frame. We will see in the result that all rows are deleted which include NAs. This is one of the reasons it is important to cut down to variables we only need, so that only incomplete observations of our variables we need are deleted. So always remember, first transforming, than dropping. #dropping NAs d2 &lt;- d1 %&gt;% drop_na() #dropping NAs d2 &lt;- na.omit(d1) #Checking if there are NAs colSums(is.na(d2)) ## year cntry internet_use age ## 0 0 0 0 ## education female happy ## 0 0 0 We see that there are no missing values left in our dataset, and our number of observations are reduced. Now we have all ingredients to make our dataset. Do you remember our research question? We want to find out if people, who tend to not trust science are less willing to get vaccinated. We want to do that for all people over 40. 3.5.7 The group_by() and summarize() functions Two of the most useful commands in R for summary statistics are group_by() and summarize(). group_by() helps us to, when we have categorical variables with several observations and we want to calculate a metric e.g. for this group. The dataset we have loaded, the ESS for example asked 9000 respondents about their level of happiness. Image you are interested in the average level of happiness of men and women. Here the group_by() functions defines the groups we want to aggregate e.g. gender. 3.5.7.1 With one grouping variable and one metric summarize() defines the metric we want to search. For example we want to calculate the mean of the level of happiness of men and women. We also could just calculate the median for example. Let us have a look: First, we call the group_by() function and define the group we want to aggregate, thus the group we are interested in. In our example, we want to aggregate based on the sexes, therefore we need the to define that. Before that we have to delete NAs or transform the variable to the categories we are interested in, therefore we first call mutate() and transform the variable. Second, we call summarize() and define the name of the new column, let us call it average_happiness() and then we call the metric of the variable we are interested in. In our example, we were interested in the average happiness, so we have to call mean() and the happy variable: #group_by and summarize d1 &lt;- ess %&gt;% mutate( gndr_fac = as.factor(gndr), female = case_when( gndr_fac == 1 ~ &quot;Male&quot;, gndr_fac == 2 ~ &quot;Female&quot;, gndr_fac == 9 ~ NA_character_ )) %&gt;% drop_na() %&gt;% group_by(female) %&gt;% summarize(average_happiness = mean(happy)) #Checking it head(d1) ## # A tibble: 2 × 2 ## female average_happiness ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female 6.54 ## 2 Male 7.17 Et voilà, we get a dataset with two observations, because we have only two groups. The second row is the average_happiness row we defined in the summarize() function. 3.5.7.2 With more grouping variables and metrics The group_by() and summarize() functions are of course way more flexible, for example, we can define more groups. What about that, you are interested in the level of happiness of females, and males in the countries conducted by the ESS. You just put the countries and then the gender variable in the group_by(). Further we might be interested in more metrics, no problem, let us just define more columns with summarize(). Again, you have to first clean the data by transforming the class and deleting missing values. #grouping and summarize d1 &lt;- ess %&gt;% mutate( country = cntry, gndr_fac = as.factor(gndr), female = case_when( gndr_fac == 1 ~ &quot;Male&quot;, gndr_fac == 2 ~ &quot;Female&quot;, gndr_fac %in% c(77, 88, 99) ~ NA_character_), age = case_when( agea == 999 ~ NA_real_, TRUE ~ agea) ) %&gt;% drop_na() %&gt;% group_by(country, female) %&gt;% summarize(average_happiness = mean(happy), median_happiness = median(happy), average_age = mean(age), meadian_age = median(age) ) ## `summarise()` has grouped output by &#39;country&#39;. You ## can override using the `.groups` argument. #Check it out glimpse(d1) ## Rows: 40 ## Columns: 6 ## Groups: country [20] ## $ country &lt;chr&gt; &quot;BE&quot;, &quot;BE&quot;, &quot;BG&quot;, &quot;BG&quot;, &quot;C… ## $ female &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;… ## $ average_happiness &lt;dbl&gt; 6.142857, 7.793103, 6.2626… ## $ median_happiness &lt;dbl&gt; 6.0, 6.0, 6.0, 6.0, 5.0, 5… ## $ average_age &lt;dbl&gt; 53.42857, 53.00431, 51.903… ## $ meadian_age &lt;dbl&gt; 53.0, 52.0, 50.0, 52.0, 55… 3.6 Merging Datasets 3.6.0.1 Introduction to merging with dplyr and preparing data Sometimes it could be the case that you need variables, which are not in one dataset a priori available, but in another dataset. For this case you load both datasets and merge them together. This only works if there is a similiar data structure, so know your data ! As an example, I will show how to do that with World Bank Data. From this data we can gather nearly all important economic indicators for countries since the 1970s. But mostly we need to merge them to datasets we are interested in. We will merge the World Bank Data with the ESS data. So we can analyze variables, which were not collected in the same dataset. There are several ways of getting World Bank Data, but I will show you the most efficient. There is the package WDI with which you can get data through an API (Application Programming Interface). Long story short, we do not need to download anything and get the data directly with code: First we define, which countries should be included: countries &lt;- c(&quot;BE&quot;, &quot;BG&quot;, &quot;CH&quot;, &quot;EE&quot;, &quot;FR&quot;,&quot;GB&quot;) Afterwards we define, which variables we want. You do that by using the official indicator, thus the variable you want. You can find the indicators on the website of the world bank data. Click on the variables you want, then click on the details, and there you find the indicator. I will use GDP per capita, Fuel exports, CO2 emissions (kt). indicators = c(&quot;NY.GDP.PCAP.CD&quot;, &quot;TX.VAL.FUEL.ZS.UN&quot;, &quot;EN.ATM.CO2E.KT&quot;) Now we are ready to use the API. To do so we call the WDI function. We define the argument country to only get countries we are interested in. We also define the indicators. Lastly, with the “start =” argument we define the starting year, so data which goes back to that date is loaded and with “end =” is analagos to define where the time should stop. Thus, both arguments define the time span we want to inspect wb &lt;- WDI( country = countries, #We include our countries indicator = indicators, #We include our variables start = 2020, #start date end = 2020) #end date #This takes some time, especially if you have more countries, more indicators and a longer time span. #Checking it head(wb) ## country iso2c iso3c year NY.GDP.PCAP.CD ## 1 Belgium BE BEL 2020 45587.97 ## 2 Bulgaria BG BGR 2020 10148.34 ## 3 Estonia EE EST 2020 23565.18 ## 4 France FR FRA 2020 39179.74 ## 5 Switzerland CH CHE 2020 85897.78 ## 6 United Kingdom GB GBR 2020 40217.01 ## TX.VAL.FUEL.ZS.UN EN.ATM.CO2E.KT ## 1 5.0182074 85364.10 ## 2 4.6375716 34138.10 ## 3 4.8606100 7097.52 ## 4 1.8844347 267154.70 ## 5 0.6102448 34916.10 ## 6 7.0642437 308650.30 Let us transform the dataset (Only variables we need, arranging it alphabetically, renaming it and rounding one variable to make the numbers more intuitive): #Cleaning the wb data wb &lt;- wb %&gt;% select(iso2c, NY.GDP.PCAP.CD, TX.VAL.FUEL.ZS.UN, EN.ATM.CO2E.KT) %&gt;% arrange(iso2c) %&gt;% rename(gdp_per_cap = NY.GDP.PCAP.CD, fuel_exp = TX.VAL.FUEL.ZS.UN, co2 = EN.ATM.CO2E.KT ) %&gt;% mutate(fuel_exp = round(fuel_exp, 2)) #Checking it head(wb) ## iso2c gdp_per_cap fuel_exp co2 ## 1 BE 45587.97 5.02 85364.10 ## 2 BG 10148.34 4.64 34138.10 ## 3 CH 85897.78 0.61 34916.10 ## 4 EE 23565.18 4.86 7097.52 ## 5 FR 39179.74 1.88 267154.70 ## 6 GB 40217.01 7.06 308650.30 Now, we cut down and prepare our ESS data by selecting the countries we are interested in, renaming the country variable (I’ll explain later why), we group by the country (iso2c) and the year (year) to get the average happiness by country. Lastly, we round the value to get only two decimals. #preparing ess d1 &lt;- ess %&gt;% filter(cntry == c(&quot;BE&quot;, &quot;BG&quot;, &quot;CZ&quot;, &quot;EE&quot;, &quot;FI&quot;)) %&gt;% rename(iso2c = cntry) %&gt;% group_by(iso2c, year) %&gt;% summarise(happy_agg = round(mean(happy), 2)) ## `summarise()` has grouped output by &#39;iso2c&#39;. You can ## override using the `.groups` argument. #Checking it head(d1) ## # A tibble: 5 × 3 ## # Groups: iso2c [5] ## iso2c year happy_agg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BE 2020 7.53 ## 2 BG 2020 6.41 ## 3 CZ 2020 7.08 ## 4 EE 2020 8.52 ## 5 FI 2020 7.51 3.6.0.2 left_join() and right_join() with one identifier To merge data there are important functions from the dplyr package: The left_join() and the right_join() function. Since they are a bit complicated to understand, we will go through them and in the end, you can choose the one you prefer. left_join(): You want to keep all observations in the first table, including matching observations in the second table. You merging the data from the right table to left table and get a datset with the same number of rows as the left table: right_join(): You want to keep all observations in the second table, including matching observations in the first table. You join from the left table to the right table, this time the table takes on the number of observations of the table which is right joined. Well, in the end of the day it is a matter of programming socialisation and taste, which one do you prefer. I will show you both. To merge two datasets, you need at least one common variable. One variable will be your unique identifier. Since every country is unique in the dataset that is our unique identifier i.e. the variable we give R to tell him how to merge the datasets: First, we define a new object, where we will save the dataset called merged_data. Second, we call left_join() Then we set our left table and our right table. We define the by = argument and put the unique identifier in quotation marks, meaning the variable name #left_join merged_data &lt;- left_join(d1, wb, by = &quot;iso2c&quot;) #Checking it head(merged_data) For right_join() you do the exact same, but remember you get a different result. #right_join merged_data2 &lt;- right_join(d1, wb, by = &quot;iso2c&quot;) #Checking it head(merged_data2) ## # A tibble: 6 × 6 ## # Groups: iso2c [6] ## iso2c year happy_agg gdp_per_cap fuel_exp co2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BE 2020 7.53 45588. 5.02 85364. ## 2 BG 2020 6.41 10148. 4.64 34138. ## 3 EE 2020 8.52 23565. 4.86 7098. ## 4 CH NA NA 85898. 0.61 34916. ## 5 FR NA NA 39180. 1.88 267155. ## 6 GB NA NA 40217. 7.06 308650. 3.6.0.3 left_join() and right_join() with two identifiers Sometimes, you have multiple dimensions. For example, what if we also include the year? Then every country-year observation is our unique identifier. Why? Because one observation was collected in country X to time point Y. That is why you should always know your data and your research goal, because accordingly you have to write your code. Let us get again World Bank Data and clean it: #Getting the Data wb &lt;- WDI( country = c(&quot;BE&quot;, &quot;BG&quot;), #We include our countries indicator = indicators, #We include our variables start = 2019, #start date end = 2020) #end date #Cleaning the Data wb &lt;- wb %&gt;% select(-iso3c) %&gt;% arrange(iso2c) %&gt;% rename(gdp_per_cap = NY.GDP.PCAP.CD, fuel_exp = TX.VAL.FUEL.ZS.UN, co2 = EN.ATM.CO2E.KT ) %&gt;% mutate(fuel_exp = round(fuel_exp, 2)) #Checking the Data head(wb) ## country iso2c year gdp_per_cap fuel_exp co2 ## 1 Belgium BE 2019 46641.721 7.38 92989.4 ## 2 Belgium BE 2020 45587.968 5.02 85364.1 ## 3 Bulgaria BG 2019 9874.336 9.53 39159.9 ## 4 Bulgaria BG 2020 10148.342 4.64 34138.1 Now we just simulate some data we want to merge: #Getting the Data d1 &lt;- data.frame( iso2c = c(&quot;BE&quot;, &quot;BE&quot;, &quot;BG&quot;, &quot;BG&quot;, &quot;CZ&quot;, &quot;CZ&quot;), year = c(2019, 2020, 2019, 2020, 2019, 2020), happy_agg = c(5.95, 6.76, 6.56, 7.54, 6.27, 6.88) ) #Checking the Data head(d1) ## iso2c year happy_agg ## 1 BE 2019 5.95 ## 2 BE 2020 6.76 ## 3 BG 2019 6.56 ## 4 BG 2020 7.54 ## 5 CZ 2019 6.27 ## 6 CZ 2020 6.88 How does it look like when we have two variables and the combination out of those is our unique identifier? left_join() with two identifiers: As you can see the dataset again takes on the number of observations of our left table To implement it, we have to change by argument. We define a vector, where we put our two identifiers in quotation marks: #Merging the Data with left_join() merged_data3 &lt;- left_join(d1, wb, by = c(&quot;iso2c&quot;, &quot;year&quot;)) #Checking it head(merged_data3) ## iso2c year happy_agg country gdp_per_cap fuel_exp ## 1 BE 2019 5.95 Belgium 46641.721 7.38 ## 2 BE 2020 6.76 Belgium 45587.968 5.02 ## 3 BG 2019 6.56 Bulgaria 9874.336 9.53 ## 4 BG 2020 7.54 Bulgaria 10148.342 4.64 ## 5 CZ 2019 6.27 &lt;NA&gt; NA NA ## 6 CZ 2020 6.88 &lt;NA&gt; NA NA ## co2 ## 1 92989.4 ## 2 85364.1 ## 3 39159.9 ## 4 34138.1 ## 5 NA ## 6 NA right_join() with two identifiers: Again we do the same with right_join(): merged_data4 &lt;- right_join(d1, wb, by = c(&quot;iso2c&quot;, &quot;year&quot;)) head(merged_data4) ## iso2c year happy_agg country gdp_per_cap fuel_exp ## 1 BE 2019 5.95 Belgium 46641.721 7.38 ## 2 BE 2020 6.76 Belgium 45587.968 5.02 ## 3 BG 2019 6.56 Bulgaria 9874.336 9.53 ## 4 BG 2020 7.54 Bulgaria 10148.342 4.64 ## co2 ## 1 92989.4 ## 2 85364.1 ## 3 39159.9 ## 4 34138.1 Note: However, this chapter only touched the basics and for merging alone there are several further commands, like inner_join(), anti_join(), semi_join()…etc. But when you encounter problems with the two functions I showed you will run into them eventually. 3.7 Outlook This Chapter introduced you to the basic functions of the dplyr package. You are now able to transform variables according to your needs. Further, you learned how to use pipes to work efficient code. Loading data, transforming data, and preparing datasets for the analysis. There are a lot of techniques, and in the end of the day data wrangling is the most extensive part, because every analysis is individual and requires an individual preparation. The more individual the analysis, the more individual the preparation. I recommend the standard book for data science in R in this chapter, since it has a strong emphasis on data manipulation: “R for Data Science” by Hadley Wickham &amp; Garrett Grolemund. 3.8 Exercise Section 3.8.1 Exercise 1: Let’s wrangle kid You are interested in discrimination and the perception of the judicial. More specifically, you want to know if people, who fell discriminated evaluate courts differently. Below you see a table with all variables you want to include in your analysis: Variable Description Scales idnt Respondent’s identification number unique number from 1-9000 year The year when the survey was conducted only 2020 cntry Country BE, BG, CH, CZ, EE, FI, FR,GB, GR, HR, HU, IE, IS, IT, LT,NL, NO, PT, SI, SK agea Age of the Respondent, calculated Number of Age = 15-90 999 = Not available gndr Gender 1 = Male; 2 = Female; 9 = No answer happy How happy are you 0 (Extremly unhappy) - 10 (Extremly happy); 77 = Refusal; 88 = Don’t Know; 99 = No answer eisced Highest level of education, ES - ISCED 0 = Not possible to harmonise into ES-ISCED; 1 (ES-ISCED I , less than lower secondary) - 7 (ES-ISCED V2, higher tertiary education, =&gt; MA level; 55 = Other; 77 = Refusal; 88 = Don’t know; 99 = No answer netusoft Internet use, how often 1 (Never) - 5 (Every day); 7 = Refusal; 8 = Don’t know; 9 = No answer trstprl Most people can be trusted or you can’t be too careful 0 (You can’t be too careful) - 10 (Most people can be trusted); 77 = Refusal; 88 = Don’t Know; 99 = No answer lrscale Left-Right Placement 0 (Left) - 10 (Right); 77 = Refusal; 88 = Don’t know; 99 = No answer Wrangle the data, and assign it to an object called ess. Select the variables you need Filter for Austria, Belgium, Denmark, Georgia, Iceland and the Russian Federation Have a look at the codebook and code all irrelevant values as missing. If you have binary variables recode them from 1, 2 to 0 to 1 You want to build an extremism variable: You do so by subtracting 5 from the from the variable and squaring it afterwards. Call it extremism Rename the variables to more intuitive names, don’t forget to name binary varaibles after the category which is on 1 drop all missing values Check out your new dataset 3.8.2 Exercise 2: Merging Datasets The gapminder package in R loads automatically the gapminder dataset. The gapminder project is an independent educational non-profit fighting global misconceptions, check out their website: https://www.gapminder.org/ The gapminder dataset is already loaded. Get an overview of the gapminder dataset. There are different ways to do so, you can choose by yourself Load World Bank Data from 1972 to 2007 and load the variable “Exports and Goods (% of GDP)”. Merge the World Bank data to the gapminder data, so a dataset evolves with the number of observations of the gapminder data. d. Clean the data by dropping all missing values "],["data-visualisation.html", "Chapter 4 Data Visualisation 4.1 Introduction 4.2 Distributions: Histogram, Density Plots, and Boxplots 4.3 Ranking: Barplot 4.4 Evolution: Line Chart 4.5 Correlation: Scatterplots 4.6 Making Plots with facet_wrap() and facet_grid() 4.7 Outlook 4.8 Outlook 4.9 Exercise Section", " Chapter 4 Data Visualisation pacman::p_load(&quot;tidyverse&quot;, &quot;babynames&quot;, &quot;sf&quot;, &quot;ggridges&quot;, &quot;rnaturalearth&quot;, &quot;forcats&quot; ,&quot;tmap&quot;) 4.1 Introduction The tidyverse includes the most popular package for data visualization in R, ggplot2. With its relative straight forward code and its huge flexibility, and I mean HUGE FLEXIBILTY, it became the standard form of Data Visualization. It is aims to simplify data visualization by utilizing the “Grammar of Graphics” defined by Leland Wilkinson. While it may appear complicated at first, it just creates a frame and adds elements to it. Let us start by looking at the code structure and creating the frame. The central code here is ggplot(): ggplot() As we can see, we get an empty frame and in the following we will go through the standard forms of data visualizations by simply adding elements to this empty frame. But this is only the Peak of what is possible with Data Visualization in R. I strongly recommend to further work on this topic for two reasons, especially R is the perfect language to dive deeply in this topic. R is known for beautiful data visualizations and it is a reason for its popularity. 4.2 Distributions: Histogram, Density Plots, and Boxplots The first type of visualizations are displaying distributions. We should always get an overview of how our variables are distributed, because the distributions gives us valuable information about the data structure. For example a lot of statistical models assume certain distributions and to identify if we can test data with those models, we have to make sure that it does not violate the distribution assumption. Further, distributions make it easy to detect outliers or biases, since they are easy to spot with such visualizations. 4.2.1 Histograms 4.2.1.1 Basic Histogram Let us start with a normal Histogram. A histogram is an accurate graphical representation of the distribution of a numeric variable. It takes as input numeric variables only. The variable is cut into several bins, and the number of observation per bin is represented by the height of the bar. Before making our first plot, let us simulate some data: #Setting Seed for reproducibility set.seed(123) #Simulating data data1 &lt;- data.frame( type = c(rep(&quot;Variable 1&quot;, 1000)), value = c(rnorm(1000)) ) #Looking at the data glimpse(data1) ## Rows: 1,000 ## Columns: 2 ## $ type &lt;chr&gt; &quot;Variable 1&quot;, &quot;Variable 1&quot;, &quot;Variable … ## $ value &lt;dbl&gt; -0.56047565, -0.23017749, 1.55870831, … We now have a dataset for a random variable called “Variable 1” and this variable has 500 values assigned to it. We now want to know the distribution of these values and decide to plot a histogram. Now we have data and we can go straight to business. For a histogram in ggplot, we need the ggplot() command. Afterward, we include our dataset, in our case data. We use a comma in the ggplot() command after the data and add a new command, called aes(). In this command we need to define the x-axis and the y-axis. Here we just need the x-axis, since a histogram logically plots the “count” thus how often one value appears in the dataset, ggplot does that automatically. Last thing remaining is to close the bracket of aes() and of the ggplot() command and to tell ggplot, what kind of visualization we want. Our answer comes with a “+” after the closed command and we add the command geom_histogram(). ggplot(data1, aes(x = value)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. And you just made your first histogram. But as you can see, it does not look nice. The reason is that we have to tell ggplot2 what we specifically want to change. And we can do so by defining the inside of the geom_histogram() function. I guess the first step is to make the bins visible and to change the color from gray to something nicer. We can do so by the defining the color for the borders of the bins, and the fill command to change the color of the bins in the geom_historgram() function. Let us set it to white to make it visible. Note: I could have defined any color, the only condition is to put it in quotation marks. Some colors such as white can be just written down, but you can always use any hexcode inside the quotation marks and it will work fine. ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. Looks better! But still, we have to think about that we want to publish this in an article or report. And for this purpose it is not sufficient. Next we should change the names of the labs, we can do so by adding a plus + again after the geom_histogram() command and using the labs() function. In this function we define the name of our x-axis and the y-axis. While we are at it, we can define the title in this function as well. What I like to do next is to scale the x-axis and to have ggplot display the values of each of the horizontal grid lines. Here an important mechanic is needed. The code scale_x_continous() helps us to rescale the x-axis. In general, the family of scale_* functions are powerful, because re-scaling the axis can (must not necessarily) change the visualization, thus these are powerful tools we should be aware of: ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Histogram&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) I do not know about you, but I have a huge problem with the gray grid as a background. This is the default grid by ggplot2 and we can change that. Again, we need a “+”, and then we can just add the function without any things in it. I decided for the theme_bw() function, which is my favorite theme, but I found a website, where you can have a look at the different themes, look here. ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Histogram&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. Well, we did it. I think that this plot can be displayed in an article or report. Good job! One elemental thing I want to talk about is the width of the size. Currently, the binwidth is at 0.3. We can adjust that by including binwidth in the geom_histogram() command: #histogram bindwidth = 0.1 ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;, binwidth = 0.1) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Histogram with binwidth = 0.1&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_minimal() #histogram with bindwidth = 0.6 ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;, binwidth = 0.6) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Histogram with binwidth = 0.6&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_minimal() 4.2.1.2 Multiple Histograms In this part, I want to show you variations of the Histogram visualization plot. We will start with multiple distributions we probably want to display. To do so, we need a new variable we will call “Variable 2”, with its own observations and add it to our dataset: #Creating data data2 &lt;- data.frame( type = c(rep(&quot;Variable 2&quot;, 1000)), value = c(rnorm(1000, mean = 4)) ) #rowbinding it with data1 data2 &lt;- rbind(data1, data2) We have two variables, each with their own distribution. We have to tell ggplot2 to distinguish the numbers by the different variables. We do so by modifying the inside of the aes() function. Our x-axis stays the same, right? We still want the values to be on the x-axis, so that parts stays the same. We define the fill within the aes() command to tell ggplot to fill the values of the two variables. Additionally, I will specify position = “identity” in the plot, this specification helps to adjust the position, when two histograms are overlapping, which will be the case. Note: I leave out the `fill` specification for the reason that the colors are defined by default for both graphs (but we can change that, I will show that later). ggplot(data2, aes(x=value, fill=type)) + geom_histogram(color=&quot;#e9ecef&quot;, position = &quot;identity&quot;) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. As you can see, we get two plots colored by the type there are assigned to. We can now play around a bit. I want to introduce you the alpha specification. This makes colors more transparent. Again this a command should be used if objects are overlapping to have a clearer picture of the overlap. Additionally, I will scale new colors, here the scale_* function family comes again into play. We will use the scale_fill_manual command, since we want to change the color of the fill specification in the aes() command: ggplot(data2, aes(x=value, fill=type)) + geom_histogram(color=&quot;#e9ecef&quot;, alpha = 0.6, position = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;#8AA4D6&quot;, &quot;#E89149&quot;)) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. 4.2.2 Density Plots A density plot is a representation of the distribution of a numeric variable. It uses a kernel density estimate to show the probability density function of the variable. It is basically a smoothed version of a histogram. Since the logic is the same, except that the geom_histogram() is changed with geom_density(). 4.2.2.1 Basic Density Plot Let us start with a basic density plot: ggplot(data1, aes(x = value)) + geom_density() Well, we now can do the exact same things as we did above: Fill the density plot with a color with fill(), make the fill color more transparent with alpha() and change the color of the line with color() in the geom_density() function. We can rescale the x-axis with scale_x_continous, and we can change the labels of the axis with labs(), and change the theme to theme_minimal(). ggplot(data1, aes(x = value, fill =)) + geom_density(color = &quot;lightgrey&quot;, fill = &quot;#F8E59A&quot;, alpha = 0.6) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Density Plot&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_minimal() 4.2.2.2 Multiple Density Plots We could also do this with multiple density plots, remember that we always need the data structure to plot a graph. For this reason we again need data3. The rest stays again the same as with histograms: Note: I just copied the code from above, changed the geom_histogram() to geom_density() and then I just changed the colors, the alpha and the theme. That’s it. And that is mostly how plotting works, just copy and paste from the internet, and adjust what you do not like. ggplot(data2, aes(x=value, fill=type)) + geom_density(color=&quot;#0a0a0a&quot;, alpha = 0.9, position = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;#FDE725FF&quot;, &quot;#440154FF&quot;)) + theme_minimal() 4.2.3 Boxplots 4.2.3.1 Basic Boxplots The last visualization form of distributions are Boxplots. Boxplots are a really interesting form of showing distributions with a lot of information. Let us have a look at their anatomy, before I show you how to program them: Anatomy of a Boxplot The black rectangle represents the Interquartile Range (IQR), thus the difference between the 25th and 75th percentiles of the data The red line in the black rectangle represents the median of the data. The end of the lines show the value at the 0th percentile, respectively 100th percentile, thus the minimum and the maximum value of the IQR, not the data. The dots beyond the black lines are potential outliers and the points at the ends are the minimum value, respectively maximum value in the data. We should be aware of them, because if we ignore them, they could bias our statistical models, but more to that in Chapter 6. Let us implement a boxplot in R. Again the only thing that changes is that we use the standard ggplot() function and go on with the function geom_boxplot(): ggplot(data1, aes(x = value)) + geom_boxplot() We can also make that graph pretty with the same techniques as above: ggplot(data1, aes(x = value)) + geom_boxplot() + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Boxplot&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_classic() 4.2.3.2 Multiple Boxplots A huge advantage of Boxplots are that it is an easy way to compare the structure of distributions of different groups. Consider following example: We want to compare the income of people with migration background and people without migration background. Let us say we collected a sample of people with 2000 respondents, 1000 with and 1000 without migration background. We further collected the incomes of each respondent. Be aware that we now need to define the y-axis with income. Since we do not look anymore at the count of the distribution, but the distribution over another variable (here:income). Let us look at the plot: # Set seed for reproducibility set.seed(123) # Simulate income data income_18_24 &lt;- rnorm(1000, mean = 40000, sd = 11000) income_25_34 &lt;- rnorm(1000, mean = 55000, sd = 17500) income_35_59 &lt;- rnorm(1000, mean = 70000, sd = 25000) # Combine into a data frame data5 &lt;- data.frame( income = c(income_18_24, income_25_34, income_35_59), age = factor(rep(c(&quot;18-24&quot;, &quot;25-34&quot;, &quot;35-59&quot;), each = 1000)) ) ggplot(data5, aes(x = age, y = income, fill = age)) + geom_boxplot() Before interpreting the plot, let us make it prettier: We change labels of the x-axis, y-axis and give the plot a title with the labs() function. I do not like the colors, we change them with the scale_fill_manual(). Again, we define alpha = 0.5 and also width = 0.5 of the boxes in geom_boxplot(). I also think, we do not need a legend, therefore we can remove it, and use the theme() function. This function is powerful, since its specification gives us a lot of possibilities to design the plot according to our wishes. We specify in the theme() function that legend.position = \"none\", which means that we do not want the legend to be displayed at all: # Create boxplot ggplot(data5, aes(x = age, y = income, fill = age)) + geom_boxplot(alpha = 0.5, width = 0.5) + scale_fill_manual(values = c(&quot;#acf6c8&quot;, &quot;#ecec53&quot; ,&quot;#D1BC8A&quot;)) + labs( title = &quot;Comparison of Income Distribution by Age&quot;, x = &quot;Age&quot;, y = &quot;Income&quot; ) + theme_minimal() + theme(legend.position = &quot;none&quot;) We have a lot of information here. First, we clearly see that the median of people with migration background is lower than the median income of people without migration background. But we further see, that the income distribution of respondents without migration background is more spread out over a higher range. We can see that by the longer lines of the boxplot of respondents without migration background. Also the IQR range of both variables are varying. The box of people without migration background is again smaller, which again is an indicator that respondents without migration background are more spread out. In comparison, we can see that respondents with migration background in the 50th -75th percentile earn as much as respondents without migration background in the 25th to 50th percentile. I could go on the whole day, boxplots are very informative and a nice tool to inspect and compare distribution structures. Note: I used simulated data, therefore this data is fictional. 4.3 Ranking: Barplot 4.3.1 Basic Barplot The most famous, and easiest way of showing values of different groups is the Barplot. A barplot (or barchart) is one of the most common types of graphic. It shows the relationship between a numeric and a categoric variable. Each entity of the categoric variable is represented as a bar. The size of the bar represents its numeric value. In ggplot, we only have to define the x-axis, and y-axis inside the ggplot() function, and add the function geom_bar(). Inside geom_bar() you have to add stat = “identity”, for the simple reason, that we have to tell ggplot2 to display the numbers of the column “strength”, otherwise it will give us an error. # Create data data4 &lt;- data.frame( name=c(&quot;King Kong&quot;,&quot;Godzilla&quot;,&quot;Superman&quot;, &quot;Odin&quot;,&quot;Darth Vader&quot;) , strength=c(10,15,45,61,22) ) #Plotting it ggplot(data4, aes(x = name, y = strength)) + geom_bar(stat = &quot;identity&quot;) Again, we can change the look of our plot. We start by changing the color by setting color within the geom_bar() function, we set a theme, let us do theme_test() this time and we change the names of the columns with the labs() function. Note: I can disable the name of the x-lab by simply adding empty quotation marks in the labs() function ggplot(data4, aes(x = name, y = strength)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot; ) + theme_test() There is also another possibility to use Barplots. We could use them to count categories. Like we would in a histogram with the difference that we now have not a range of numbers, where we count how many numbers for one variable. We have a groups and want to count how often those groups appear in our dataset. Let us assume we asked 20 kids what their favorite fictional character is among Superman, King Kong and Godzilla. data5 &lt;- data.frame( hero = c(rep(&quot;Superman&quot;, 10), rep(&quot;King Kong&quot;, 3), rep(&quot;Godzilla&quot;, 7)), id = c(seq(1:20)), female = c(rep(&quot;Female&quot;, 7), rep(&quot;Male&quot;, 5), rep(&quot;Female&quot;, 1), rep(&quot;Female&quot;, 3), rep(&quot;Male&quot;, 4)) ) ggplot(data5, aes(x = hero)) + geom_bar(fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Count&quot;, title = &quot;What is your favourite fictional Character?&quot; ) + scale_y_continuous(breaks = seq(0,10,1)) + theme_test() We could also turn around both Barplots to have a vertical Barplot. That is quite easy, we just have to add the coord_flip() function. This function swaps the x-axis and the y-axis. Let us look at the plots: #Plot 1 ggplot(data4, aes(x = name, y = strength)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot; ) + theme_test() + coord_flip() #Plot 2 ggplot(data5, aes(x = hero)) + geom_bar(fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Count&quot;, title = &quot;What is your favourite fictional Character?&quot; ) + scale_y_continuous(breaks = seq(0,10,1)) + theme_test() + coord_flip() 4.3.2 Reordering them To make a Barplot more intuitive, we can order it so the bar with the highest x-value is at the beginning and then it decreases or vice versa. To do so, we use the forcats package We take the code from above and wrap the x-value in the fct_reorder() command and determine the value it should be reorder based on, in our case the x-value is the name of the fictional characters and the value is the strength or the count: Note: You could also do it in descending order by just wrapping a desc() around the value the variable should be reorder based on thus it would look like this: fct_reorder(name, desc(strength)). #Plot 1 ggplot(data4, aes(x = fct_reorder(name, strength), y = strength)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot; ) + theme_test() #Plot 2 ggplot(data4, aes(x = fct_reorder(name, strength), y = strength)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot; ) + theme_test() + coord_flip() 4.3.3 Grouped and Stacked Barplots We can go a step further with barplots and group them. Let us assume we asked respondents to tell us how healthy they feel on a scale from 0-10. But we want to separate respondents older than 40 and younger than 40. And we again separate the group between female and male respondents. Therefore we look at the average answer of 4 groups: Female, older 40, Male, older 40, Female younger 40 and Male younger 40. To see if there are gender differences within these groups. Let us get the data: data6 &lt;- data.frame( female = c(&quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;), age = c(&quot;Old&quot;, &quot;Old&quot;, &quot;Young&quot;, &quot;Young&quot;), value = c(5, 2, 8, 7) ) Now we got the data. We have to define 3 parameters within aes(). The x-axis is the age groups, the y-axis the average value, and we have to define fill = female, since this is our group we want to investigate within the age groups. Inside geom_bar(), we need two arguments stat = “identity” and position = dodge. Et voila we will get our first grouped barplot. ggplot(data6, aes(x = age, y = value, fill = female)) + geom_bar(position = &quot;dodge&quot;, stat=&quot;identity&quot;) We could have also used the a stacked barplot. The difference is, that we have one bar for our x-axis group, in our example the age group, and then the amount of the second group, the gender, is stacked on top of each it other. You could also see that as a normal barplot, where the bar is colored depending on the percentual distribution of the other group. In the code the only thing changing is that we set the position argument in the geom_bar() code to position = “stack”: ggplot(data6, aes(x = age, y = value, fill = female)) + geom_bar(position = &quot;stack&quot;, stat=&quot;identity&quot;) Let us make them pretty with our well-known techniques, it is always the same story. But twonew thing are introduced The argument width = 0.35 is included to the geom_bar() so we can determine the width of the bars I introduce you so-called color palettes. Instead of manually scaling the color, you can use built-in color palettes for different types of plots. For Barplot you can use the scale_fill_brewer, which includes different palettes and colors, which are automatically displayed. Have a look at the palettes of the command here. That can be really helpful, if you have a lot of groups, so you do not have to think about different colors, which look good together. #Plot 1 ggplot(data6, aes(x = age, y = value, fill = female)) + geom_bar(position = &quot;dodge&quot;, stat=&quot;identity&quot;, width = 0.35) + scale_fill_brewer(palette = &quot;Accent&quot;) + scale_y_continuous(breaks = seq(0, 15, 1)) + labs( x = &quot;Age Cohort&quot;, y = &quot;Average Score Well-Being&quot;, title = &quot;Impact of Age on Well-Being by Gender&quot; ) + theme_minimal() + theme(legend.title=element_blank()) #Plot 2 ggplot(data6, aes(x = age, y = value, fill = female)) + geom_bar(position = &quot;stack&quot;, stat=&quot;identity&quot;, width = 0.35) + scale_fill_brewer(palette = &quot;Accent&quot;) + scale_y_continuous(breaks = seq(0, 15, 1)) + labs( x = &quot;Age Cohort&quot;, y = &quot;Average Score Well-Being&quot;, title = &quot;Impact of Age on Well-Being by Gender&quot; ) + theme_minimal() + theme(legend.title=element_blank()) 4.4 Evolution: Line Chart A quite familiar plot is the line chart. A quite popular way of showing the evolution of a variable over a variable on the x-axis. We know them mostly from time series analyses, where a certain period is on the x-axis. Since such line charts with dates are well known, I will stick with them as an example. A line chart or line graph displays the evolution of one or several numeric variables. Data points are connected by straight line segments the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. 4.4.1 Basic Line Plot In ggplot, we stick with the ggplot() function, define our x-axis and our y-axis. We add the function geom_line() to it. # Setting Seed set.seed(500) # create data date &lt;- 2000:2024 y &lt;- cumsum(rnorm(25)) y2 &lt;- cumsum(rnorm(25)) data6 &lt;- data.frame(date,y, y2) ggplot(data6, aes(x = date, y = y)) + geom_line() Normally we would go on and make the plot pretty. But there are additional aesthetics to a line plot. First, we can change the line type. The line type can be straight as in the default layout, but I will change set it in the geom_line() command to line type = \"dashed\". For an overview of all line types look here. Second, I change the size of the line with setting size = 1 in the geom_line() command. The rest of the aesthetics are stay the same, re-scaling axes, coloring, and themes. ggplot(data6, aes(x = date, y = y)) + geom_line(color = &quot;#0F52BA&quot;, linetype = &quot;dashed&quot;, size = 1) + scale_y_continuous(breaks = seq(-1, 6, 1), limits = c(-1, 6)) + scale_x_continuous(breaks = seq(2000, 2024, 2)) + labs( y = &quot;&quot;, x = &quot;Year&quot;, title = &quot;A Line Plot&quot; ) + theme_bw() 4.4.2 Multiple Line Chart In the next step, we want to plot multiple lines in one plot. This is useful when we want to compare the evolution of variables for example over time. In ggplot2 we only need to add another layer with a plus and add another geom_line() command. But now things get a bit complicated: Inside the ggplot() command we only add our dataset with our dataset, nothing more. In the first geom_line() command we add the aes() function and define x and y. Until now, we only wrote the aes() function inside the ggplot() function, but now we have to write it in the geom_line() function, since we add another geom_line() layer. In the second geom_line() command we define the our next layer. This time the x-axis stays the same logically. But now we change y and set it to the second variable we want to inspect. ggplot(data6) + geom_line(aes(x = date, y = y)) + geom_line(aes(x = date, y = y2)) As always, we make the plot pretty in the next step. I will use the same code as above. But regarding the lines itself, we can separate the aesthetics separately: We can set the line type, color and size differently for each layer. We just have to specify it inside the geom_line() command for the respective layer. ggplot(data6) + geom_line(aes(x = date, y = y), linetype = &quot;twodash&quot;, size = 1, color = &quot;#365E32&quot;) + geom_line(aes(x = date, y = y2), linetype = &quot;longdash&quot;, size = 1, color = &quot;#FD9B63&quot;) + scale_y_continuous(breaks = seq(-5, 6, 1), limits = c(-5, 6)) + scale_x_continuous(breaks = seq(2000, 2024, 2)) + labs( y = &quot;&quot;, x = &quot;Year&quot;, title = &quot;A Line Plot&quot; ) + theme_bw() 4.4.3 Grouped Line Charts Another possibility of using line charts is to look at the evolution of groups separately. I introduce you to the babynames dataset, which is a package in R, which loads automatically the dataset about the most popular babynames in the US from 1880 until 2017. Let us have a look at it: ###Looking at the dataset head(babynames) ## # A tibble: 6 × 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Anna 2604 0.0267 ## 3 1880 F Emma 2003 0.0205 ## 4 1880 F Elizabeth 1939 0.0199 ## 5 1880 F Minnie 1746 0.0179 ## 6 1880 F Margaret 1578 0.0162 Well, let us say we are interested in the popularity of the names Michael, Abby, and Lisa. Let us cut down the dataset to these three names with the filter() function you learned in the previous chapter: babynames_cut &lt;- babynames %&gt;% filter(name %in% c(&quot;Emma&quot;, &quot;Kimberly&quot;, &quot;Ruth&quot;)) %&gt;% filter(sex == &quot;F&quot;) In the next step, let us plot the popularity of these three names over time. We have to specify the x and y-axis and further add a geom_line() layer. So far, so normal. The next thing we do, is to tell ggplot2 that we want groups. We do so, in the ggplot() function by setting group = name. We should also set the colors = name, otherwise all lines will be black and we cannot distinguish, which line belongs to which group. ggplot(babynames_cut, aes(x = year, y = n, group = name, color = name)) + geom_line() Well, that looks good, we can see that Ruth had its peak in the 20s, Kimberly in the 60s and Emma is currently on the rise. Let us design the plot with a theme, remove the legend title, add some meaningful lab names and add a color palette with scale_color_brewer(). Regarding the labs, I will introduce you a way of re-naming the legend, by simply setting color = \"New Name\" in the labs() function ggplot(babynames_cut, aes(x = year, y = n, group = name, color = name)) + geom_line(size = 1) + scale_color_brewer(palette = &quot;Set1&quot;) + labs( x = &quot;Year&quot;, y = &quot;Number of Babies named&quot;, title = &quot;Popularity of Babynames over time&quot;, color = &quot;Name&quot; ) + theme_minimal() 4.5 Correlation: Scatterplots The last type of visualization are scatter plots. A Scatter plot displays the relationship between 2 numeric variables. Each dot represents an observation. Their position on the X (horizontal) and Y (vertical) axis represents the values of the 2 variables. It is a quite popular way in articles to investigate the relationship between two variables. 4.5.1 Basic Scatterplot We want to investigate the relationship between two variables. Let us assume we are the owner of a big choclate company. We want to find the out the relationship of our marketing spendings on the sales of our chocolate. We have the data for each quarter of the year and for years: # Set the seed for reproducibility set.seed(123) # Simulate data n &lt;- 100 marketing_budget &lt;- runif(n, min = 1000, max = 10000) sales &lt;- 2000 + 0.65 * marketing_budget + rnorm(n, mean = 1400, sd = 750) quarters &lt;- rep(c(&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;), 25) # Create a data frame data_point &lt;- data.frame(marketing_budget, sales, quarters) #Give it a name data_point$name &lt;- &quot;Chocolate Milk&quot; A scatter plot in R is made with the same logic as always. First, we define our x and y-axis in the ggplot() command. We add a comma and call the geom_point() function ggplot(data_point, aes(x = marketing_budget, y = sales)) + geom_point() Let us make the plot pretty and as always, we define a color for the dots in the layer, thus the geom_point() function, re-scale the axes (in this case I would just re-scale the x-axis), re-name the labels, give a title and define a theme. ggplot(data_point, aes(x = marketing_budget, y = sales)) + geom_point(color = &quot;#99582a&quot;) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot; ) + theme_classic() 4.5.2 Scatter Plots with multiple Groups Let us go on with our example. We do not only have one sort of chocolate but two. Chocolate milk and dark chocolate. Let us get the data for dark chocolate as well: # Set the seed for reproducibility set.seed(123) # Simulate data n &lt;- 100 marketing_budget &lt;- runif(n, min = 1000, max = 10000) sales &lt;- 1500 + 0.3 * marketing_budget + rnorm(n, mean = 1400, sd = 750) quarters &lt;- rep(c(&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;), 25) #Making a df df_dark &lt;- data.frame(marketing_budget, sales, quarters) #Give it a name df_dark$name &lt;- &quot;Dark Chocolate&quot; #rowbind it with the other dataset data_point &lt;- rbind(data_point, df_dark) Now, we could run the same code as above, but we would not be able to distinguish, which dots belong to which chocolate. That is the reason we need to specify in the aes() function the argument color = name. That will color the dots in the group they belong to. I will manually give the colors, since I have to use brown colors for this example. ggplot(data_point, aes(x = marketing_budget, y = sales, color = name)) + geom_point() + scale_color_brewer(palette = &quot;BrBG&quot;) + scale_color_manual(values = c(&quot;#e71d36&quot;, &quot;#260701&quot;))+ scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot;, color = &quot;Product&quot; ) + theme_classic() ## Scale for colour is already present. ## Adding another scale for colour, which will replace ## the existing scale. As we can see, in general marketing leads to higher sales of chocolate. Further we can see that Marketing has a higher effect on Chocolate milk than on Dark Chocolate. Using colors is one way to differentiate between groups in scatter plots. Another way is to use different shapes. The only thing we have to change the color argument with a the shape argument. We can also adjust the size and I want to do that, since I want to make the forms more visible. Since this changes the design of the points, we have to set the argument size = 2.5 inside the geom_point() function. In the labs() function we change the argument color = “Product” to shape = “Product”, because we now name the legend of the shape layer, and not the color layer. Let us have a look: ggplot(data_point, aes(x = marketing_budget, y = sales, shape = name)) + geom_point(size = 2.5) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot;, shape = &quot;Product&quot; ) + theme_classic() There are different types of shapes and we can set them manually via numbers. For this purpose we can use the scale_shape_manual() and call the argument size = 4. There are different shapes and they have numbers assigned to them, to call them we have to set size equal to the number of the shape. Check out this website for an overview over the different shapes. We can also combine different colors with different shapes. We just leave the color = name argument in the ggplot() function. In the labs() function we will set the argument to color = \"\" and shape = \"\". So the legend shows the colored shape as the legend. ggplot(data_point, aes(x = marketing_budget, y = sales, shape = name, color = name)) + geom_point(size = 2.5) + scale_color_manual(values = c(&quot;#e71d36&quot;, &quot;#260701&quot;)) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot;, shape = &quot;&quot;, color = &quot;&quot; ) + theme_classic() 4.6 Making Plots with facet_wrap() and facet_grid() Sometimes we do not want to compare the elements in a plot (e.g. dots, lines), but the plot itself with other plots from the same dataset. This can be a powerful tool, in terms of telling a story with data. Further, we can gain several information by splitting the data into graphs and directly comparing them. 4.6.1 The facet_wrap() function That is rather abstract, let us stick with our chocolate company. We want to compare the effect of our marketing budget on sales for different quarters. We want to plot the same scatter plot as before, but this time for each quarter. We could of course split up the data set to each quarter and plot 4 plots. But that is not efficient. Let us copy the code from above for the basic plot, and just add the facet_wrap() function and inside this wave symbol ~ and add the variable we want separate for, in our case the quarters. ggplot(data_point, aes(x = marketing_budget, y = sales)) + geom_point() + facet_wrap(~ quarters) As you can see ggplot2 plots 4 graphs for each quarter. Instead of plotting 4 graphs and writing unnecessary long code, we can use the handy facet_warp() function. If we want to make the graph pretty, it is quite easy, since it is identical as if we want to make a single plot pretty. Thus, we can just copy the code from above and include it: ggplot(data_point, aes(x = marketing_budget, y = sales)) + geom_point(color = &quot;#99582a&quot;) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot; ) + theme_classic() + facet_wrap(~ quarters) We can also add a facet_wrap() function for our plot with different shapes and colors for chocolate milk and dark chocolate: ggplot(data_point, aes(x = marketing_budget, y = sales, shape = name, color = name)) + geom_point(size = 2.5) + scale_color_manual(values = c(&quot;#e71d36&quot;, &quot;#260701&quot;)) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot;, shape = &quot;&quot;, color = &quot;&quot; ) + theme_classic() + facet_wrap(~ quarters) 4.6.2 The facet_grid() function The facet grid function does the same as the facet_wrap() function, but it allows to add a second dimension. Image we want to know the development of the temperature for the first four months of the years 2018, 2019, 2020 of the cities London, Paris and Berlin. This time, we decide for a line chart to visualize the evolution of the temperatures. Manually we would have to make nine plots, For each city one plot for each year. Or we just use the facet_grid() function: Since we have two dimensions, we have to define them. We define the row and then we define the column and separate them with this wave symbol ~ , thus facet_wrap(row ~ column) We use the geom_line() function and make the plot pretty by giving meaningful label names, coloring each year with a unique color, giving a title, defining a theme and hiding the legend, since it would only show that the years have unique colors. #Set seed for reproducilty set.seed(123) # Define the cities, years, and months cities &lt;- c(&quot;London&quot;, &quot;Paris&quot;, &quot;Berlin&quot;) years &lt;- 2018:2020 months &lt;- 1:4 # Only the first four months # Create a data frame with all combinations of City, Year, and Month data &lt;- expand.grid(City = cities, Year = years, Month = months) # Simulate temperature data with some variation depending on the city data$Temperature &lt;- round(rnorm(nrow(data), mean = 15, sd = 10), 1) + with(data, ifelse(City == &quot;London&quot;, 0, ifelse(City == &quot;Paris&quot;, 5, -5))) # Check the first few rows of the dataset head(data) ## City Year Month Temperature ## 1 London 2018 1 9.4 ## 2 Paris 2018 1 17.7 ## 3 Berlin 2018 1 25.6 ## 4 London 2019 1 15.7 ## 5 Paris 2019 1 21.3 ## 6 Berlin 2019 1 27.2 # Convert Month to a factor for better axis labeling data$Month &lt;- factor(data$Month, levels = 1:4, labels = month.abb[1:4]) # Basic ggplot object p &lt;- ggplot(data, aes(x = Month, y = Temperature, group = Year, color = factor(Year))) + geom_line() + labs(title = &quot;Average Monthly Temperature (Jan-Apr, 2018-2020)&quot;, x = &quot;Month&quot;, y = &quot;Temperature (°C)&quot;, color = &quot;Year&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + facet_grid(Year ~ City) #Printing it p 4.7 Outlook That was a brief introduction to data visualization in R and the basic visualization used in Data Analysis. The start of most visualizations are those basic plots and as you saw it is the same workflow. First, you have to built the basic plot, second you have to add the layers you want. And ggplot2 seems to be complicated at first, but since data visualization is a crucial task in Data Science and Research you will have get very fluent, very fast. I can only encourage you to go on and explore the world of data visualization in R with ggplot2. In this section, I want to give a glimpse of what is possible: 4.7.1 Combining different types of Graphs You can also combine different types of graphs. But be careful! Too much in one graph can be distracting. In the following, I will present a graph with two y-axis, one for a line chart with dots and one for a barplot. The x-axis presents the months of the year # Simulating example data example_data &lt;- data.frame( months = factor(1:12, levels = 1:12, labels = month.abb), avg_temp = c(0.6, 1.8, 4.6, 6.1, 10.4, 19, 18.3, 17.9, 15.2, 9.6, 4.7, 2.6), n_deaths = c(149, 155, 200, 218, 263, 282, 318, 301, 247, 250, 194, 205) ) # Scaling factor to align avg_temp with n_deaths scale_factor &lt;- max(example_data$n_deaths) / max(example_data$avg_temp) # Create the combined graph with dual y-axes ggplot(example_data, aes(x = months)) + geom_bar(aes(y = n_deaths), stat = &quot;identity&quot;, fill = &quot;skyblue&quot;, alpha = 0.6) + geom_line(aes(y = avg_temp * scale_factor, group = 1), color = &quot;#2c2c2c&quot;, linewidth = 1, linetype = &quot;dashed&quot;) + scale_y_continuous( name = &quot;Number of Traffic Deaths&quot;, sec.axis = sec_axis(~ . / scale_factor, name = &quot;Average Temperature (Celsius)&quot;) ) + labs(x = &quot;&quot;, title = &quot;Number of Traffic Deaths and Average Temperature per Month&quot;) + theme_bw() + theme( axis.title.y.left = element_text(color = &quot;skyblue&quot;), axis.title.y.right = element_text(color = &quot;#2c2c2c&quot;) ) 4.7.2 Distributions: Ridgeline Chart and Violin Chart Two visualizations, which get more and more popular: The Ridgeline Chart and the Violin Chart. The violin chart displays a density plot horizontally. Moreover, it displays mirrors the density plot and puts it toegether: # Setting seed for reproducibility set.seed(123) # Simulate example sports data sports_data &lt;- data.frame( sport = factor(rep(c(&quot;Basketball&quot;, &quot;Soccer&quot;, &quot;Swimming&quot;, &quot;Gymnastics&quot;, &quot;Tennis&quot;), each = 100)), height = c( rnorm(100, mean = 200, sd = 10), # Basketball players are typically tall rnorm(100, mean = 175, sd = 7), # Soccer players have average height rnorm(100, mean = 180, sd = 8), # Swimmers rnorm(100, mean = 160, sd = 6), # Gymnasts are typically shorter rnorm(100, mean = 170, sd = 9) # Tennis players ) ) # Create the violin plot ggplot(sports_data, aes(x = sport, y = height, fill = sport)) + geom_violin(trim = FALSE) + labs( title = &quot;Distribution of Athletes&#39; Heights by Sport&quot;, x = &quot;Sport&quot;, y = &quot;Height (cm)&quot; ) + theme_bw() + theme( legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5, size = 16, face = &quot;bold&quot;), axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14) ) + scale_fill_brewer(palette = &quot;RdBu&quot;) The Ridgeline chart is a nice way to compare more than 2 distributions. The idea is to plot the scale on the x-axis. On the y-axis the groups you want to compare are plotted: # Setting seed for reproducibility set.seed(123) # Normal distribution normal_data &lt;- rnorm(1000, mean = 50, sd = 10) # Left-skewed distribution (using exponential distribution) left_skewed_data &lt;- rexp(1000, rate = 0.1) # Right-skewed distribution (using log-normal distribution) right_skewed_data &lt;- rlnorm(1000, meanlog = 3, sdlog = 0.5) # Bimodal distribution (combining two normal distributions) bimodal_data &lt;- c(rnorm(500, mean = 35, sd = 5), rnorm(500, mean = 60, sd = 5)) # Combine the data into a data frame example_data &lt;- data.frame( value = c(normal_data, left_skewed_data, right_skewed_data, bimodal_data), distribution = factor(rep(c(&quot;Normal&quot;, &quot;Left-Skewed&quot;, &quot;Right-Skewed&quot;, &quot;Bimodal&quot;), each = 1000)) ) # Create the ridgeline chart ggplot(example_data, aes(x = value, y = distribution, fill = distribution)) + geom_density_ridges() + scale_fill_brewer(palette = &quot;Dark2&quot;) + labs( x = &quot;Values&quot;, y = &quot;Distribution&quot;, title = &quot;A Ridgeline Chart&quot; ) + theme_ridges() + theme(legend.position = &quot;none&quot;) ## Picking joint bandwidth of 2.34 4.7.3 Ranking: Lollipop Charts and Radar Charts 4.7.3.1 Lollipop Charts Lollipop Charts are getting more and more popular, so I want to show them to you. The idea is quite simple, it is a Bar Chart, instead a bar it uses a line and a dot: To implement it, we need to add a geom_point() layer in combination with a geom_segment() layer. We define the axis within ggplot() layer. Lastly, we have to define the aesthetics in the geom_segment() plot. ggplot(data4, aes(x=name, y=strength)) + geom_point() + geom_segment(aes(x=name, xend=name, y=0, yend=strength)) Let us make it pretty. We can give the line different colors and adjust it with the same methods as the line chart. The same goes for the dots we can adjust them as much as we like: ggplot(data4, aes(x=name, y=strength)) + geom_segment(aes(x=name, xend=name, y=0, yend=strength), color = &quot;grey&quot;) + geom_point(size = 4, color = &quot;#74B72E&quot;) + labs(x = &quot;Fictional Character&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot;) + theme_light() + theme( panel.grid.major.x = element_blank(), panel.border = element_blank(), axis.ticks.x = element_blank() ) 4.7.4 Maps R also offers a variety of possibilities to work with spatial data. Of course, visualization of maps is an integral part, when working with spatial data. With R you can plot all sorts of maps: Interactive maps with leaflet, shape files of countries and multiple layers with the sf package and standard visualization tools such as connection maps or Cartograms. Here is an example of an interactive map filled with data. To keep the code as simple as possible I used the tmap package. It is a map of the world, which displays via its color, if a country is an high income, upper middle income, lower middle income or low income country: # Get country-level shapefiles world &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) world &lt;- world %&gt;% filter(gdp_year == 2019) %&gt;% mutate(`Income Group` = case_when( income_grp %in% c(&quot;1. High income: OECD&quot;, &quot;2. High income: nonOECD&quot;) ~ &quot;1. High Income&quot;, income_grp == &quot;3. Upper middle income&quot; ~ &quot;2. Upper Middle Income&quot;, income_grp == &quot;4. Lower middle income&quot; ~ &quot;3. Lower Middle Income&quot;, income_grp == &quot;5. Low income&quot; ~ &quot;4. Low Income&quot;) ) # Plot using tmap tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(world) + tm_polygons(&quot;Income Group&quot;, title = &quot;Income Groups&quot;, palette = &quot;viridis&quot;, style = &quot;cat&quot;, id = &quot;sovereignt&quot;) 4.8 Outlook This chapter was an introduction to one of the most fun part of R, making plots. I introduced you to the standard forms of visualization and gave you a little primer to further visualizations and what is possible in R. The package ggplot2 is one of the most intuitive (although not for beginners) for data visualization. There is only one book I have to recommend regarding data visualization and that is the “R Gallery Book” by Kyle W. Brown. Also check out the website of this book, it is the standard website, where I search for code snippets for graphs, I can only recommend it. 4.9 Exercise Section "],["loops-and-functions.html", "Chapter 5 Loops and Functions 5.1 Loops 5.2 apply() Function Family 5.3 Writing your own functions 5.4 Outlook 5.5 Exercise Section", " Chapter 5 Loops and Functions For our last chapter I want to introduce to a way to work more efficient. R is a programming language for statistical analysis, but it also includes classical elements of programming. Two main operations are loops and functions. We can automate tasks and the earlier you learn about it the faster you can advance and understand the logic of R. 5.1 Loops For example, you can use a loop to iterate through a list of numbers and perform calculations on each number, or to go through the rows of a dataset and apply certain operations to each row. Loops provide a way to streamline your code and avoid writing repetitive instructions manually. There are different type of Loops, but for this course we focus just on the for loops, since you will see them also in the QM Tutorial. 5.1.1 For loops Do you remember my grade example from the first chapter? grade &lt;- 4.0 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 3.3 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 4.0 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 2.3 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;OK&quot; grade &lt;- 1.7 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;Good Job&quot; I could now write down all my grades and assign them as I did in the first chapter, but there is a way to automatize this process. For that I will use the For loop. First, let us make a vector with grades: grades &lt;- c(1.7, 3.3, 4.0, 2.3, 1.0) Now, we can directly dive into the loop. Write down for and in brackets you define the loop iterator, this is the i in the loop. Then you define in which object of interest you want to iterate. In our case, the operation should be iterated in the grades vector. I could also write down the number 5, but it is convention to define an object. Why? After closing the brackets you open fancy brackets and write down your function, as you would normally, but this time you need to define how the iterator is used. Since I use the numbers in grades, my iterator needs to be put in brackets, after the name of the grades. Why? And that’s it basically for (i in 1:length(grades)) { if (grades[i] == 1.0) { print(&quot;Amazing&quot;) } else if (grades[i] &gt; 1.0 &amp; grades[i] &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grades[i] &gt; 2.0 &amp; grades[i] &lt;= 3.0) { print(&quot;OK&quot;) } else if (grades[i] &gt; 3.0 &amp; grades[i] &lt;= 4.0) { print(&quot;Life goes on&quot;) } } ## [1] &quot;Good Job&quot; ## [1] &quot;Life goes on&quot; ## [1] &quot;Life goes on&quot; ## [1] &quot;OK&quot; ## [1] &quot;Amazing&quot; Loops can look differently: In this example I have a number vector and my let the console print a sentence, where I vary the number and therefore the sentence changes over every loop # creating num vector num &lt;- c(1, 2, 3, 4, 5, 249) # looping through vector for (i in num) { print(stringr::str_c(&quot;This is the &quot;, i, &quot;th Iteration&quot;)) } ## [1] &quot;This is the 1th Iteration&quot; ## [1] &quot;This is the 2th Iteration&quot; ## [1] &quot;This is the 3th Iteration&quot; ## [1] &quot;This is the 4th Iteration&quot; ## [1] &quot;This is the 5th Iteration&quot; ## [1] &quot;This is the 249th Iteration&quot; 5.1.2 Nested Loops Because you will eventually encounter them, I will show you shortly nested loops: First, let us play a game of tic tac toe: #Defining a matrix ttt &lt;- matrix(c(&quot;X&quot;, &quot;O&quot;, &quot;X&quot;, &quot;O&quot;, &quot;X&quot;, &quot;O&quot;, &quot;O&quot;, &quot;X&quot;, &quot;O&quot;), nrow = 3, ncol = 3, byrow = TRUE) We define a loop with an iterator i for the rows of the matrix, and we define another one for the columns with the iterator j. Afterwords, we built up the body, in which aim to get information about the matrix and its content. The sentence shows, which rows and columns contain which values. for (i in 1:nrow(ttt)) { for (j in 1:ncol(ttt)) { print(paste(&quot;On row&quot;, i, &quot;and column&quot;, j, &quot;the board contains&quot;, ttt[i,j])) } } ## [1] &quot;On row 1 and column 1 the board contains X&quot; ## [1] &quot;On row 1 and column 2 the board contains O&quot; ## [1] &quot;On row 1 and column 3 the board contains X&quot; ## [1] &quot;On row 2 and column 1 the board contains O&quot; ## [1] &quot;On row 2 and column 2 the board contains X&quot; ## [1] &quot;On row 2 and column 3 the board contains O&quot; ## [1] &quot;On row 3 and column 1 the board contains O&quot; ## [1] &quot;On row 3 and column 2 the board contains X&quot; ## [1] &quot;On row 3 and column 3 the board contains O&quot; 5.2 apply() Function Family 5.2.1 apply() apply() takes a data frame or matrix as an input and gives output in vector, list or array. Apply function in R is primarily used to avoid explicit uses of loop constructs. The idea is to apply a function repeatedly to a matrix or data frame: apply(X, MARGIN, FUNCTION) #Let us create a matrix with random numbers mat &lt;- matrix(1:10, nrow = 5, ncol = 6) #Checking it head(mat) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 6 1 6 1 6 ## [2,] 2 7 2 7 2 7 ## [3,] 3 8 3 8 3 8 ## [4,] 4 9 4 9 4 9 ## [5,] 5 10 5 10 5 10 Assume you now want to calculate the mean of every column: apply(mat, 2, mean) #calculating mean ## [1] 3 8 3 8 3 8 apply(mat, 2, sum) #calculating sum ## [1] 15 40 15 40 15 40 apply(mat, 2, sd) #calculating sd ## [1] 1.581139 1.581139 1.581139 1.581139 1.581139 ## [6] 1.581139 #The corresponding Loop would look like this: for (i in 1:ncol(mat)) { mean_col &lt;- mean(mat[, i]) print(mean_col) } ## [1] 3 ## [1] 8 ## [1] 3 ## [1] 8 ## [1] 3 ## [1] 8 The apply() function is useful especially if you are working with dimensional bodies and want to calculate anything. However, they can not keep up with the flexibility of loops, you should be aware of that. 5.2.2 Notes Loops and the apply function are widely used in programming. However, this is no programming course, it is an introduction, so now you have an idea what is happening, if you are seeing those two things in the scripts. But if you are interested in this topic, please read into while loops and repeat loops. The apply() function is part of a family: sapply(), lapply(), tapply() are also in that family. 5.3 Writing your own functions We can again safe a lot of time and be more efficient by writing our own function. First, you need to define a name for your function# Afterwards, you write down the command with the function() command. In to your brackets you put your variables. Later your input follows those variables. After the fancy brackets, you define your operation with your predefined variables. Lastly, you want the function to return your quantity of interest and close the fancy brackets Afterwards you have a function saved and can operate with it #My function is just a sum add &lt;- function(x, y) { result &lt;- x + y return(result) } add(2,7) #Now I can use my function ## [1] 9 Let us calculate the area of a circle aoc &lt;- function(radius) { pi &lt;- 3.14159 area &lt;- pi * radius^2 return(area) } aoc(5) ## [1] 78.53975 Let us combine what we have learned in this chapter with a meeting of animals. You do not have to understand the code, and I also do not want to explain it, but just to make clear what loops and functions are able to do. And all packages are written based on functions and loops, so it is useful to get an overview how it could look like. In this loop, we let R print out what persons in a classroom are doing. Before that we identify where the individuals are sitting. And the seating order in a class, is nothing more that a matrix right? We have rows and columns. This loops identifies in which row (i) and in which column (j) an individual sits. Moreover, the values in the matrix are student ids and the loop knows the id of students and prints out where which student sits. So this is an unnecessary loop, but I think a good example. Imagine the class are 1000 of people, then this loop would be necessary. As I said you do not need to completely understand it. Its purpose is to illustrate what you can do in R. # The function print_classroom &lt;- function(x) { for (i in 1:length(x)) { # Outer loop iterates over rows for (j in 1:length(x[[i]])) { # Inner loop iterates over columns student &lt;- x[[i]][j] if (student == 1) { comment &lt;- &quot;Alice&quot; } else if (student == 2) { comment &lt;- &quot;Bob&quot; } else if (student == 3) { comment &lt;- &quot;Cathy&quot; } else if (student == 4) { comment &lt;- &quot;David&quot; } else if (student == 5) { comment &lt;- &quot;Eva&quot; } else { comment &lt;- paste(&quot;Unknown student&quot;, student, &quot;is doing something interesting.&quot;) } cat(&quot;At row&quot;, i, &quot;column&quot;, j, &quot;:&quot;, comment, &quot;\\n&quot;) } } } # Example usage seating_order &lt;- list( c(1, 5, 2), c(4, 3) ) #Checking it print_classroom(seating_order) ## At row 1 column 1 : Alice ## At row 1 column 2 : Eva ## At row 1 column 3 : Bob ## At row 2 column 1 : David ## At row 2 column 2 : Cathy 5.4 Outlook This course was a short introduction to automatize work and write efficient code: Loops and Functions. Since both concepts can get really complicated really fast, and in the beginning of your R-journey you will be less likely to use loops and functions. At some point however will you be confronted with loops and functions. And the earlier you see them, the better. For further information on programming in R, I recommend “Hands-On Programming with R” by Garret Grolemund. In this book, the authors shows projects where he uses functions and loops, so it is a nice illustration of the usage of loops and functions. 5.5 Exercise Section 5.5.1 Exercise 1: Write a for loop that prints the square of each number from 1 to 10 #Assigning an object for a better workflow number &lt;- 10 #The Loop 5.5.2 Exercise 2: Do the same, but this time defining a function that squares numbers. #Defining a function for squaring sq &lt;- function (x) { } #Defining a vector containing a vector from 1 to 10 numbers &lt;- c(1:10) #Applying the number sq(numbers) 5.5.3 Exercise 3: This is the midnight formula separated in two equations: $$ x_{1,2} = $$ Make one function for the midnight formula, so the output are both. Test it with a = 2, b = -6, c = -8 Hint: You need two split up the formula into two equations with two outputs. mnf &lt;- mnf(2, 6, 8) 5.5.4 "],["data-analysis.html", "Chapter 6 Data Analysis 6.1 Introduction 6.2 Linear Regression 6.3 Model Fit 6.4 Hypothesis Testing in R 6.5 Multivariate Regression 6.6 Categorical Variables 6.7 Outlook 6.8 Exercise Section", " Chapter 6 Data Analysis 6.1 Introduction Hello, this is an introductory course to the central concepts of statistical analysis and hypothesis testing in R! The goal of this course is to provide you with the basic ideas of statistical analysis and hypothesis testing. For me, it is important that you get an intuition about what is going on rather than terrorizing you with complicated math. Although, I cannot and will not leave out central formulas, you will be fine with the maths. I wish you fun and please do not hesitate to help making that course better. pacman::p_load(&quot;tidyverse&quot;, &quot;ggpubr&quot;,&quot;gapminder&quot;, &quot;sjPlot&quot;, &quot;kableExtra&quot;, &quot;GGally&quot;, &quot;car&quot;, &quot;margins&quot;, &quot;plotly&quot;) 6.2 Linear Regression 6.2.1 Terminology The classical (bivariate) linear regression can be expressed in the following equation (systematic component): \\[ Y_i = \\beta_0 + \\beta_1X_i + e_i \\] where the index i runs over the observations (Respondents, Countries,…), i = 1,…,n \\(Y_i\\) is the dependent variable, the variable we want to explain \\(X_i\\) is the independent variable or explanatory variable. \\(\\beta_0\\) is the intercept of the regression line \\(\\beta_1\\) is the slope of the regression line \\(\\epsilon_i\\) is the error term, thus how our observed data differs from actual population data (e.g. Measurement Error). 6.2.2 Estimating the Ordinary Least Squares Estimator To get the idea of linear regression, let us look at an example. To do so, let us simulate some data and plot it.You now should have the data frame df in your environment. It contains a variable X, which is our independent variable. Y is also included, which is your dependent variable. You want to explain Y with your X. Let us plot the variables with a scatterplot: ggplot(df, aes(x, y)) + geom_point() + theme_bw() + scale_x_continuous(breaks = seq(0, 10, by = 1)) + scale_y_continuous(breaks = seq(0, 20, by = 2)) We can see that there has to be some relationship between both those variables: The higher x gets the higher y gets. Linear regression can help us investigate the relationship. We just have to take the formula and estimated \\(\\beta_0\\) and \\(\\beta_1\\) . To do so, we estimate the ordinary least square (OLS) estimator. To understand what the OLS estimator does, look at the scatter plot again: The OLS estimator fits a line through all the dots, that minimizes the distance to the dots as much as possible. Afterward, we only extract the intercept \\(\\beta_0\\) (that is the point, where the line crosses the y-axis), and \\(\\beta_1\\) (that is the slope of the line). However, since these are estimated values for our model we have to call them by convention \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) . We will denote them in the following as such. 6.2.2.1 Visualization The visual estimation is good to get an intuition with the data, but we will see later, that it does not work with multiple independent variables. Further, it does not give much information, at least not as much as we would like to have. ggplot(df, aes(x, y)) + geom_point() + theme_bw() + scale_x_continuous(breaks = seq(0, 10, by = 1)) + scale_y_continuous(breaks = seq(0, 30, by = 5)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_segment(aes(x = x, y = y, xend = x, yend = predict(lm(y ~ x, data = df))), linewidth = 0.5) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 6.2.2.2 Calculation per Hand We could also just calculate \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) by hand. The formula for both are as follows: \\[ \\hat{\\beta_1} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\] where \\(x_i\\) is the answer of respondent i for our independent variable x, \\(\\bar{x}\\) is the average answer of the respondents. Those two values are subtracted, thus the deviation from the mean is calculated. The same goes with our dependent variable y, where \\(y_i\\) is the answer of respondent i, and \\(\\bar{y}\\) is the average answer of all respondents. his is done for every respondent i, thus n - times. This is displayed with the sum symbol. We just calculated the so-called covariance. The covariance is then divided by the squared deviation to the average answer of our independent variable x. This will get us the estimated coefficient for our model \\(\\hat{\\beta_1}\\) . Sounds complicated but lets do it in R: #Let us first get the covariance cov &lt;- sum((df$x - mean(df$x)) * (df$y - mean(df$y))) #Now we get the variance of x x_sq &lt;- sum((df$x - mean(df$x))^2) x_sq ## [1] 246.1869 # We just have to divide them slope &lt;- cov/x_sq #printing it print(slope) ## [1] 1.539357 To get the intercept \\(\\hat{\\beta_0}\\) we have to take the result of the slope and implement it into this formula: \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}*\\bar{x} \\] We multiply \\(\\hat{\\beta_1}\\) with the average of our independent variable X. The result is subtracted from the average of our dependent variable y. #calculating the intercept beta_0 &lt;- mean(df$y) - (slope * mean(df$x)) #printing it beta_0 ## [1] 1.682133 Now we estimated our parameters and can display the for our model by simply putting it into the systematic component: \\[ Y_i = -0.33 + 0.68 * X_i \\] 6.2.2.3 Automated Calculation This procedure by hand is way to time-wasting. R has a built in function to calculate the parameter for us called lm() : #running a linear regression model1 &lt;- lm(y ~ x, data = df) #Printing a summary of the model results summary(model1) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1985 -1.4215 -0.4309 1.5505 5.9720 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6821 1.0376 1.621 0.116 ## x 1.5394 0.1621 9.496 2.97e-10 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.543 on 28 degrees of freedom ## Multiple R-squared: 0.7631, Adjusted R-squared: 0.7546 ## F-statistic: 90.18 on 1 and 28 DF, p-value: 2.974e-10 When we check the results, we see that we did everything right and get the exact same values. The interpretation of the coefficient is with a one-unit increase in the independent variable X, the dependent increases on average about 0.68 units, holding all else constant. But you noticed that you get more information on the model, than just the coefficients. We will get to that later. 6.2.3 Predictions with Linear Regression We could calculated predictions based on our OLS calculations, reconsider the systematic component we calculated: \\[ \\hat{y_i} = -0.33 + 0.68x_i \\] We just have to put in x-values and according to our model we would get a prediction of the respondents y-value \\(\\hat{y_i}\\). We can do that for all x-values in our dataset and get a column with all the predicted values, let us call it (y_hat) : #First, we calculate the predictions for y df$y_hat &lt;- 1.6821 + 1.5394*df$x #We could also do it automatically via the predict() function df$auto_y_hat &lt;- predict(model1) #Checking it head(df) ## x y categorical_variable y_hat ## 1 2.875775 6.680633 0 6.109068 ## 2 7.883051 12.527668 1 13.817269 ## 3 4.089769 10.029008 0 7.977891 ## 4 8.830174 17.562679 1 15.275270 ## 5 9.404673 18.312220 1 16.159653 ## 6 0.455565 3.594825 0 2.383397 ## auto_y_hat ## 1 6.108979 ## 2 13.816967 ## 3 7.977750 ## 4 15.274927 ## 5 16.159286 ## 6 2.383411 6.3 Model Fit Now the linear regression has a lot of assumptions, it is not like we can run the model every time how we want. Since it is a model, it makes assumptions and instead of just assuming them to be right, we can test them. To techniques to test them are called Measures of Fit. Because they test how much our data fits the data. Let us have a look at the assumptions and how we can test them: 6.3.1 Measures of Fit 6.3.1.1 Residuals 1.) Calculating Residuals: \\[ Residuals = y_i - \\hat{y_i} = y_i - (\\hat{\\beta_0} - \\hat{\\beta_1}x_i) \\] where \\(y_i\\) = our actual observed values of our dependent variable (df$y) \\(\\hat{y_i}\\) = are our predicted values based on our OLS estimator Reconsider the graph at 2.2.1, the residuals are basically the red lines, thus the distance from the line to the point. We can calculate the residuals for our graph: #We get the Residuals by subtracting our actual y from y_hat df$residuals &lt;- df$y - df$y_hat #cheking it head(df) ## x y categorical_variable y_hat ## 1 2.875775 6.680633 0 6.109068 ## 2 7.883051 12.527668 1 13.817269 ## 3 4.089769 10.029008 0 7.977891 ## 4 8.830174 17.562679 1 15.275270 ## 5 9.404673 18.312220 1 16.159653 ## 6 0.455565 3.594825 0 2.383397 ## auto_y_hat residuals ## 1 6.108979 0.5715646 ## 2 13.816967 -1.2896015 ## 3 7.977750 2.0511170 ## 4 15.274927 2.2874090 ## 5 16.159286 2.1525664 ## 6 2.383411 1.2114280 #We could have done that automatically with R as well ! df$residuals_auto &lt;- residuals(model1) #Checking it head(df) ## x y categorical_variable y_hat ## 1 2.875775 6.680633 0 6.109068 ## 2 7.883051 12.527668 1 13.817269 ## 3 4.089769 10.029008 0 7.977891 ## 4 8.830174 17.562679 1 15.275270 ## 5 9.404673 18.312220 1 16.159653 ## 6 0.455565 3.594825 0 2.383397 ## auto_y_hat residuals residuals_auto ## 1 6.108979 0.5715646 0.5716538 ## 2 13.816967 -1.2896015 -1.2892991 ## 3 7.977750 2.0511170 2.0512579 ## 4 15.274927 2.2874090 2.2877518 ## 5 16.159286 2.1525664 2.1529337 ## 6 2.383411 1.2114280 1.2114141 Let us have a look at a so-called Residual Plot: On the x-axis you plot the fitted values, thus our y_hat. On the y-axis you plot the residuals, thus \\(y-\\hat{y}\\). Then you plot a horizontal line at y = 0. All dots on those lines show us the values correctly predicted by our model. ggplot(df, aes(x, residuals_auto)) + geom_point() + geom_hline(yintercept = 0) + scale_y_continuous(&quot;Residuals&quot;, seq(-6, 6, 1), limits = c(-6, 6)) + scale_x_continuous(&quot;Fitted Values&quot;, seq(0, 10, 1), limits = c(0, 10)) + theme_bw() 6.3.1.2 Homoskedasticity and Heteroskedasticity One assumption of linear regression is that the variance of the error term is not correlated with our independent variable. Well, that is quite technocratic and means basically, that the residuals are distributed equally over the independent variables. Let us plot it to get a visual intuition: #setting seed for reproduciability set.seed(123) # Generate some data x &lt;- runif(150, 0.05, 1) e &lt;- rnorm(150, 0, 0.5) #homoskedastic data y_homo &lt;- 2 * x + e #heteroskedastic data y_hetero &lt;- 2 * x + e*x^2 #making a data frame with both data df_homo_hetero &lt;- data.frame(x, y_homo, y_hetero) # Scatterplot with homoscedasticity homoskedastic_plot &lt;- ggplot(df_homo_hetero, aes(x = x, y = y_homo)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) +# Add linear regression line scale_y_continuous(&quot;Y&quot;, seq(-0.5, 3.5, 0.5), limits = c(-0.5, 3.5)) + labs(title = &quot;Homoskedastic Plot&quot;) + theme_minimal() # Scatterplot with heteroscedasticity heteroskedastic_plot &lt;- ggplot(df_homo_hetero, aes(x = x, y = y_hetero)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) +#Add linear regression line labs(title = &quot;Heteroskedastic Plot&quot;) + scale_y_continuous(&quot;Y&quot;, seq(-0.5, 3.5, 0.5), limits = c(-0.5, 3.5)) + theme_minimal() # Combine plots using facet_wrap facet_plots &lt;- ggarrange(homoskedastic_plot, heteroskedastic_plot, nrow = 1) # Print the combined plots print(facet_plots) In the left plot, you see the homoskedastic data. The dots are equally and constantly distributed around the fitted line. However, the right plot shows that the more the independent variable x increases, the more the observations are increasing. The dots are not constantly distributed over the line. In the case, that the data is heteroskedastic, then this is a problem. You could try to transform the independent variable by taking the logarithm (We will look into that later). You could also use so-called heteroskedastic regression, but this an advanced model. 6.3.2 TSS, ESS and \\(R^2\\) Now that we have the Residuals, we can calculate the Total Sum of Square (TSS), the Explained Sum of Squared ESS, and \\(R^2\\): TSS (Variation in the DV): \\(TSS =\\sum(y_i - \\bar{y})^2\\) , we just subtract our actual values (df$y) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable. ESS (Variation we explain in the DV): \\(ESS = \\sum(\\hat{y_i} - \\bar{y})^2\\) , now we use our predicted values (df$y_hat) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model. \\(R^2\\) (The Variation we can predict from our model): \\(R^2 = \\frac{ESS}{TSS}\\) , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this \\(R^2\\) is 1. If our model could not explain anything the variation would be 0, since the values of both cannot be negative. Let us calculate them: #total sum of squares tss &lt;- sum((df$y - mean(df$y))^2) #explained sum of squares ess &lt;- sum((df$y_hat - mean(df$y))^2) #caculating r squared r_squared &lt;- ess/tss #Printing it r_squared ## [1] 0.7631199 #Summarizing it summary(model1)$r.squared ## [1] 0.7630777 6.3.3 Influential Outliers Outliers are extremely deviating values, which can impact our analysis and bias it. Therefore, we have to check, if our data contains such values. But first let us see how they can impact our data: #set seed set.seed(069) #generate fake data with outlier x1 &lt;- sort(runif(10, min = 30, max = 70)) y1 &lt;- rnorm(10 , mean = 200, sd = 50) y1[9] &lt;- 2000 data_outlier &lt;- data.frame(x1, y1) #Model with Outlier model_outlier &lt;- lm(y1 ~ x1) #Model without Outlier model_without_outlier &lt;- lm(y1[-9] ~ x1[-9]) #Plotting the Data # Scatter plot with points ggplot(data_outlier, aes(x = x1, y = y1)) + geom_point(shape = 20, size = 3) + # Regression line for the model with outlier geom_abline(aes(slope = model_outlier$coefficients[2], intercept = model_outlier$coefficients[1], color = &quot;Model with Outlier&quot;), linewidth = 0.75, show.legend = TRUE) + # Regression line for the model without outlier geom_abline(aes(slope = model_without_outlier$coefficients[2], intercept = model_without_outlier$coefficients[1], color = &quot;Model without Outlier&quot;), linewidth = 0.75, show.legend = TRUE) + xlab(&quot;Independent Variable&quot;) + # Adding legend theme_classic() + theme(legend.position = c(0.15,0.9), legend.title = element_blank()) We see that this one observation completely biases our sample. But how do we find out, which observation is an influential outlier? There is a metric called Cook’s Distance, we can use. Let us do it and plot it in R. #Cooks Distance can be calculated with a built-in function data_outlier$cooks_distance &lt;- cooks.distance(model_outlier) #Plotting it ggplot(data_outlier, aes(x = x1, y = cooks_distance)) + geom_point(colour = &quot;darkgreen&quot;, size = 3, alpha = 0.5) + labs(y = &quot;Cook&#39;s Distance&quot;, x = &quot;Independent Variables&quot;) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;) + theme_bw() We can clearly see that Cook’s Distance detected the outlier. The rule is that values with a cooks distance bigger than 1 have to be eliminated. You can do that with the filter() function and run the model afterward again without the outliers. 6.3.4 Functional Form Linear regression is a mathematical model. Therefore it is based on assumptions. But we should not just assume them, we should test them! One assumption is that linearity is assumed between X and Y. But that can be problematic consider following example: # estimate a simple regression model model_simple &lt;- lm(Y_quadratic ~ X_quadratic, data = df2) # Summarize it summary(model_simple) ## ## Call: ## lm(formula = Y_quadratic ~ X_quadratic, data = df2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.667 -5.109 -1.743 3.449 17.188 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.6588 0.9420 7.069 5.77e-09 *** ## X_quadratic 2.1226 0.3656 5.805 4.96e-07 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.552 on 48 degrees of freedom ## Multiple R-squared: 0.4125, Adjusted R-squared: 0.4003 ## F-statistic: 33.7 on 1 and 48 DF, p-value: 4.965e-07 #Plot it ggplot(df2, aes(x = X_quadratic, y = Y_quadratic)) + geom_point(shape = 20, size = 3) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; As you can see something look wrong. There seems to be a correlation between the two variables, but it does not seem linear. In such a case it does make sense to square the independent variable and run the regression again: # estimate a simple regression model model_quadratic &lt;- lm(Y_quadratic ~ X_quadratic^2, data = df2) #Summarize it summary(model_quadratic) ## ## Call: ## lm(formula = Y_quadratic ~ X_quadratic^2, data = df2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.667 -5.109 -1.743 3.449 17.188 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.6588 0.9420 7.069 5.77e-09 *** ## X_quadratic 2.1226 0.3656 5.805 4.96e-07 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.552 on 48 degrees of freedom ## Multiple R-squared: 0.4125, Adjusted R-squared: 0.4003 ## F-statistic: 33.7 on 1 and 48 DF, p-value: 4.965e-07 #Plot it ggplot(df2, aes(x = X_quadratic, y = Y_quadratic)) + geom_point(shape = 20, size = 3) + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2), color = &quot;red&quot;, se = FALSE,) + scale_x_continuous(&quot;X&quot;, breaks = seq(-5,5,1), limits = c(-5,5)) + ylab(&quot;Y&quot;) + theme_bw() Well that looks better and is the proper way to deal with quadratic relationships in linear regression. Well, data can take on not only a quadratic form, it could also take on a form of a square-root function. I will show the most classical example of such a functional form. The gapminder data is loaded. It contains data about the average life expectancy (lifeExp)and the GDP per capita (gdpPercap) of countries in different years. Let us look if the GDP per Capita is correlated with Life Expectancy: #checking the data head(gapminder) ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8.43e6 779. ## 2 Afghanistan Asia 1957 30.3 9.24e6 821. ## 3 Afghanistan Asia 1962 32.0 1.03e7 853. ## 4 Afghanistan Asia 1967 34.0 1.15e7 836. ## 5 Afghanistan Asia 1972 36.1 1.31e7 740. ## 6 Afghanistan Asia 1977 38.4 1.49e7 786. #Plotting it ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + scale_y_continuous(&quot;Life Expectancy&quot;, seq(30, 80, 10), limits = c(30, 80)) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Well, that looks terrible. What can we do? I already mentioned that the fitted line looks like a square-root function (\\(y = \\beta{\\sqrt{x}}\\) ). When you take the logarithm of square root, you neutralize the square root and only x remains \\(\\log{(\\sqrt{x})} = x\\). When you do that the functional form changes to \\(y = \\beta{x}\\). Well, that is exactly the systematic component we are after: ggplot(gapminder, aes(log(gdpPercap), lifeExp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_y_continuous(&quot;Life Expectancy&quot;, seq(30, 80, 10), limits = c(30, 80)) + xlab(&quot;GDP per Capita&quot;) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; This looks more like what we want to achieve. What can we see in that plot? When we run a model we want to take the logarithm of the independent variable, when we expect the following: If the most observations of a variable are low, but some observations are extremely high such functional forms can occur. In our example, most of the countries have a low GDP per Capita, but some countries such as the Western European countries or the USA have such a a high level of GDP per Capita, they change the functional form of the fitted line. These are influential outliers, but too much to delete them. It could bias the representativeness of the sample, therefore we can deal with them by taking the logarithm. 6.3.5 Independent Observation Another assumption of the linear regression model is the independent, identically distributed (i.i.d) assumption. That sounds complicated but it really is not. Consider following plot: ggplot(df_time_series, aes(date, y_time)) + geom_line() + ylab(&quot;Y&quot;) + xlab(&quot;Year&quot;) + theme_bw() If you look at the plot, can we assume that the observation Year = 2000 is independent from the Years before? No, the observation in the years are correlated to each other, thus the assumption is violated. This is basically the huge problem of working with longitudinal data (time-series cross-sectional or panel). If you face such problems there are plenty of other methods to use: Interrupted time series, Difference-in-Difference Designs, Panel-Matching, Fixed-Effects Models etc. 6.4 Hypothesis Testing in R We are not only interested if our model fits or not. We want to investigate real world phenomena. Well, and if our model does not fit well, we have to make adjustments so it does. What comes next is that we want to know more about the world. Researcher do so by formulating hypotheses. These are nothing more than assumptions you make theoretically about the world. Let us say you think you assume that in our world education has an impact on income. This would be your Alternative Hypothesis \\(H_A\\) . What you know want to do is to test it against the so-called Null Hypothesis \\(H_0\\). That is nothing more than the opposite of our alternative hypothesis, thus that education has no impact on income. Let us formulate both to have an overview: \\(H_0\\) = Education does not impact Income. \\(H_A\\) = The more educated a person is, the higher the income. The advantage of this approach is obvious, one of those will be true. Therefore statistical testing is needed. But before introducing it to you, we have to decide how we want to test our alternative hypothesis. More specifically, how do we want to measure our variables. For our example, we decide to conduct a survey and to get data by asking a random sample of 1000 people over 18 living in Germany (N=1000). We decide to measure our independent variable Education, by asking the respondents about the years they invested in their education. To get their income, you just ask them about it to fill it in. Can you think about possible critics about our data collection strategy? 6.4.1 Standard Error 6.4.1.1 Root Mean Square Error (RMSE) Before moving on to Standard Errors, I will introduce another metric, the root mean square error. It is the average difference between the actual values \\(y_i\\) and our predicted values \\(\\hat{y_i}\\). To calculate it, we square the residuals, to get only positive values. Then we take the mean, and lastly we take the square root. #Getting the the sum of squared residuals (SSR) SSR &lt;- df$y_hat^2 #Calculating the mean of the squared residuals mean_SSR &lt;- mean(SSR) #Calculatin the RSME rsme &lt;- sqrt(mean_SSR) #Printing it print(rsme) ## [1] 11.38257 The rule of thumb is that the lower the RSME, the better. It is a non-standardized goodness of fit measure. Its counterpart is the R-squared measure, which is a standardized measure. You can use both, and should use both to get a metric about how close the predicted values are distributed around the actual values. 6.4.1.2 Standard Error of the Estimate The standard error of a coefficient estimate is the metric directly presented next to the coefficient in the regression output. It is the square root of the variance of the regression coefficient. #calculating standard error by hand se &lt;- SSR/(nrow(df) - 2 * (sum(df$x - mean(x)))) #Printing it print(se) ## [1] -0.13248275 -0.67772549 -0.22593606 -0.82829905 ## [5] -0.92698661 -0.02016518 -0.34174553 -0.84406921 ## [9] -0.36722086 -0.26938168 -0.95611653 -0.26626739 ## [13] -0.52081770 -0.39116308 -0.03787745 -0.85659649 ## [17] -0.10622910 -0.01926455 -0.16078810 -0.95194240 ## [21] -0.83922300 -0.54117834 -0.47290747 -1.02444396 ## [25] -0.49227475 -0.56260928 -0.35907515 -0.41622841 ## [29] -0.13354127 -0.05529584 6.4.2 T-Value or T-Statistic The formula for calculating the t-value is simple: \\[ t_i = \\frac{\\beta_i}{SE(\\beta_i)} \\] where \\(\\beta_i\\) is the coefficient calculated by the linear regression, and \\(SE(\\beta_i)\\) is the standard error. Let us calculate it manually by hand for our example: #t value by hand t_value_intercept &lt;- -0.32773/0.30271 t_value_x &lt;- 0.67949/0.04813 #printing it print(t_value_intercept) #-1.082653 ## [1] -1.082653 print(t_value_x) #-14.11781 ## [1] 14.11781 Well, that is the t-value on its own does not tell us about statistical significance. It is used to calculate the p-value, which tells us about the significance of the value. But before we calculate it, we have to understands some key concepts before. Degrees of Freedom is the first concept. Let us say, we have a sample of me and my sister (N = 2). We collected the numbers of books each of us has, and calculated a mean of 30. I have 20 books. How many books does my sister have? Obviously, she has 40, if we have a mean of 30. Given my value, and the mean of the sample, the value of my sister had to be 20. That is what the degrees of freedom tells us. Given the mean of a sample, the values, which can freely be chosen. Or to put it more technically, the maximum number of logically independent values. The rule is that the larger the sample, the higher the degrees of freedom and the lower the sample, the lower the degrees of freedom. T-distributions are the distribution, we will use to calculate the p-value. I will talk about that in detail in a minute. But they are connected to the degrees of freedom, because degrees of freedom determine the tail behavior and shape of the curve until the point, where the t-distribution looks like a standard normal distribution. Let us visualize that: # Generate data x &lt;- seq(-5, 5, length.out = 100) # Calculate densities densities &lt;- data.frame( x = rep(x, 4), density = c(dt(x, df = 1), dt(x, df = 2), dt(x, df = 10), dnorm(x, mean = 0, sd = 1)), distribution = rep(c(&quot;t(df=1)&quot;, &quot;t(df=2)&quot;, &quot;t(df=10)&quot;, &quot;Normal&quot;), each = 100) ) densities$distribution &lt;- factor(densities$distribution, levels = c(&quot;Normal&quot;, &quot;t(df=10)&quot;, &quot;t(df=2)&quot;, &quot;t(df=1)&quot;)) # Plot plotly::ggplotly(ggplot(densities, aes(x = x, y = density, color = distribution)) + geom_line() + theme_minimal() + labs(x = &quot;x&quot;, y = &quot;Density&quot;, title = &quot;t-distributions with different degrees of freedom&quot;) + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)) + scale_x_continuous(&quot;X&quot;, seq(-5,5,1), limits = c(-5,5))) Alright, before we find out, how to determine if a value allows us to reject the null hypothesis, we have to determine a so-called critical value. This is no math or anything, it is a probability we decide about. More precisely, the probability that our t-value and thus our coefficient occured by chance alone. If we say the probability that it occured by chance alone is 10%, then this is our critical value. The rule of thumb is, that a probability lower than 5% that the coefficient occured by chance alone indicates statistical significance. Keep in mind, that the critical value can vary depending on the sample size, the degrees of freedom, the field you are working in etc. #setting seed set.seed(42) # Generate data x &lt;- seq(-5, 5, length.out = 100) t_density &lt;- function(x) dt(x, df = 28) # Calculate densities t_value_data &lt;- data.frame( x = rep(x, 1), density = dt(x, df = 28), distribution = rep(&quot;t(df=28)&quot;, 100) ) # Plot plotly:: ggplotly(ggplot(t_value_data, aes(x = x, y = density)) + geom_line(lineend = &quot;round&quot;) + stat_function(fun = t_density, geom = &quot;area&quot;, fill = &quot;gray&quot;, alpha = 0.75, xlim = c(-5, -1.701), n = 10000) + stat_function(fun = t_density, geom = &quot;area&quot;, fill = &quot;gray&quot;, alpha = 0.75, xlim = c(5, 1.701), n = 10000) + geom_vline(xintercept = -1.701, linetype = &quot;dashed&quot;, colour = &quot;red&quot;) + geom_vline(xintercept = 1.701, linetype = &quot;dashed&quot;, colour = &quot;red&quot;) + ggtitle(&quot;t-distribution with 28 df&quot;, subtitle = &quot;The pink area marks the interval of significant values on a 95% level&quot;) + geom_segment(x = -1.082653, xend = -1.082653, yend = dt(-1.082653, df = 28), y = -1, color = &quot;pink&quot;, linetype = &quot;dashed&quot;, linewidth = 0.2) + annotate(&quot;point&quot;, x = -1.082653, y = dt(-1.082653, df = 28), color = &quot;pink&quot;) + scale_x_continuous(&quot;X&quot;, seq(-5,5,1), limits = c(-5,5)) + theme_classic() + theme(legend.position = &quot;none&quot;) ) As we can see, the blue line representing the value of the intercept is not in the area it would have to be, for us to reject the null hypothesis. However, the t-value of our coefficient from variable is with about 14 far away from the threshold, so we can reject the null hypothesis. This is how the t-value works. The problem is, that the threshold varies with the degrees of freedom, and you will not have the threshold value in your head for every degree of freedom. You could look it up every time or you use p-values, which directly tell you the probability of the coefficient occuring by chance alone. 6.4.3 p-values The p-value is a statistics that shows us the probability that a statistical measure (in our example 0) is greater/less than an observed value (in our example, the estimated coefficient). The null hypothesis would tell us that our independent variable X has no impact on Y. Statistically speaking, that would mean that our coefficient has a high probability to be zero, because zero means no effect, thus we cannot reject the null hypothesis. But if the probability that our estimated coefficient is 0 is low, we may reject the null hypothesis and would found an effect. Before showing you two ways to find the p-value, we have to determine a critical value. This is no math or anything, it is a probability we decide about. The rule of thumb is that if the p-value is smaller than 5%, we can reject the null hypothesis. However, depending on various factors, it could also be different, that depends on your data, sample size, degrees of freedom etc. For our example, we follow the rule of thumb and set the significance level to 0.05, thus if the coefficient has a smaller probability (p-value &lt; 0.05) than 5% to be zero we can reject the null hypothesis, otherwise we cannot reject it. Since the math is complicated and thus not help to understand the intuition, I directly show you how to calculate the p-value by hand: #calculating the p values by hand p_value_1 &lt;- 2 * pt(-abs(t_value_intercept), 28) p_value_2 &lt;- 2 * pt(-abs(t_value_x), 28) #printing it print(p_value_1) ## [1] 0.2881981 print(p_value_2) ## [1] 2.941061e-14 We get the same values as in our regression, and we decided before that our critical value (denoted as \\(\\alpha\\)) should be less than 0.05 to reject the null hypothesis. Therefore we can reject the null hypothesis for the coefficient of our variable x. Since the coefficient is positive, the interpretation would be that x has on average a positive effect on y, since the probability that the value is different than 0 is lower than the critical value of 5%. 6.4.4 Confidence Interval The last way of testing hypothesis are confidence intervals. I recommend to use them, since they are intuitive and easier to interpret than p-values. To understand confidence intervals, we must remember the difference between a population and a sample. The population are all people we want to infer to, for example in an election, the population are all citizens eligible to vote in that election. Let us assume, all else equal, that the the vote share in the population for Party A is 24%. This is what we call the true population parameter. And our goal is to estimate this parameter with statistical methods, since it is more efficient. Think about Germany, we cannot ask 80 million people before the election to give us their thoughts, so what we do instead, is to draw a representative sample of less citizens from that sample to infer to the population. The problem is that even if the sample is representative, it could be that our sample today gives us a vote share of Party A of 23%, but tomorrow we would get 25% (This could have several reasons, can you think of some?). In the following, let us assume we draw 1000 samples before the election, to get the vote share of Party A. We know that our true population parameter is 24%. But before we start, we have to set a rule again. This rule is basically the definition of confidence intervals: In 95% of all samples, that could be drawn, the confidence intervals will cover the true population parameter. So if we draw 1000 samples, in 950 the confidence intervals have to cover the true population parameter: #Since this is a simulation we need to set a seed set.seed(187) #We will need to have vectors for the upper confidence interval and the lower one lower_ci &lt;- numeric(100) upper_ci &lt;- numeric(100) estimates &lt;- numeric(100) #This loop represents for(i in 1:length(lower_ci)) { Y &lt;- rnorm(100, mean = 24, sd = 2) estimates[i] &lt;- Y[i] lower_ci[i] &lt;- Y[i] - 1.96 * 24 / 10 upper_ci[i] &lt;- Y[i] + 1.96 * 24 / 10 } #Let us bind both vectors together CIs &lt;- data.frame(estimates, lower_ci, upper_ci) #Print it head(CIs) ## estimates lower_ci upper_ci ## 1 22.86723 18.16323 27.57123 ## 2 25.36001 20.65601 30.06401 ## 3 26.32276 21.61876 31.02676 ## 4 23.37235 18.66835 28.07635 ## 5 22.71026 18.00626 27.41426 ## 6 21.89276 17.18876 26.59676 Now we have drawn our 1000 samples and computed our estimates as well as their corresponding intervals. Thus, 1000 samples about the vote share of Party A. Let us check, if the true population parameter is 95% of times within our computed confidence intervals: #Getting the true mean true_mean &lt;- 24 #First, we identify those who are not including 24 our true population parameter CIs$missed &lt;- ifelse(CIs$lower_ci &gt; true_mean | CIs$upper_ci &lt; true_mean, &quot;Out&quot;, &quot;In&quot;) #Let us give every sample an identification number CIs$id &lt;- 1:nrow(CIs) #Plotting it ggplot(data = CIs) + geom_pointrange( aes( x = estimates, # point value xmin = lower_ci, # lower CI xmax = upper_ci, # upper CI y = id, # y axis - just observation number color = missed ) # color varies by missed variable ) + geom_vline( aes(xintercept = true_mean), # add vertical line at true_mean ) + scale_color_manual(values = c(&quot;azure4&quot;, &quot;red&quot;)) + theme_minimal() + labs( title = &quot;Confidence Interval for Mean&quot;, subtitle = &quot;Population mean equals 24&quot;, x = &quot;Estimates&quot;, y = &quot;Sample&quot;, color = &quot;Is true population parameter inside the CI?&quot; ) + theme(legend.position = &quot;top&quot;) + # switch the legend to the top scale_x_continuous(breaks = c(seq(15, 30, by = 1))) Well, we can see that the majority of the computed intervals include the true population parameter. But there are three, which are not. That is within our definition. What is now with confidence intervals for coefficients of linear regression? Well, it is the same story, we compute an estimate, in this case a coefficient. The true population parameter is unknown, but we know that the true population parameter is within 95% of the intervals. Its calculation is fairly easy and you already saw it: \\[ CI_{lower} = \\beta_i - 1.96 * SE(\\beta_i) \\\\\\ CI_{upper} = \\beta_i + 1.96 * SE(\\beta_i) \\] #Let us look at the confindence intervals of model 1 confint(model1) ## 2.5 % 97.5 % ## (Intercept) -0.4432173 3.807484 ## x 1.2073136 1.871401 #Let us compute the confidence values by hand for the intercept and x ci_lower_int &lt;- model1$coefficients[1] - 1.96 * summary(model1)$coef[, &quot;Std. Error&quot;][1] ci_upper_int &lt;- model1$coefficients[1] + 1.96 * summary(model1)$coef[, &quot;Std. Error&quot;][1] #Print it print(ci_lower_int) ## (Intercept) ## -0.3514894 print(ci_upper_int) ## (Intercept) ## 3.715756 #Estimate X ci_lower_est &lt;- model1$coefficients[2] - 1.96 * summary(model1)$coef[, &quot;Std. Error&quot;][2] ci_upper_est &lt;- model1$coefficients[2] + 1.96 * summary(model1)$coef[, &quot;Std. Error&quot;][2] #Print it print(ci_lower_est) ## x ## 1.221644 print(ci_upper_est) ## x ## 1.857071 6.5 Multivariate Regression The world is a complex place and of course if we have a dependent variable Y, let us say for example income, we cannot explain it exclusively by the years of education or exclusively by the profession or any other single factor. Rather it is plausible that all these factors matter. The Multivariate Linear Regression Model allows us that we explain the variation in our dependent variable Y with multiple independent variables X. Mathematically, the systematic component changes like this: \\[ Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... + \\beta_kX_{ki} + \\epsilon_i \\] where the index i runs over the observations (Respondents, Countries,…), i = 1,…,n \\(Y_i\\) is the dependent variable, the variable we want to explain \\(X_{1i}\\) is the first independent variable or explanatory variable. \\(X_{2i}\\) is the second independent variable or explanatory variable. \\(X_{ki}\\) is the k-th independent variable or explanatory variable, where k is the number of all our independent variables in our model. \\(\\beta_0\\) is the intercept of the regression line \\(\\beta_1\\) is the slope of the regression line of the first explanatory variable. \\(\\beta_2\\) is the slope of the regression line of the second explanatory variable. \\(\\beta_k\\) is the slope of the regression line of the k-th explanatory variable, where k is the number of all our independent variables in our model. \\(\\epsilon_i\\) is the error term, thus how our observed data differs from actual population data (e.g. Measurement Error). In R, we can implement a multivariate model very easy with the already known lm() function. Let us run a multiple regression with our independent Variable X and our categorical Variable Z, the systematic component for this model looks like this: \\[ Y_i = \\beta_0 + \\beta_1X_i + \\beta_2Z_i + \\epsilon_i \\] Let us compute the slopes: lm(y ~ x + categorical_variable, data = df) %&gt;% summary() ## ## Call: ## lm(formula = y ~ x + categorical_variable, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5183 -1.3840 -0.5014 1.2393 5.5808 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 1.9799 1.0679 1.854 ## x 1.5557 0.1621 9.596 ## categorical_variable1 -1.0667 0.9637 -1.107 ## Pr(&gt;|t|) ## (Intercept) 0.0747 . ## x 3.42e-10 *** ## categorical_variable1 0.2781 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.533 on 27 degrees of freedom ## Multiple R-squared: 0.7734, Adjusted R-squared: 0.7566 ## F-statistic: 46.07 on 2 and 27 DF, p-value: 1.982e-09 We can see, that this works fine, we only had to add the independent variable in the command such as in our systematic component. Regarding the interpretation it is analogous to the normal linear regression one. For a one-unit increase in our independent variable x, the dependent variable y increases 0.67 units on average, holding all else constant. For category 1 in comparison to category 0, the dependent variable y increases 0.52 units on average, holding all else equal. Now, we can put the calculated coefficients into our systematic component and make predictions: \\[ Y_i = -0.54 + 0.67X_i + 0.52Z_i + \\epsilon_i \\] If we want to visualize this, we would need a three-dimensional coordinate system. Why? Because every independent variable is one-dimension if you want. I said that k is the number of our independent variables, technically it is the number of dimensions our model has. I am not a fan of plotting graphs more than three dimensions and I do not recommend it to you either. 6.5.1 Model Fit: Adjusted R-squared The last important aspect of Multivariate Regression is the Adjusted R-squared measure. Reconsider, the calculation of the classical R-squared: TSS (Variation in the DV): \\(TSS =\\sum(y_i - \\bar{y})^2\\) , we just subtract our actual values (df$y) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable. ESS (Variation we explain in the DV): \\(ESS = \\sum(\\hat{y_i} - \\bar{y})^2\\) , now we use our predicted values (df$y_hat) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model. \\(R^2\\) (The Variation we can predict from our model): \\(R^2 = \\frac{ESS}{TSS}\\) , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this \\(R^2\\) is 1. The problem with the classical R-squared is, that if you would add useless independent variables to it, the classical R-squared would decrease, although your model did not increase in explanatory power. This is called overfitting. However, adjusted R-squared will account for that problem by introducing a “penalty” for every additional variable. Mathematically, it looks like this: \\[ Adj.R^2 = 1 - \\frac{(1 - R^2)*(N - 1)}{N - k - 1} \\] where \\(R^2\\) is our classical R-squared calculated (\\(\\frac{TSS}{ESS}\\)) \\(N\\) is the number of observations in our sample \\(k\\) is the number of independent variables In R, we can extract the adjusted R-squared simply from our model in chunk, multivariate regression: #running multivariate model multivariate_model &lt;- lm(y ~ x + categorical_variable, data = df) #Getting summary summary(multivariate_model) ## ## Call: ## lm(formula = y ~ x + categorical_variable, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5183 -1.3840 -0.5014 1.2393 5.5808 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 1.9799 1.0679 1.854 ## x 1.5557 0.1621 9.596 ## categorical_variable1 -1.0667 0.9637 -1.107 ## Pr(&gt;|t|) ## (Intercept) 0.0747 . ## x 3.42e-10 *** ## categorical_variable1 0.2781 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.533 on 27 degrees of freedom ## Multiple R-squared: 0.7734, Adjusted R-squared: 0.7566 ## F-statistic: 46.07 on 2 and 27 DF, p-value: 1.982e-09 #Extract Adjusted R-squared summary(multivariate_model)$adj.r.squared ## [1] 0.7565721 #Calculating by hand adj_r_squared &lt;- 1 - (((1-summary(multivariate_model)$r.squared) * (nrow(df) - 1))/(nrow(df) - 2 - 1)) #printing it print(adj_r_squared) ## [1] 0.7565721 6.5.2 Omitted Variable Bias I already mentioned one (abstract) reason why we should include other variables in our model. But there is more to it: You could find effects between two variables X and Y, but it could be that in Reality there is not an association. For example, let us say you collect data about ice cream and shark attacks. Ice cream sales is your independent variable and you want to explain the number of shark attacks, here is your data: # Set seed for reproducibility set.seed(0) # Number of data points n &lt;- 100 # Simulate diet data (assuming a normal distribution) temperature &lt;- rnorm(n, mean = 1500, sd = 200) # Simulate exercise data (assuming a normal distribution) ice_cream_sales &lt;- rnorm(n, mean = 3, sd = 1) # Simulate weight loss data violence_crime_true &lt;- 0.2 * temperature - 0.5 * ice_cream_sales + rnorm(n, mean = 0, sd = 5) # Create a data frame data &lt;- data.frame(temperature = temperature,ice_cream_sales = ice_cream_sales, violence_crime_true = violence_crime_true) # Fit a model without including the diet variable model_without_temperature &lt;- lm(violence_crime_true ~ ice_cream_sales, data = data) #Fit a model with only the temperature variable model_with_only_temperature &lt;- lm(violence_crime_true ~temperature, data = data) # Fit a model including both diet and exercise variables model_with_temperature &lt;- lm(violence_crime_true ~ ice_cream_sales + temperature, data = data) # Output the summary of both models summary(model_without_temperature) ## ## Call: ## lm(formula = violence_crime_true ~ ice_cream_sales, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -94.788 -23.736 -2.348 22.264 98.754 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 287.681 11.623 24.751 &lt;2e-16 ## ice_cream_sales 4.090 3.741 1.093 0.277 ## ## (Intercept) *** ## ice_cream_sales ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.94 on 98 degrees of freedom ## Multiple R-squared: 0.01205, Adjusted R-squared: 0.001969 ## F-statistic: 1.195 on 1 and 98 DF, p-value: 0.2769 summary(model_with_only_temperature) ## ## Call: ## lm(formula = violence_crime_true ~ temperature, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.9969 -3.8454 0.1718 2.9121 11.7502 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.535495 4.576293 -0.773 0.442 ## temperature 0.201591 0.003021 66.727 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.307 on 98 degrees of freedom ## Multiple R-squared: 0.9785, Adjusted R-squared: 0.9782 ## F-statistic: 4452 on 1 and 98 DF, p-value: &lt; 2.2e-16 summary(model_with_temperature) ## ## Call: ## lm(formula = violence_crime_true ~ ice_cream_sales + temperature, ## data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.4770 -3.6734 -0.0914 2.9449 12.1843 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.396657 4.694786 -0.510 0.611 ## ice_cream_sales -0.596143 0.556470 -1.071 0.287 ## temperature 0.202005 0.003043 66.373 &lt;2e-16 ## ## (Intercept) ## ice_cream_sales ## temperature *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.303 on 97 degrees of freedom ## Multiple R-squared: 0.9787, Adjusted R-squared: 0.9783 ## F-statistic: 2230 on 2 and 97 DF, p-value: &lt; 2.2e-16 #Let us display both models next to each other #EDIT: I created this function specifically, the code for the function is at the top. table_ovb(model_without_temperature, model_with_temperature) Model without Temperature Model with Temperature Intercept 287.680876 -2.3966572 Ice Cream Sales 4.090356 -0.5961434 Temperature NA 0.2020051 We can see that the coefficient changes dramatically. What happened? Well, one important assumption of linear regression is that the error term captures all variance not explained by our model and is not correlated with the independent variable(s) nor the dependent variable. But if there is unexplained variation in our model that is correlated with our independent variable, then this assumption is violated. In our example, we can see that ice cream sales coefficient in the first model is biased, because ice cream sales and is correlated to temperature. The warmer it gets, the more ice cream is sold. But, the warmer it gets, the more violent people get, therefore we have an omitted variable and that is temperature. When we include temperature in the model, we see the problem of omitted variable bias: It biases our coefficients, by either overestimating (like in our example) or by underestimating it. What we should do in such a case, is to delete the omitted variable, which is the drastically changing variable (ice cream sales in our case). This is also the reason, why people talk about additional variables as control variables in a multiple linear model. This way you can control if an association between two variables is due to omitted variable bias or other variables, which can explain the variation better. 6.5.3 Multicollinearity Another, and I promise, the last OLS assumption, which has to tested is that there is no Multicollinearity. The concept is simple: The independent variables should not be correlated. In our previous example, ice cream sales and temperature were correlated. This would have hurt these assumption. In strong cases, multicollinearity can bias our estimates, so that they gain statistical significance and lead us to wrong conclusions. Let us look at an obvious example. You want to find out how the grades of children is affected by different factors. You choose 2 factors: The time they spent on doing their homework (learning time) and the time they spent on playing video games (gaming time). The systematic component looks like this: \\[ Grades_i = \\beta_0 + \\beta_1*\\text{learning time}_i + \\beta_2 *\\text{gaming time}_i + \\epsilon_i \\] Let us compute them: # Set seed for reproducibility set.seed(42) # Number of samples n &lt;- 100 # True coefficients beta_0 &lt;- 80 beta_1 &lt;- 1.5 beta_2 &lt;- 1.5 # Generate independent variables learning_time &lt;- runif(n, 1, 10) gaming_time &lt;- 0.7 * learning_time + sqrt(1 - 0.7^2) * rnorm(n, sd = 1) #generate error term epsilon &lt;- rnorm(n, 0, 3) # Generate grades grades &lt;- beta_0 + beta_1 * learning_time + beta_2 * gaming_time + epsilon # Create a data frame df_grades &lt;- data.frame(learning_time, gaming_time, grades) # Display first few rows of the data frame grades_model &lt;- lm(grades ~ learning_time + gaming_time, data = df_grades) #Getting the summary summary(grades_model) ## ## Call: ## lm(formula = grades ~ learning_time + gaming_time, data = df_grades) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2868 -1.9842 -0.0991 1.9482 6.2446 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 80.1718 0.6668 120.231 &lt; 2e-16 ## learning_time 1.2665 0.3335 3.798 0.000255 ## gaming_time 1.7860 0.4312 4.142 7.36e-05 ## ## (Intercept) *** ## learning_time *** ## gaming_time *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.822 on 97 degrees of freedom ## Multiple R-squared: 0.8661, Adjusted R-squared: 0.8634 ## F-statistic: 313.8 on 2 and 97 DF, p-value: &lt; 2.2e-16 By looking at the model, we could conclude that the more a student learns, the better its grades on average, holding all else constant. So far, so clear, but the same goes for gaming time. Why is that the case? Because if we think that a student has per day 3 hours, which the student can assign to either learning or gaming, than both are correlated, because assigning 2 hours to learning means 1 hour for gaming, 0.5 hours for learning means 2.5 hours for gaming and so forth. This means both coefficients are explaining each other and bias each other. How to detect them? 6.5.3.1 Testing Correlations to each other The first technique is to check the correlations of the variables to each other beforehand. You can do that two-ways: Just print out a correlation table: #First store the variables you need in a seperate data frame cormatrix_data &lt;- df_grades %&gt;% select(learning_time, gaming_time) #Second, calculate the table, the 2 at the end are the dimensions cormatrix &lt;- cor(cormatrix_data) #Calculate the correlations round(cormatrix, 2) #round it to the second digit and display it ## learning_time gaming_time ## learning_time 1.00 0.95 ## gaming_time 0.95 1.00 #We could have also done this code in one step #df_grades %&gt;% # select(learning_time, gaming_time) %&gt;% # cor() %&gt;% # round(2) We can see that the correlation between both variables is way to high. Now, with only two variables the table works fine, but what if we have, let us say, 20 variables? It could get messy, therefore you could also use the correlation matrix, which I introduced in the chapter before. In this case, it does not make sense, since it would just print out one block. But keep it nevertheless in mind for the future. 6.5.3.2 Variance of Inflation (VIF) Another measure for multicolinearity and probably the most famous one, is the Variance-of-Inflation (VIF) factor. Intuitively spoken, this measure fits models with multiple variables, by calculating the variance of each variable. Then it fits a model with only one independent variable and calculates the variance of it. The result is a measure that displays high values if the variance of a variable increases, when other variables are added. That is exactly what this measure does.. The formula of it is really simple: \\[ VIF_i = \\frac{1}{1 - R^2_i} \\] where, the Variance of inflation (VIF) of variable i is calculated by 1 divided by 1 - the R-squared (\\(R^2_i\\)) of the regression with only that variable. In R, we can use the VIF() function from the car - package to do it. #We only have to use the function VIF() on our model vif(grades_model) ## learning_time gaming_time ## 10.21358 10.21358 The rule is that if a value exceeds 10, it is considered critical. We should always test for multicolinearity, and if we detect it, run the regression separately without the correlated variables. 6.6 Categorical Variables As I mentioned in the first chapter, there are different types of variables, let us reconsider them: Numeric Numbers c(1, 2.4, 3.14, 4) Character Text c(\"1\", \"blue\", \"fun\", \"monster\") Logical True or false c(TRUE, FALSE, TRUE, FALSE) Factor Category c(\"Strongly disagree\", \"Agree\", \"Neutral\") Dependent variables must be numeric, when conducting linear regression. However, independent variables can take be scaled differently. Numeric variables are the easiest case, the interpretation is as mentioned in the previous examples. But categorical variables are differently to interpret. Let us inspect the categorical variable in our dataset, called categorical_variable: table(df$categorical_variable) ## ## 0 1 ## 19 11 As we can see our data set now contains a categorical variable with two categories, named “A” and “B”. Those categories could be anything: Female and Male, bought a product or did not buy a product, vaccine or placebo and so on. What happens, when we now run a model? #running a model with a categorical variable model2 &lt;- lm(y ~ categorical_variable, data = df) #Getting the summary summary(model2) ## ## Call: ## lm(formula = y ~ categorical_variable, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2770 -3.7683 0.1863 4.0996 10.2377 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 10.5765 1.1985 8.825 ## categorical_variable1 -0.2265 1.9792 -0.114 ## Pr(&gt;|t|) ## (Intercept) 1.41e-09 *** ## categorical_variable1 0.91 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.224 on 28 degrees of freedom ## Multiple R-squared: 0.0004673, Adjusted R-squared: -0.03523 ## F-statistic: 0.01309 on 1 and 28 DF, p-value: 0.9097 We see a coefficient of 0.64 that is fine. But as you see the category “category_A” is not display, why? Categorical variables are calculated based on so-called “reference categories”. R determines the reference category based on alphabetical order. In this case the category “A” is the reference category. The reference category is named like this, since the computed coefficients refer to it. The reference category “A” takes on the value 0. The coefficient of the category B is “0.92”. That means that category “B” if the a respondent is part of category “B” than the dependent variable Y increases on average 0.64 units in comparison to category “A”, holding all else equal. That sounds technocratic, let us get an intuition. We add a zero to our code and look at the output: #running the model model3 &lt;- lm(y ~ categorical_variable + 0, data = df) #getting a summary summary(model3) ## ## Call: ## lm(formula = y ~ categorical_variable + 0, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2770 -3.7683 0.1863 4.0996 10.2377 ## ## Coefficients: ## Estimate Std. Error t value ## categorical_variable0 10.576 1.198 8.825 ## categorical_variable1 10.350 1.575 6.571 ## Pr(&gt;|t|) ## categorical_variable0 1.41e-09 *** ## categorical_variable1 3.99e-07 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.224 on 28 degrees of freedom ## Multiple R-squared: 0.8122, Adjusted R-squared: 0.7987 ## F-statistic: 60.53 on 2 and 28 DF, p-value: 6.812e-11 The coefficients changed, but not really. Let us subtract the coefficient from category A from category B: #results result &lt;- coefficients(model3)[2] - coefficients(model3)[1] #coefficents can be extracted this way #printing it result ## categorical_variable1 ## -0.2264615 We get the same coefficient as above and that is what R does automatically when computing coefficients of categorical variables. The reference category is scaled to 0 and the other categories are following. 6.6.1 Interaction Effects One technique, which is important and widely used in statistics are interaction effects. To keep things simple concentrate on our independent variable X and our categorical variable Z. As mentioned, Z shows us the coefficients for our categories “A” and “B”. But what if we could further investigate the dynamics of the group? Interaction effects allow us to multiply our variable X by our categorical variable Z. Mathematically our systematic component looks like this: \\[ Y_i = \\beta_0 + \\beta_1X_i + \\beta_2Z_i + \\beta_3X_i*Z_i + e_i \\] where the index i runs over the observations (Respondents, Countries,…), i = 1,…,n \\(Y_i\\) is the dependent variable, the variable we want to explain \\(X_i\\) is the independent variable or explanatory variable. \\(Z_i\\) is our categorical variable \\(\\beta_0\\) is the intercept of the regression line \\(\\beta_1\\) is the slope of the regression line \\(\\epsilon_i\\) is the error term, thus how our observed data differs from actual population data (e.g. Measurement Error). In the following, I will show you an example how to use interaction effects. Let’s say that I conducted a survey among 1000 respondents and asked them about how many hours they spent on R online courses such as this one. Then I asked about their coding ability. Lastly, I asked if they learned with this course or with other courses (0 = other courses, 1 = this course). Now we want to find out if spending more hours on a course lead to a higher coding ability in R. Furthermore, we want to find out if this course in comparison to other courses lead to a higher ability. To do so, we interact hours spent on courses with the variable indicating if the respondent worked through other courses or this course. Interactions effects can be implemented in R, by simply writing it explicitly into the function lm(), either with an asterisks * or a double point :, let us have a look it: # Setting seed for reproducibility set.seed(123) # Generate hours spent on a course hours_spent &lt;- runif(100, min = 0, max = 10) # Generate the course dummy (0 = other courses, 1 = this course) this_course = sample(c(0, 1), n, replace = TRUE) # Generate y with interaction effect coding_ability &lt;- 2 + 0.5 * hours_spent + 0 * this_course + 1.5 * hours_spent * this_course + rnorm(n) # Create a data frame df_int &lt;- data.frame(hours_spent, this_course, coding_ability) # Fit the interaction model model_interaction &lt;- lm(coding_ability ~ hours_spent * this_course, data = df_int) # Summarizing models summary(model_interaction) ## ## Call: ## lm(formula = coding_ability ~ hours_spent * this_course, data = df_int) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9122 -0.7295 -0.0765 0.5901 3.3882 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 1.82617 0.32506 5.618 ## hours_spent 0.52184 0.05380 9.699 ## this_course 0.03336 0.41030 0.081 ## hours_spent:this_course 1.47571 0.07068 20.877 ## Pr(&gt;|t|) ## (Intercept) 1.88e-07 *** ## hours_spent 6.59e-16 *** ## this_course 0.935 ## hours_spent:this_course &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9799 on 96 degrees of freedom ## Multiple R-squared: 0.9696, Adjusted R-squared: 0.9687 ## F-statistic: 1022 on 3 and 96 DF, p-value: &lt; 2.2e-16 Interaction effects on their own are not intuitive. To get an intuition we have to graphically plot it. The sjPlot package offers the plot_model() command, which automatically plots so called predicted probabilities. It would be too much to go into detail about predicted probabilities. What is more important is, that we get a graph, which shows the effect of of the interaction: We call plot_model() and include our model with the interaction effect model_interaction Then we have to call the type and set it to type=\"int\", which explicitly plots interaction effects. plot_model(model_interaction, type = &quot;int&quot;) + scale_x_continuous(breaks = seq(0,10, 1)) + labs(title = &quot;Coding Ability after this Course in Comparison&quot;, x = &quot;Hours spent&quot;, y = &quot;Coding Ability in R&quot;) + scale_color_manual( values = c(&quot;red&quot;, &quot;blue&quot;), labels = c(&quot;Other Courses&quot;, &quot;This Course&quot;) ) + theme_sjplot() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) On the x-axis, we have our independent variable and on the y-axis we do have our dependent variable. So far, so normal. But we see two lines: One line for all observations in category “Other Courses” (red line) and one line for all observations with “This Course” (blue line). That is exactly what an interaction does. For every point in our independent variable a probability for y is computed in one category, and one line for every observation in the other category. To interpret it we follow two steps: First, looking at the direction of the lines. We can see that X has a positive effect on Y. Both lines are increasing with higher X-values. Now, the interaction allows us to compare the effect from X on Y for all groups of the categorical variable. In our example that means, that observations in group 1 display a higher effect than observations in group 0. That could be for several reasons. Let us fill this example with life: We are studying the effect of study hours (x) on exam scores (y) for two different groups of students: those who attend a preparatory course (group 1) and those who do not (group 0). Interpretation: Direction of the Lines: Both lines (for group 0 and group 1) are increasing, indicating that more hours spent on courses lead to higher coding ability in R, on average. Effect Comparison: The slope for group 1 ( attended this course) is steeper compared to group 0 (attended to other courses). This means that for students who attended this course, each additional hour of study has a larger positive impact on their coding ability in R compared to those who did attend another course By visualizing and analyzing the interaction effect, we can draw meaningful conclusions about how different factors (like attending a preparatory course) modify the relationship between study efforts and performance outcomes. 6.7 Outlook This chapter was an introduction to hypothesis testing and basic statistical applications. Further, you were introduced in the most popular and important model, linear regression. The chapter showed not only how linear regression in R is computed, but also how to check if effects are robust and how the models fits. Also extensions to linear regression, namely multivariate regression and categorical variable were introduced. Linear regression is the basis of nearly everything. Advanced modelling of different classes of dependent variables to even machine learning techniques are based on linear regression. This was the reason, I did not show just linear regression, but also how vulnerable the model is. Poor modelling will always lead to poor results, so we have to aim to check how good our model fits our data and thus research interest. Further Links: A course I can recommend if you want to have a detailed deep dive into statistics is “Introduction to Econometrics with R” by Christoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer. 6.8 Exercise Section "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
